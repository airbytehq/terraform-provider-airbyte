---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "airbyte_destination_kafka Resource - terraform-provider-airbyte"
subcategory: ""
description: |-
  DestinationKafka Resource
---

# airbyte_destination_kafka (Resource)

DestinationKafka Resource

## Example Usage

```terraform
resource "airbyte_destination_kafka" "my_destination_kafka" {
  configuration = {
    acks                                  = "all"
    additional_properties                 = "{ \"see\": \"documentation\" }"
    batch_size                            = 16384
    bootstrap_servers                     = "kafka-broker1:9092,kafka-broker2:9092"
    buffer_memory                         = 33554432
    client_dns_lookup                     = "use_all_dns_ips"
    client_id                             = "airbyte-producer"
    compression_type                      = "none"
    delivery_timeout_ms                   = 120000
    enable_idempotence                    = false
    linger_ms                             = 0
    max_block_ms                          = 60000
    max_in_flight_requests_per_connection = 5
    max_request_size                      = 1048576
    protocol = {
      # ...
    }
    receive_buffer_bytes                   = 32768
    request_timeout_ms                     = 30000
    retries                                = 2147483647
    send_buffer_bytes                      = 131072
    socket_connection_setup_timeout_max_ms = 30000
    socket_connection_setup_timeout_ms     = 10000
    sync_producer                          = false
    test_topic                             = "test.topic"
    topic_pattern                          = "sample.topic"
  }
  definition_id = "9b27ec9b-64f6-46b9-b65b-698023e603c3"
  name          = "...my_name..."
  workspace_id  = "36bd7644-dd78-4484-93ec-f85297476150"
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `configuration` (Attributes) The values required to configure the destination. The schema for this must match the schema return by destination_definition_specifications/get for the destinationDefinition. (see [below for nested schema](#nestedatt--configuration))
- `name` (String) Name of the destination e.g. dev-mysql-instance.
- `workspace_id` (String)

### Optional

- `definition_id` (String) The UUID of the connector definition. One of configuration.destinationType or definitionId must be provided. Default: "9f760101-60ae-462f-9ee6-b7a9dafd454d"; Requires replacement if changed.

### Read-Only

- `created_at` (Number)
- `destination_id` (String)
- `destination_type` (String)
- `resource_allocation` (Attributes) actor or actor definition specific resource requirements. if default is set, these are the requirements that should be set for ALL jobs run for this actor definition. it is overriden by the job type specific configurations. if not set, the platform will use defaults. these values will be overriden by configuration at the connection level. (see [below for nested schema](#nestedatt--resource_allocation))

<a id="nestedatt--configuration"></a>
### Nested Schema for `configuration`

Required:

- `batch_size` (Number) The producer will attempt to batch records together into fewer requests whenever multiple records are being sent to the same partition.
- `bootstrap_servers` (String) A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. The client will make use of all servers irrespective of which servers are specified here for bootstrapping&mdash;this list only impacts the initial hosts used to discover the full set of servers. This list should be in the form <code>host1:port1,host2:port2,...</code>. Since these servers are just used for the initial connection to discover the full cluster membership (which may change dynamically), this list need not contain the full set of servers (you may want more than one, though, in case a server is down).
- `buffer_memory` (String) The total bytes of memory the producer can use to buffer records waiting to be sent to the server.
- `delivery_timeout_ms` (Number) An upper bound on the time to report success or failure after a call to 'send()' returns.
- `linger_ms` (String) The producer groups together any records that arrive in between request transmissions into a single batched request.
- `max_block_ms` (String) The configuration controls how long the KafkaProducer's send(), partitionsFor(), initTransactions(), sendOffsetsToTransaction(), commitTransaction() and abortTransaction() methods will block.
- `max_in_flight_requests_per_connection` (Number) The maximum number of unacknowledged requests the client will send on a single connection before blocking. Can be greater than 1, and the maximum value supported with idempotency is 5.
- `max_request_size` (Number) The maximum size of a request in bytes.
- `protocol` (Attributes) Protocol used to communicate with brokers. (see [below for nested schema](#nestedatt--configuration--protocol))
- `receive_buffer_bytes` (Number) The size of the TCP receive buffer (SO_RCVBUF) to use when reading data. If the value is -1, the OS default will be used.
- `request_timeout_ms` (Number) The configuration controls the maximum amount of time the client will wait for the response of a request. If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted.
- `retries` (Number) Setting a value greater than zero will cause the client to resend any record whose send fails with a potentially transient error.
- `send_buffer_bytes` (Number) The size of the TCP send buffer (SO_SNDBUF) to use when sending data. If the value is -1, the OS default will be used.
- `socket_connection_setup_timeout_max_ms` (String) The maximum amount of time the client will wait for the socket connection to be established. The connection setup timeout will increase exponentially for each consecutive connection failure up to this maximum.
- `socket_connection_setup_timeout_ms` (String) The amount of time the client will wait for the socket connection to be established.
- `topic_pattern` (String) Topic pattern in which the records will be sent. You can use patterns like '{namespace}' and/or '{stream}' to send the message to a specific topic based on these values. Notice that the topic name will be transformed to a standard naming convention.

Optional:

- `acks` (String) The number of acknowledgments the producer requires the leader to have received before considering a request complete. This controls the  durability of records that are sent. Default: "1"; must be one of ["0", "1", "all"]
- `additional_properties` (String) Parsed as JSON.
- `client_dns_lookup` (String) Controls how the client uses DNS lookups. If set to use_all_dns_ips, connect to each returned IP address in sequence until a successful connection is established. After a disconnection, the next IP is used. Once all IPs have been used once, the client resolves the IP(s) from the hostname again. If set to resolve_canonical_bootstrap_servers_only, resolve each bootstrap address into a list of canonical names. After the bootstrap phase, this behaves the same as use_all_dns_ips. If set to default (deprecated), attempt to connect to the first IP address returned by the lookup, even if the lookup returns multiple IP addresses. Default: "use_all_dns_ips"; must be one of ["default", "use_all_dns_ips", "resolve_canonical_bootstrap_servers_only"]
- `client_id` (String) An ID string to pass to the server when making requests. The purpose of this is to be able to track the source of requests beyond just ip/port by allowing a logical application name to be included in server-side request logging.
- `compression_type` (String) The compression type for all data generated by the producer. Default: "none"; must be one of ["none", "gzip", "snappy", "lz4", "zstd"]
- `enable_idempotence` (Boolean) When set to 'true', the producer will ensure that exactly one copy of each message is written in the stream. If 'false', producer retries due to broker failures, etc., may write duplicates of the retried message in the stream. Default: false
- `sync_producer` (Boolean) Wait synchronously until the record has been sent to Kafka. Default: false
- `test_topic` (String) Topic to test if Airbyte can produce messages.

<a id="nestedatt--configuration--protocol"></a>
### Nested Schema for `configuration.protocol`

Optional:

- `plaintext` (Attributes) (see [below for nested schema](#nestedatt--configuration--protocol--plaintext))
- `sasl_plaintext` (Attributes) (see [below for nested schema](#nestedatt--configuration--protocol--sasl_plaintext))
- `sasl_ssl` (Attributes) (see [below for nested schema](#nestedatt--configuration--protocol--sasl_ssl))

<a id="nestedatt--configuration--protocol--plaintext"></a>
### Nested Schema for `configuration.protocol.plaintext`

Optional:

- `security_protocol` (String) Default: "PLAINTEXT"; must be "PLAINTEXT"


<a id="nestedatt--configuration--protocol--sasl_plaintext"></a>
### Nested Schema for `configuration.protocol.sasl_plaintext`

Optional:

- `sasl_jaas_config` (String) JAAS login context parameters for SASL connections in the format used by JAAS configuration files. Default: ""
- `sasl_mechanism` (String) SASL mechanism used for client connections. This may be any mechanism for which a security provider is available. Default: "PLAIN"; must be one of ["GSSAPI", "OAUTHBEARER", "SCRAM-SHA-256", "SCRAM-SHA-512", "PLAIN"]
- `security_protocol` (String) Default: "SASL_PLAINTEXT"; must be "SASL_PLAINTEXT"


<a id="nestedatt--configuration--protocol--sasl_ssl"></a>
### Nested Schema for `configuration.protocol.sasl_ssl`

Optional:

- `sasl_jaas_config` (String) JAAS login context parameters for SASL connections in the format used by JAAS configuration files. Default: ""
- `sasl_mechanism` (String) SASL mechanism used for client connections. This may be any mechanism for which a security provider is available. Default: "GSSAPI"; must be one of ["GSSAPI", "OAUTHBEARER", "SCRAM-SHA-256", "SCRAM-SHA-512", "PLAIN"]
- `security_protocol` (String) Default: "SASL_SSL"; must be "SASL_SSL"




<a id="nestedatt--resource_allocation"></a>
### Nested Schema for `resource_allocation`

Read-Only:

- `default` (Attributes) optional resource requirements to run workers (blank for unbounded allocations) (see [below for nested schema](#nestedatt--resource_allocation--default))
- `job_specific` (Attributes List) (see [below for nested schema](#nestedatt--resource_allocation--job_specific))

<a id="nestedatt--resource_allocation--default"></a>
### Nested Schema for `resource_allocation.default`

Read-Only:

- `cpu_limit` (String)
- `cpu_request` (String)
- `ephemeral_storage_limit` (String)
- `ephemeral_storage_request` (String)
- `memory_limit` (String)
- `memory_request` (String)


<a id="nestedatt--resource_allocation--job_specific"></a>
### Nested Schema for `resource_allocation.job_specific`

Read-Only:

- `job_type` (String) enum that describes the different types of jobs that the platform runs.
- `resource_requirements` (Attributes) optional resource requirements to run workers (blank for unbounded allocations) (see [below for nested schema](#nestedatt--resource_allocation--job_specific--resource_requirements))

<a id="nestedatt--resource_allocation--job_specific--resource_requirements"></a>
### Nested Schema for `resource_allocation.job_specific.resource_requirements`

Read-Only:

- `cpu_limit` (String)
- `cpu_request` (String)
- `ephemeral_storage_limit` (String)
- `ephemeral_storage_request` (String)
- `memory_limit` (String)
- `memory_request` (String)

## Import

Import is supported using the following syntax:

In Terraform v1.5.0 and later, the [`import` block](https://developer.hashicorp.com/terraform/language/import) can be used with the `id` attribute, for example:

```terraform
import {
  to = airbyte_destination_kafka.my_airbyte_destination_kafka
  id = "..."
}
```

The [`terraform import` command](https://developer.hashicorp.com/terraform/cli/commands/import) can be used, for example:

```shell
terraform import airbyte_destination_kafka.my_airbyte_destination_kafka "..."
```
