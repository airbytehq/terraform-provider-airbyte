---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "airbyte_destination_databricks Resource - terraform-provider-airbyte"
subcategory: ""
description: |-
  DestinationDatabricks Resource
---

# airbyte_destination_databricks (Resource)

DestinationDatabricks Resource

## Example Usage

```terraform
resource "airbyte_destination_databricks" "my_destination_databricks" {
  configuration = {
    accept_terms = false
    data_source = {
      data_source_type = "MANAGED_TABLES_STORAGE"
    }
    database                         = "...my_database..."
    databricks_http_path             = "sql/protocolvx/o/1234567489/0000-1111111-abcd90"
    databricks_personal_access_token = "dapi0123456789abcdefghij0123456789AB"
    databricks_port                  = "443"
    databricks_server_hostname       = "abc-12345678-wxyz.cloud.databricks.com"
    destination_type                 = "databricks"
    purge_staging_data               = true
    schema                           = "default"
  }
  name         = "Joanna Kohler"
  workspace_id = "29cdb1a8-422b-4b67-9d23-22715bf0cbb1"
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `configuration` (Attributes) (see [below for nested schema](#nestedatt--configuration))
- `name` (String)
- `workspace_id` (String)

### Read-Only

- `destination_id` (String)
- `destination_type` (String)

<a id="nestedatt--configuration"></a>
### Nested Schema for `configuration`

Required:

- `accept_terms` (Boolean) You must agree to the Databricks JDBC Driver <a href="https://databricks.com/jdbc-odbc-driver-license">Terms & Conditions</a> to use this connector.
- `data_source` (Attributes) Storage on which the delta lake is built. (see [below for nested schema](#nestedatt--configuration--data_source))
- `databricks_http_path` (String) Databricks Cluster HTTP Path.
- `databricks_personal_access_token` (String) Databricks Personal Access Token for making authenticated requests.
- `databricks_server_hostname` (String) Databricks Cluster Server Hostname.
- `destination_type` (String) must be one of [databricks]

Optional:

- `database` (String) The name of the catalog. If not specified otherwise, the "hive_metastore" will be used.
- `databricks_port` (String) Databricks Cluster Port.
- `purge_staging_data` (Boolean) Default to 'true'. Switch it to 'false' for debugging purpose.
- `schema` (String) The default schema tables are written. If not specified otherwise, the "default" will be used.

<a id="nestedatt--configuration--data_source"></a>
### Nested Schema for `configuration.data_source`

Optional:

- `destination_databricks_data_source_amazon_s3` (Attributes) Storage on which the delta lake is built. (see [below for nested schema](#nestedatt--configuration--data_source--destination_databricks_data_source_amazon_s3))
- `destination_databricks_data_source_azure_blob_storage` (Attributes) Storage on which the delta lake is built. (see [below for nested schema](#nestedatt--configuration--data_source--destination_databricks_data_source_azure_blob_storage))
- `destination_databricks_data_source_recommended_managed_tables` (Attributes) Storage on which the delta lake is built. (see [below for nested schema](#nestedatt--configuration--data_source--destination_databricks_data_source_recommended_managed_tables))
- `destination_databricks_update_data_source_amazon_s3` (Attributes) Storage on which the delta lake is built. (see [below for nested schema](#nestedatt--configuration--data_source--destination_databricks_update_data_source_amazon_s3))
- `destination_databricks_update_data_source_azure_blob_storage` (Attributes) Storage on which the delta lake is built. (see [below for nested schema](#nestedatt--configuration--data_source--destination_databricks_update_data_source_azure_blob_storage))
- `destination_databricks_update_data_source_recommended_managed_tables` (Attributes) Storage on which the delta lake is built. (see [below for nested schema](#nestedatt--configuration--data_source--destination_databricks_update_data_source_recommended_managed_tables))

<a id="nestedatt--configuration--data_source--destination_databricks_data_source_amazon_s3"></a>
### Nested Schema for `configuration.data_source.destination_databricks_data_source_amazon_s3`

Required:

- `data_source_type` (String) must be one of [S3_STORAGE]
- `s3_access_key_id` (String) The Access Key Id granting allow one to access the above S3 staging bucket. Airbyte requires Read and Write permissions to the given bucket.
- `s3_bucket_name` (String) The name of the S3 bucket to use for intermittent staging of the data.
- `s3_bucket_path` (String) The directory under the S3 bucket where data will be written.
- `s3_bucket_region` (String) must be one of [, us-east-1, us-east-2, us-west-1, us-west-2, af-south-1, ap-east-1, ap-south-1, ap-northeast-1, ap-northeast-2, ap-northeast-3, ap-southeast-1, ap-southeast-2, ca-central-1, cn-north-1, cn-northwest-1, eu-central-1, eu-north-1, eu-south-1, eu-west-1, eu-west-2, eu-west-3, sa-east-1, me-south-1, us-gov-east-1, us-gov-west-1]
The region of the S3 staging bucket to use if utilising a copy strategy.
- `s3_secret_access_key` (String) The corresponding secret to the above access key id.

Optional:

- `file_name_pattern` (String) The pattern allows you to set the file-name format for the S3 staging file(s)


<a id="nestedatt--configuration--data_source--destination_databricks_data_source_azure_blob_storage"></a>
### Nested Schema for `configuration.data_source.destination_databricks_data_source_azure_blob_storage`

Required:

- `azure_blob_storage_account_name` (String) The account's name of the Azure Blob Storage.
- `azure_blob_storage_container_name` (String) The name of the Azure blob storage container.
- `azure_blob_storage_sas_token` (String) Shared access signature (SAS) token to grant limited access to objects in your storage account.
- `data_source_type` (String) must be one of [AZURE_BLOB_STORAGE]

Optional:

- `azure_blob_storage_endpoint_domain_name` (String) This is Azure Blob Storage endpoint domain name. Leave default value (or leave it empty if run container from command line) to use Microsoft native from example.


<a id="nestedatt--configuration--data_source--destination_databricks_data_source_recommended_managed_tables"></a>
### Nested Schema for `configuration.data_source.destination_databricks_data_source_recommended_managed_tables`

Required:

- `data_source_type` (String) must be one of [MANAGED_TABLES_STORAGE]


<a id="nestedatt--configuration--data_source--destination_databricks_update_data_source_amazon_s3"></a>
### Nested Schema for `configuration.data_source.destination_databricks_update_data_source_amazon_s3`

Required:

- `data_source_type` (String) must be one of [S3_STORAGE]
- `s3_access_key_id` (String) The Access Key Id granting allow one to access the above S3 staging bucket. Airbyte requires Read and Write permissions to the given bucket.
- `s3_bucket_name` (String) The name of the S3 bucket to use for intermittent staging of the data.
- `s3_bucket_path` (String) The directory under the S3 bucket where data will be written.
- `s3_bucket_region` (String) must be one of [, us-east-1, us-east-2, us-west-1, us-west-2, af-south-1, ap-east-1, ap-south-1, ap-northeast-1, ap-northeast-2, ap-northeast-3, ap-southeast-1, ap-southeast-2, ca-central-1, cn-north-1, cn-northwest-1, eu-central-1, eu-north-1, eu-south-1, eu-west-1, eu-west-2, eu-west-3, sa-east-1, me-south-1, us-gov-east-1, us-gov-west-1]
The region of the S3 staging bucket to use if utilising a copy strategy.
- `s3_secret_access_key` (String) The corresponding secret to the above access key id.

Optional:

- `file_name_pattern` (String) The pattern allows you to set the file-name format for the S3 staging file(s)


<a id="nestedatt--configuration--data_source--destination_databricks_update_data_source_azure_blob_storage"></a>
### Nested Schema for `configuration.data_source.destination_databricks_update_data_source_azure_blob_storage`

Required:

- `azure_blob_storage_account_name` (String) The account's name of the Azure Blob Storage.
- `azure_blob_storage_container_name` (String) The name of the Azure blob storage container.
- `azure_blob_storage_sas_token` (String) Shared access signature (SAS) token to grant limited access to objects in your storage account.
- `data_source_type` (String) must be one of [AZURE_BLOB_STORAGE]

Optional:

- `azure_blob_storage_endpoint_domain_name` (String) This is Azure Blob Storage endpoint domain name. Leave default value (or leave it empty if run container from command line) to use Microsoft native from example.


<a id="nestedatt--configuration--data_source--destination_databricks_update_data_source_recommended_managed_tables"></a>
### Nested Schema for `configuration.data_source.destination_databricks_update_data_source_recommended_managed_tables`

Required:

- `data_source_type` (String) must be one of [MANAGED_TABLES_STORAGE]


