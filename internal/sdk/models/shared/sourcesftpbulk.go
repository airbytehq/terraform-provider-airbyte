// Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.

package shared

import (
	"encoding/json"
	"errors"
	"fmt"
	"github.com/airbytehq/terraform-provider-airbyte/internal/sdk/internal/utils"
	"github.com/airbytehq/terraform-provider-airbyte/internal/sdk/types"
	"time"
)

type SourceSftpBulkSchemasAuthType string

const (
	SourceSftpBulkSchemasAuthTypePrivateKey SourceSftpBulkSchemasAuthType = "private_key"
)

func (e SourceSftpBulkSchemasAuthType) ToPointer() *SourceSftpBulkSchemasAuthType {
	return &e
}
func (e *SourceSftpBulkSchemasAuthType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "private_key":
		*e = SourceSftpBulkSchemasAuthType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceSftpBulkSchemasAuthType: %v", v)
	}
}

type AuthenticateViaPrivateKey struct {
	authType *SourceSftpBulkSchemasAuthType `const:"private_key" json:"auth_type"`
	// The Private key
	PrivateKey string `json:"private_key"`
}

func (a AuthenticateViaPrivateKey) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AuthenticateViaPrivateKey) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (a *AuthenticateViaPrivateKey) GetAuthType() *SourceSftpBulkSchemasAuthType {
	return SourceSftpBulkSchemasAuthTypePrivateKey.ToPointer()
}

func (a *AuthenticateViaPrivateKey) GetPrivateKey() string {
	if a == nil {
		return ""
	}
	return a.PrivateKey
}

type SourceSftpBulkAuthType string

const (
	SourceSftpBulkAuthTypePassword SourceSftpBulkAuthType = "password"
)

func (e SourceSftpBulkAuthType) ToPointer() *SourceSftpBulkAuthType {
	return &e
}
func (e *SourceSftpBulkAuthType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "password":
		*e = SourceSftpBulkAuthType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceSftpBulkAuthType: %v", v)
	}
}

type AuthenticateViaPassword struct {
	authType *SourceSftpBulkAuthType `const:"password" json:"auth_type"`
	// Password
	Password string `json:"password"`
}

func (a AuthenticateViaPassword) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AuthenticateViaPassword) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (a *AuthenticateViaPassword) GetAuthType() *SourceSftpBulkAuthType {
	return SourceSftpBulkAuthTypePassword.ToPointer()
}

func (a *AuthenticateViaPassword) GetPassword() string {
	if a == nil {
		return ""
	}
	return a.Password
}

type SourceSftpBulkAuthenticationType string

const (
	SourceSftpBulkAuthenticationTypeAuthenticateViaPassword   SourceSftpBulkAuthenticationType = "Authenticate via Password"
	SourceSftpBulkAuthenticationTypeAuthenticateViaPrivateKey SourceSftpBulkAuthenticationType = "Authenticate via Private Key"
)

// SourceSftpBulkAuthentication - Credentials for connecting to the SFTP Server
type SourceSftpBulkAuthentication struct {
	AuthenticateViaPassword   *AuthenticateViaPassword   `queryParam:"inline" union:"member"`
	AuthenticateViaPrivateKey *AuthenticateViaPrivateKey `queryParam:"inline" union:"member"`

	Type SourceSftpBulkAuthenticationType
}

func CreateSourceSftpBulkAuthenticationAuthenticateViaPassword(authenticateViaPassword AuthenticateViaPassword) SourceSftpBulkAuthentication {
	typ := SourceSftpBulkAuthenticationTypeAuthenticateViaPassword

	return SourceSftpBulkAuthentication{
		AuthenticateViaPassword: &authenticateViaPassword,
		Type:                    typ,
	}
}

func CreateSourceSftpBulkAuthenticationAuthenticateViaPrivateKey(authenticateViaPrivateKey AuthenticateViaPrivateKey) SourceSftpBulkAuthentication {
	typ := SourceSftpBulkAuthenticationTypeAuthenticateViaPrivateKey

	return SourceSftpBulkAuthentication{
		AuthenticateViaPrivateKey: &authenticateViaPrivateKey,
		Type:                      typ,
	}
}

func (u *SourceSftpBulkAuthentication) UnmarshalJSON(data []byte) error {

	var candidates []utils.UnionCandidate

	// Collect all valid candidates
	var authenticateViaPassword AuthenticateViaPassword = AuthenticateViaPassword{}
	if err := utils.UnmarshalJSON(data, &authenticateViaPassword, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkAuthenticationTypeAuthenticateViaPassword,
			Value: &authenticateViaPassword,
		})
	}

	var authenticateViaPrivateKey AuthenticateViaPrivateKey = AuthenticateViaPrivateKey{}
	if err := utils.UnmarshalJSON(data, &authenticateViaPrivateKey, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkAuthenticationTypeAuthenticateViaPrivateKey,
			Value: &authenticateViaPrivateKey,
		})
	}

	if len(candidates) == 0 {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkAuthentication", string(data))
	}

	// Pick the best candidate using multi-stage filtering
	best := utils.PickBestUnionCandidate(candidates, data)
	if best == nil {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkAuthentication", string(data))
	}

	// Set the union type and value based on the best candidate
	u.Type = best.Type.(SourceSftpBulkAuthenticationType)
	switch best.Type {
	case SourceSftpBulkAuthenticationTypeAuthenticateViaPassword:
		u.AuthenticateViaPassword = best.Value.(*AuthenticateViaPassword)
		return nil
	case SourceSftpBulkAuthenticationTypeAuthenticateViaPrivateKey:
		u.AuthenticateViaPrivateKey = best.Value.(*AuthenticateViaPrivateKey)
		return nil
	}

	return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkAuthentication", string(data))
}

func (u SourceSftpBulkAuthentication) MarshalJSON() ([]byte, error) {
	if u.AuthenticateViaPassword != nil {
		return utils.MarshalJSON(u.AuthenticateViaPassword, "", true)
	}

	if u.AuthenticateViaPrivateKey != nil {
		return utils.MarshalJSON(u.AuthenticateViaPrivateKey, "", true)
	}

	return nil, errors.New("could not marshal union type SourceSftpBulkAuthentication: all fields are null")
}

type SourceSftpBulkSchemasDeliveryType string

const (
	SourceSftpBulkSchemasDeliveryTypeUseFileTransfer SourceSftpBulkSchemasDeliveryType = "use_file_transfer"
)

func (e SourceSftpBulkSchemasDeliveryType) ToPointer() *SourceSftpBulkSchemasDeliveryType {
	return &e
}
func (e *SourceSftpBulkSchemasDeliveryType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "use_file_transfer":
		*e = SourceSftpBulkSchemasDeliveryType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceSftpBulkSchemasDeliveryType: %v", v)
	}
}

// SourceSftpBulkCopyRawFiles - Copy raw files without parsing their contents. Bits are copied into the destination exactly as they appeared in the source. Recommended for use with unstructured text data, non-text and compressed files.
type SourceSftpBulkCopyRawFiles struct {
	deliveryType *SourceSftpBulkSchemasDeliveryType `const:"use_file_transfer" json:"delivery_type"`
	// If enabled, sends subdirectory folder structure along with source file names to the destination. Otherwise, files will be synced by their names only. This option is ignored when file-based replication is not enabled.
	PreserveDirectoryStructure *bool `default:"true" json:"preserve_directory_structure"`
}

func (s SourceSftpBulkCopyRawFiles) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkCopyRawFiles) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkCopyRawFiles) GetDeliveryType() *SourceSftpBulkSchemasDeliveryType {
	return SourceSftpBulkSchemasDeliveryTypeUseFileTransfer.ToPointer()
}

func (s *SourceSftpBulkCopyRawFiles) GetPreserveDirectoryStructure() *bool {
	if s == nil {
		return nil
	}
	return s.PreserveDirectoryStructure
}

type SourceSftpBulkDeliveryType string

const (
	SourceSftpBulkDeliveryTypeUseRecordsTransfer SourceSftpBulkDeliveryType = "use_records_transfer"
)

func (e SourceSftpBulkDeliveryType) ToPointer() *SourceSftpBulkDeliveryType {
	return &e
}
func (e *SourceSftpBulkDeliveryType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "use_records_transfer":
		*e = SourceSftpBulkDeliveryType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceSftpBulkDeliveryType: %v", v)
	}
}

// SourceSftpBulkReplicateRecords - Recommended - Extract and load structured records into your destination of choice. This is the classic method of moving data in Airbyte. It allows for blocking and hashing individual fields or files from a structured schema. Data can be flattened, typed and deduped depending on the destination.
type SourceSftpBulkReplicateRecords struct {
	deliveryType *SourceSftpBulkDeliveryType `const:"use_records_transfer" json:"delivery_type"`
}

func (s SourceSftpBulkReplicateRecords) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkReplicateRecords) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkReplicateRecords) GetDeliveryType() *SourceSftpBulkDeliveryType {
	return SourceSftpBulkDeliveryTypeUseRecordsTransfer.ToPointer()
}

type SourceSftpBulkDeliveryMethodType string

const (
	SourceSftpBulkDeliveryMethodTypeSourceSftpBulkReplicateRecords SourceSftpBulkDeliveryMethodType = "source-sftp-bulk_Replicate Records"
	SourceSftpBulkDeliveryMethodTypeSourceSftpBulkCopyRawFiles     SourceSftpBulkDeliveryMethodType = "source-sftp-bulk_Copy Raw Files"
)

type SourceSftpBulkDeliveryMethod struct {
	SourceSftpBulkReplicateRecords *SourceSftpBulkReplicateRecords `queryParam:"inline" union:"member"`
	SourceSftpBulkCopyRawFiles     *SourceSftpBulkCopyRawFiles     `queryParam:"inline" union:"member"`

	Type SourceSftpBulkDeliveryMethodType
}

func CreateSourceSftpBulkDeliveryMethodSourceSftpBulkReplicateRecords(sourceSftpBulkReplicateRecords SourceSftpBulkReplicateRecords) SourceSftpBulkDeliveryMethod {
	typ := SourceSftpBulkDeliveryMethodTypeSourceSftpBulkReplicateRecords

	return SourceSftpBulkDeliveryMethod{
		SourceSftpBulkReplicateRecords: &sourceSftpBulkReplicateRecords,
		Type:                           typ,
	}
}

func CreateSourceSftpBulkDeliveryMethodSourceSftpBulkCopyRawFiles(sourceSftpBulkCopyRawFiles SourceSftpBulkCopyRawFiles) SourceSftpBulkDeliveryMethod {
	typ := SourceSftpBulkDeliveryMethodTypeSourceSftpBulkCopyRawFiles

	return SourceSftpBulkDeliveryMethod{
		SourceSftpBulkCopyRawFiles: &sourceSftpBulkCopyRawFiles,
		Type:                       typ,
	}
}

func (u *SourceSftpBulkDeliveryMethod) UnmarshalJSON(data []byte) error {

	var candidates []utils.UnionCandidate

	// Collect all valid candidates
	var sourceSftpBulkReplicateRecords SourceSftpBulkReplicateRecords = SourceSftpBulkReplicateRecords{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkReplicateRecords, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkDeliveryMethodTypeSourceSftpBulkReplicateRecords,
			Value: &sourceSftpBulkReplicateRecords,
		})
	}

	var sourceSftpBulkCopyRawFiles SourceSftpBulkCopyRawFiles = SourceSftpBulkCopyRawFiles{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkCopyRawFiles, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkDeliveryMethodTypeSourceSftpBulkCopyRawFiles,
			Value: &sourceSftpBulkCopyRawFiles,
		})
	}

	if len(candidates) == 0 {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkDeliveryMethod", string(data))
	}

	// Pick the best candidate using multi-stage filtering
	best := utils.PickBestUnionCandidate(candidates, data)
	if best == nil {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkDeliveryMethod", string(data))
	}

	// Set the union type and value based on the best candidate
	u.Type = best.Type.(SourceSftpBulkDeliveryMethodType)
	switch best.Type {
	case SourceSftpBulkDeliveryMethodTypeSourceSftpBulkReplicateRecords:
		u.SourceSftpBulkReplicateRecords = best.Value.(*SourceSftpBulkReplicateRecords)
		return nil
	case SourceSftpBulkDeliveryMethodTypeSourceSftpBulkCopyRawFiles:
		u.SourceSftpBulkCopyRawFiles = best.Value.(*SourceSftpBulkCopyRawFiles)
		return nil
	}

	return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkDeliveryMethod", string(data))
}

func (u SourceSftpBulkDeliveryMethod) MarshalJSON() ([]byte, error) {
	if u.SourceSftpBulkReplicateRecords != nil {
		return utils.MarshalJSON(u.SourceSftpBulkReplicateRecords, "", true)
	}

	if u.SourceSftpBulkCopyRawFiles != nil {
		return utils.MarshalJSON(u.SourceSftpBulkCopyRawFiles, "", true)
	}

	return nil, errors.New("could not marshal union type SourceSftpBulkDeliveryMethod: all fields are null")
}

type SourceSftpBulkExcelFormat struct {
	filetype *string `const:"excel" json:"filetype"`
}

func (s SourceSftpBulkExcelFormat) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkExcelFormat) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkExcelFormat) GetFiletype() *string {
	return types.Pointer("excel")
}

type SourceSftpBulkSchemasMode string

const (
	SourceSftpBulkSchemasModeAPI SourceSftpBulkSchemasMode = "api"
)

func (e SourceSftpBulkSchemasMode) ToPointer() *SourceSftpBulkSchemasMode {
	return &e
}
func (e *SourceSftpBulkSchemasMode) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "api":
		*e = SourceSftpBulkSchemasMode(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceSftpBulkSchemasMode: %v", v)
	}
}

type SourceSftpBulkAPIParameterConfigModel struct {
	// The name of the unstructured API parameter to use
	Name string `json:"name"`
	// The value of the parameter
	Value string `json:"value"`
}

func (s SourceSftpBulkAPIParameterConfigModel) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkAPIParameterConfigModel) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkAPIParameterConfigModel) GetName() string {
	if s == nil {
		return ""
	}
	return s.Name
}

func (s *SourceSftpBulkAPIParameterConfigModel) GetValue() string {
	if s == nil {
		return ""
	}
	return s.Value
}

// SourceSftpBulkViaAPI - Process files via an API, using the `hi_res` mode. This option is useful for increased performance and accuracy, but requires an API key and a hosted instance of unstructured.
type SourceSftpBulkViaAPI struct {
	// The API key to use matching the environment
	APIKey *string `default:"" json:"api_key"`
	// The URL of the unstructured API to use
	APIURL *string                    `default:"https://api.unstructured.io" json:"api_url"`
	mode   *SourceSftpBulkSchemasMode `const:"api" json:"mode"`
	// List of parameters send to the API
	Parameters []SourceSftpBulkAPIParameterConfigModel `json:"parameters,omitempty"`
}

func (s SourceSftpBulkViaAPI) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkViaAPI) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkViaAPI) GetAPIKey() *string {
	if s == nil {
		return nil
	}
	return s.APIKey
}

func (s *SourceSftpBulkViaAPI) GetAPIURL() *string {
	if s == nil {
		return nil
	}
	return s.APIURL
}

func (s *SourceSftpBulkViaAPI) GetMode() *SourceSftpBulkSchemasMode {
	return SourceSftpBulkSchemasModeAPI.ToPointer()
}

func (s *SourceSftpBulkViaAPI) GetParameters() []SourceSftpBulkAPIParameterConfigModel {
	if s == nil {
		return nil
	}
	return s.Parameters
}

type SourceSftpBulkMode string

const (
	SourceSftpBulkModeLocal SourceSftpBulkMode = "local"
)

func (e SourceSftpBulkMode) ToPointer() *SourceSftpBulkMode {
	return &e
}
func (e *SourceSftpBulkMode) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "local":
		*e = SourceSftpBulkMode(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceSftpBulkMode: %v", v)
	}
}

// SourceSftpBulkLocal - Process files locally, supporting `fast` and `ocr` modes. This is the default option.
type SourceSftpBulkLocal struct {
	mode *SourceSftpBulkMode `const:"local" json:"mode"`
}

func (s SourceSftpBulkLocal) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkLocal) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkLocal) GetMode() *SourceSftpBulkMode {
	return SourceSftpBulkModeLocal.ToPointer()
}

type SourceSftpBulkProcessingType string

const (
	SourceSftpBulkProcessingTypeSourceSftpBulkLocal  SourceSftpBulkProcessingType = "source-sftp-bulk_Local"
	SourceSftpBulkProcessingTypeSourceSftpBulkViaAPI SourceSftpBulkProcessingType = "source-sftp-bulk_via API"
)

// SourceSftpBulkProcessing - Processing configuration
type SourceSftpBulkProcessing struct {
	SourceSftpBulkLocal  *SourceSftpBulkLocal  `queryParam:"inline" union:"member"`
	SourceSftpBulkViaAPI *SourceSftpBulkViaAPI `queryParam:"inline" union:"member"`

	Type SourceSftpBulkProcessingType
}

func CreateSourceSftpBulkProcessingSourceSftpBulkLocal(sourceSftpBulkLocal SourceSftpBulkLocal) SourceSftpBulkProcessing {
	typ := SourceSftpBulkProcessingTypeSourceSftpBulkLocal

	return SourceSftpBulkProcessing{
		SourceSftpBulkLocal: &sourceSftpBulkLocal,
		Type:                typ,
	}
}

func CreateSourceSftpBulkProcessingSourceSftpBulkViaAPI(sourceSftpBulkViaAPI SourceSftpBulkViaAPI) SourceSftpBulkProcessing {
	typ := SourceSftpBulkProcessingTypeSourceSftpBulkViaAPI

	return SourceSftpBulkProcessing{
		SourceSftpBulkViaAPI: &sourceSftpBulkViaAPI,
		Type:                 typ,
	}
}

func (u *SourceSftpBulkProcessing) UnmarshalJSON(data []byte) error {

	var candidates []utils.UnionCandidate

	// Collect all valid candidates
	var sourceSftpBulkLocal SourceSftpBulkLocal = SourceSftpBulkLocal{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkLocal, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkProcessingTypeSourceSftpBulkLocal,
			Value: &sourceSftpBulkLocal,
		})
	}

	var sourceSftpBulkViaAPI SourceSftpBulkViaAPI = SourceSftpBulkViaAPI{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkViaAPI, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkProcessingTypeSourceSftpBulkViaAPI,
			Value: &sourceSftpBulkViaAPI,
		})
	}

	if len(candidates) == 0 {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkProcessing", string(data))
	}

	// Pick the best candidate using multi-stage filtering
	best := utils.PickBestUnionCandidate(candidates, data)
	if best == nil {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkProcessing", string(data))
	}

	// Set the union type and value based on the best candidate
	u.Type = best.Type.(SourceSftpBulkProcessingType)
	switch best.Type {
	case SourceSftpBulkProcessingTypeSourceSftpBulkLocal:
		u.SourceSftpBulkLocal = best.Value.(*SourceSftpBulkLocal)
		return nil
	case SourceSftpBulkProcessingTypeSourceSftpBulkViaAPI:
		u.SourceSftpBulkViaAPI = best.Value.(*SourceSftpBulkViaAPI)
		return nil
	}

	return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkProcessing", string(data))
}

func (u SourceSftpBulkProcessing) MarshalJSON() ([]byte, error) {
	if u.SourceSftpBulkLocal != nil {
		return utils.MarshalJSON(u.SourceSftpBulkLocal, "", true)
	}

	if u.SourceSftpBulkViaAPI != nil {
		return utils.MarshalJSON(u.SourceSftpBulkViaAPI, "", true)
	}

	return nil, errors.New("could not marshal union type SourceSftpBulkProcessing: all fields are null")
}

// SourceSftpBulkParsingStrategy - The strategy used to parse documents. `fast` extracts text directly from the document which doesn't work for all files. `ocr_only` is more reliable, but slower. `hi_res` is the most reliable, but requires an API key and a hosted instance of unstructured and can't be used with local mode. See the unstructured.io documentation for more details: https://unstructured-io.github.io/unstructured/core/partition.html#partition-pdf
type SourceSftpBulkParsingStrategy string

const (
	SourceSftpBulkParsingStrategyAuto    SourceSftpBulkParsingStrategy = "auto"
	SourceSftpBulkParsingStrategyFast    SourceSftpBulkParsingStrategy = "fast"
	SourceSftpBulkParsingStrategyOcrOnly SourceSftpBulkParsingStrategy = "ocr_only"
	SourceSftpBulkParsingStrategyHiRes   SourceSftpBulkParsingStrategy = "hi_res"
)

func (e SourceSftpBulkParsingStrategy) ToPointer() *SourceSftpBulkParsingStrategy {
	return &e
}
func (e *SourceSftpBulkParsingStrategy) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "fast":
		fallthrough
	case "ocr_only":
		fallthrough
	case "hi_res":
		*e = SourceSftpBulkParsingStrategy(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceSftpBulkParsingStrategy: %v", v)
	}
}

// SourceSftpBulkUnstructuredDocumentFormat - Extract text from document formats (.pdf, .docx, .md, .pptx) and emit as one record per file.
type SourceSftpBulkUnstructuredDocumentFormat struct {
	filetype *string `const:"unstructured" json:"filetype"`
	// Processing configuration
	Processing *SourceSftpBulkProcessing `json:"processing,omitempty"`
	// If true, skip files that cannot be parsed and pass the error message along as the _ab_source_file_parse_error field. If false, fail the sync.
	SkipUnprocessableFiles *bool `default:"true" json:"skip_unprocessable_files"`
	// The strategy used to parse documents. `fast` extracts text directly from the document which doesn't work for all files. `ocr_only` is more reliable, but slower. `hi_res` is the most reliable, but requires an API key and a hosted instance of unstructured and can't be used with local mode. See the unstructured.io documentation for more details: https://unstructured-io.github.io/unstructured/core/partition.html#partition-pdf
	Strategy *SourceSftpBulkParsingStrategy `default:"auto" json:"strategy"`
}

func (s SourceSftpBulkUnstructuredDocumentFormat) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkUnstructuredDocumentFormat) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkUnstructuredDocumentFormat) GetFiletype() *string {
	return types.Pointer("unstructured")
}

func (s *SourceSftpBulkUnstructuredDocumentFormat) GetProcessing() *SourceSftpBulkProcessing {
	if s == nil {
		return nil
	}
	return s.Processing
}

func (s *SourceSftpBulkUnstructuredDocumentFormat) GetSkipUnprocessableFiles() *bool {
	if s == nil {
		return nil
	}
	return s.SkipUnprocessableFiles
}

func (s *SourceSftpBulkUnstructuredDocumentFormat) GetStrategy() *SourceSftpBulkParsingStrategy {
	if s == nil {
		return nil
	}
	return s.Strategy
}

type SourceSftpBulkParquetFormat struct {
	// Whether to convert decimal fields to floats. There is a loss of precision when converting decimals to floats, so this is not recommended.
	DecimalAsFloat *bool   `default:"false" json:"decimal_as_float"`
	filetype       *string `const:"parquet" json:"filetype"`
}

func (s SourceSftpBulkParquetFormat) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkParquetFormat) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkParquetFormat) GetDecimalAsFloat() *bool {
	if s == nil {
		return nil
	}
	return s.DecimalAsFloat
}

func (s *SourceSftpBulkParquetFormat) GetFiletype() *string {
	return types.Pointer("parquet")
}

type SourceSftpBulkJsonlFormat struct {
	filetype *string `const:"jsonl" json:"filetype"`
}

func (s SourceSftpBulkJsonlFormat) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkJsonlFormat) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkJsonlFormat) GetFiletype() *string {
	return types.Pointer("jsonl")
}

type SourceSftpBulkUserProvided struct {
	// The column names that will be used while emitting the CSV records
	ColumnNames          []string `json:"column_names"`
	headerDefinitionType *string  `const:"User Provided" json:"header_definition_type"`
}

func (s SourceSftpBulkUserProvided) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkUserProvided) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkUserProvided) GetColumnNames() []string {
	if s == nil {
		return []string{}
	}
	return s.ColumnNames
}

func (s *SourceSftpBulkUserProvided) GetHeaderDefinitionType() *string {
	return types.Pointer("User Provided")
}

type SourceSftpBulkAutogenerated struct {
	headerDefinitionType *string `const:"Autogenerated" json:"header_definition_type"`
}

func (s SourceSftpBulkAutogenerated) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkAutogenerated) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkAutogenerated) GetHeaderDefinitionType() *string {
	return types.Pointer("Autogenerated")
}

type SourceSftpBulkFromCSV struct {
	headerDefinitionType *string `const:"From CSV" json:"header_definition_type"`
}

func (s SourceSftpBulkFromCSV) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkFromCSV) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkFromCSV) GetHeaderDefinitionType() *string {
	return types.Pointer("From CSV")
}

type SourceSftpBulkCSVHeaderDefinitionType string

const (
	SourceSftpBulkCSVHeaderDefinitionTypeSourceSftpBulkFromCSV       SourceSftpBulkCSVHeaderDefinitionType = "source-sftp-bulk_From CSV"
	SourceSftpBulkCSVHeaderDefinitionTypeSourceSftpBulkAutogenerated SourceSftpBulkCSVHeaderDefinitionType = "source-sftp-bulk_Autogenerated"
	SourceSftpBulkCSVHeaderDefinitionTypeSourceSftpBulkUserProvided  SourceSftpBulkCSVHeaderDefinitionType = "source-sftp-bulk_User Provided"
)

// SourceSftpBulkCSVHeaderDefinition - How headers will be defined. `User Provided` assumes the CSV does not have a header row and uses the headers provided and `Autogenerated` assumes the CSV does not have a header row and the CDK will generate headers using for `f{i}` where `i` is the index starting from 0. Else, the default behavior is to use the header from the CSV file. If a user wants to autogenerate or provide column names for a CSV having headers, they can skip rows.
type SourceSftpBulkCSVHeaderDefinition struct {
	SourceSftpBulkFromCSV       *SourceSftpBulkFromCSV       `queryParam:"inline" union:"member"`
	SourceSftpBulkAutogenerated *SourceSftpBulkAutogenerated `queryParam:"inline" union:"member"`
	SourceSftpBulkUserProvided  *SourceSftpBulkUserProvided  `queryParam:"inline" union:"member"`

	Type SourceSftpBulkCSVHeaderDefinitionType
}

func CreateSourceSftpBulkCSVHeaderDefinitionSourceSftpBulkFromCSV(sourceSftpBulkFromCSV SourceSftpBulkFromCSV) SourceSftpBulkCSVHeaderDefinition {
	typ := SourceSftpBulkCSVHeaderDefinitionTypeSourceSftpBulkFromCSV

	return SourceSftpBulkCSVHeaderDefinition{
		SourceSftpBulkFromCSV: &sourceSftpBulkFromCSV,
		Type:                  typ,
	}
}

func CreateSourceSftpBulkCSVHeaderDefinitionSourceSftpBulkAutogenerated(sourceSftpBulkAutogenerated SourceSftpBulkAutogenerated) SourceSftpBulkCSVHeaderDefinition {
	typ := SourceSftpBulkCSVHeaderDefinitionTypeSourceSftpBulkAutogenerated

	return SourceSftpBulkCSVHeaderDefinition{
		SourceSftpBulkAutogenerated: &sourceSftpBulkAutogenerated,
		Type:                        typ,
	}
}

func CreateSourceSftpBulkCSVHeaderDefinitionSourceSftpBulkUserProvided(sourceSftpBulkUserProvided SourceSftpBulkUserProvided) SourceSftpBulkCSVHeaderDefinition {
	typ := SourceSftpBulkCSVHeaderDefinitionTypeSourceSftpBulkUserProvided

	return SourceSftpBulkCSVHeaderDefinition{
		SourceSftpBulkUserProvided: &sourceSftpBulkUserProvided,
		Type:                       typ,
	}
}

func (u *SourceSftpBulkCSVHeaderDefinition) UnmarshalJSON(data []byte) error {

	var candidates []utils.UnionCandidate

	// Collect all valid candidates
	var sourceSftpBulkFromCSV SourceSftpBulkFromCSV = SourceSftpBulkFromCSV{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkFromCSV, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkCSVHeaderDefinitionTypeSourceSftpBulkFromCSV,
			Value: &sourceSftpBulkFromCSV,
		})
	}

	var sourceSftpBulkAutogenerated SourceSftpBulkAutogenerated = SourceSftpBulkAutogenerated{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkAutogenerated, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkCSVHeaderDefinitionTypeSourceSftpBulkAutogenerated,
			Value: &sourceSftpBulkAutogenerated,
		})
	}

	var sourceSftpBulkUserProvided SourceSftpBulkUserProvided = SourceSftpBulkUserProvided{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkUserProvided, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkCSVHeaderDefinitionTypeSourceSftpBulkUserProvided,
			Value: &sourceSftpBulkUserProvided,
		})
	}

	if len(candidates) == 0 {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkCSVHeaderDefinition", string(data))
	}

	// Pick the best candidate using multi-stage filtering
	best := utils.PickBestUnionCandidate(candidates, data)
	if best == nil {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkCSVHeaderDefinition", string(data))
	}

	// Set the union type and value based on the best candidate
	u.Type = best.Type.(SourceSftpBulkCSVHeaderDefinitionType)
	switch best.Type {
	case SourceSftpBulkCSVHeaderDefinitionTypeSourceSftpBulkFromCSV:
		u.SourceSftpBulkFromCSV = best.Value.(*SourceSftpBulkFromCSV)
		return nil
	case SourceSftpBulkCSVHeaderDefinitionTypeSourceSftpBulkAutogenerated:
		u.SourceSftpBulkAutogenerated = best.Value.(*SourceSftpBulkAutogenerated)
		return nil
	case SourceSftpBulkCSVHeaderDefinitionTypeSourceSftpBulkUserProvided:
		u.SourceSftpBulkUserProvided = best.Value.(*SourceSftpBulkUserProvided)
		return nil
	}

	return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkCSVHeaderDefinition", string(data))
}

func (u SourceSftpBulkCSVHeaderDefinition) MarshalJSON() ([]byte, error) {
	if u.SourceSftpBulkFromCSV != nil {
		return utils.MarshalJSON(u.SourceSftpBulkFromCSV, "", true)
	}

	if u.SourceSftpBulkAutogenerated != nil {
		return utils.MarshalJSON(u.SourceSftpBulkAutogenerated, "", true)
	}

	if u.SourceSftpBulkUserProvided != nil {
		return utils.MarshalJSON(u.SourceSftpBulkUserProvided, "", true)
	}

	return nil, errors.New("could not marshal union type SourceSftpBulkCSVHeaderDefinition: all fields are null")
}

// SourceSftpBulkInferenceType - How to infer the types of the columns. If none, inference default to strings.
type SourceSftpBulkInferenceType string

const (
	SourceSftpBulkInferenceTypeNone               SourceSftpBulkInferenceType = "None"
	SourceSftpBulkInferenceTypePrimitiveTypesOnly SourceSftpBulkInferenceType = "Primitive Types Only"
)

func (e SourceSftpBulkInferenceType) ToPointer() *SourceSftpBulkInferenceType {
	return &e
}
func (e *SourceSftpBulkInferenceType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "None":
		fallthrough
	case "Primitive Types Only":
		*e = SourceSftpBulkInferenceType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceSftpBulkInferenceType: %v", v)
	}
}

type SourceSftpBulkCSVFormat struct {
	// The character delimiting individual cells in the CSV data. This may only be a 1-character string. For tab-delimited data enter '\t'.
	Delimiter *string `default:"," json:"delimiter"`
	// Whether two quotes in a quoted CSV value denote a single quote in the data.
	DoubleQuote *bool `default:"true" json:"double_quote"`
	// The character encoding of the CSV data. Leave blank to default to <strong>UTF8</strong>. See <a href="https://docs.python.org/3/library/codecs.html#standard-encodings" target="_blank">list of python encodings</a> for allowable options.
	Encoding *string `default:"utf8" json:"encoding"`
	// The character used for escaping special characters. To disallow escaping, leave this field blank.
	EscapeChar *string `json:"escape_char,omitempty"`
	// A set of case-sensitive strings that should be interpreted as false values.
	FalseValues []string `json:"false_values,omitempty"`
	filetype    *string  `const:"csv" json:"filetype"`
	// How headers will be defined. `User Provided` assumes the CSV does not have a header row and uses the headers provided and `Autogenerated` assumes the CSV does not have a header row and the CDK will generate headers using for `f{i}` where `i` is the index starting from 0. Else, the default behavior is to use the header from the CSV file. If a user wants to autogenerate or provide column names for a CSV having headers, they can skip rows.
	HeaderDefinition *SourceSftpBulkCSVHeaderDefinition `json:"header_definition,omitempty"`
	// Whether to ignore errors that occur when the number of fields in the CSV does not match the number of columns in the schema.
	IgnoreErrorsOnFieldsMismatch *bool `default:"false" json:"ignore_errors_on_fields_mismatch"`
	// How to infer the types of the columns. If none, inference default to strings.
	InferenceType *SourceSftpBulkInferenceType `json:"inference_type,omitempty"`
	// A set of case-sensitive strings that should be interpreted as null values. For example, if the value 'NA' should be interpreted as null, enter 'NA' in this field.
	NullValues []string `json:"null_values,omitempty"`
	// The character used for quoting CSV values. To disallow quoting, make this field blank.
	QuoteChar *string `default:"\"" json:"quote_char"`
	// The number of rows to skip after the header row.
	SkipRowsAfterHeader *int64 `default:"0" json:"skip_rows_after_header"`
	// The number of rows to skip before the header row. For example, if the header row is on the 3rd row, enter 2 in this field.
	SkipRowsBeforeHeader *int64 `default:"0" json:"skip_rows_before_header"`
	// Whether strings can be interpreted as null values. If true, strings that match the null_values set will be interpreted as null. If false, strings that match the null_values set will be interpreted as the string itself.
	StringsCanBeNull *bool `default:"true" json:"strings_can_be_null"`
	// A set of case-sensitive strings that should be interpreted as true values.
	TrueValues []string `json:"true_values,omitempty"`
}

func (s SourceSftpBulkCSVFormat) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkCSVFormat) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkCSVFormat) GetDelimiter() *string {
	if s == nil {
		return nil
	}
	return s.Delimiter
}

func (s *SourceSftpBulkCSVFormat) GetDoubleQuote() *bool {
	if s == nil {
		return nil
	}
	return s.DoubleQuote
}

func (s *SourceSftpBulkCSVFormat) GetEncoding() *string {
	if s == nil {
		return nil
	}
	return s.Encoding
}

func (s *SourceSftpBulkCSVFormat) GetEscapeChar() *string {
	if s == nil {
		return nil
	}
	return s.EscapeChar
}

func (s *SourceSftpBulkCSVFormat) GetFalseValues() []string {
	if s == nil {
		return nil
	}
	return s.FalseValues
}

func (s *SourceSftpBulkCSVFormat) GetFiletype() *string {
	return types.Pointer("csv")
}

func (s *SourceSftpBulkCSVFormat) GetHeaderDefinition() *SourceSftpBulkCSVHeaderDefinition {
	if s == nil {
		return nil
	}
	return s.HeaderDefinition
}

func (s *SourceSftpBulkCSVFormat) GetIgnoreErrorsOnFieldsMismatch() *bool {
	if s == nil {
		return nil
	}
	return s.IgnoreErrorsOnFieldsMismatch
}

func (s *SourceSftpBulkCSVFormat) GetInferenceType() *SourceSftpBulkInferenceType {
	if s == nil {
		return nil
	}
	return s.InferenceType
}

func (s *SourceSftpBulkCSVFormat) GetNullValues() []string {
	if s == nil {
		return nil
	}
	return s.NullValues
}

func (s *SourceSftpBulkCSVFormat) GetQuoteChar() *string {
	if s == nil {
		return nil
	}
	return s.QuoteChar
}

func (s *SourceSftpBulkCSVFormat) GetSkipRowsAfterHeader() *int64 {
	if s == nil {
		return nil
	}
	return s.SkipRowsAfterHeader
}

func (s *SourceSftpBulkCSVFormat) GetSkipRowsBeforeHeader() *int64 {
	if s == nil {
		return nil
	}
	return s.SkipRowsBeforeHeader
}

func (s *SourceSftpBulkCSVFormat) GetStringsCanBeNull() *bool {
	if s == nil {
		return nil
	}
	return s.StringsCanBeNull
}

func (s *SourceSftpBulkCSVFormat) GetTrueValues() []string {
	if s == nil {
		return nil
	}
	return s.TrueValues
}

type SourceSftpBulkAvroFormat struct {
	// Whether to convert double fields to strings. This is recommended if you have decimal numbers with a high degree of precision because there can be a loss precision when handling floating point numbers.
	DoubleAsString *bool   `default:"false" json:"double_as_string"`
	filetype       *string `const:"avro" json:"filetype"`
}

func (s SourceSftpBulkAvroFormat) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkAvroFormat) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkAvroFormat) GetDoubleAsString() *bool {
	if s == nil {
		return nil
	}
	return s.DoubleAsString
}

func (s *SourceSftpBulkAvroFormat) GetFiletype() *string {
	return types.Pointer("avro")
}

type SourceSftpBulkFormatType string

const (
	SourceSftpBulkFormatTypeSourceSftpBulkAvroFormat                 SourceSftpBulkFormatType = "source-sftp-bulk_Avro Format"
	SourceSftpBulkFormatTypeSourceSftpBulkCSVFormat                  SourceSftpBulkFormatType = "source-sftp-bulk_CSV Format"
	SourceSftpBulkFormatTypeSourceSftpBulkJsonlFormat                SourceSftpBulkFormatType = "source-sftp-bulk_Jsonl Format"
	SourceSftpBulkFormatTypeSourceSftpBulkParquetFormat              SourceSftpBulkFormatType = "source-sftp-bulk_Parquet Format"
	SourceSftpBulkFormatTypeSourceSftpBulkUnstructuredDocumentFormat SourceSftpBulkFormatType = "source-sftp-bulk_Unstructured Document Format"
	SourceSftpBulkFormatTypeSourceSftpBulkExcelFormat                SourceSftpBulkFormatType = "source-sftp-bulk_Excel Format"
)

// SourceSftpBulkFormat - The configuration options that are used to alter how to read incoming files that deviate from the standard formatting.
type SourceSftpBulkFormat struct {
	SourceSftpBulkAvroFormat                 *SourceSftpBulkAvroFormat                 `queryParam:"inline" union:"member"`
	SourceSftpBulkCSVFormat                  *SourceSftpBulkCSVFormat                  `queryParam:"inline" union:"member"`
	SourceSftpBulkJsonlFormat                *SourceSftpBulkJsonlFormat                `queryParam:"inline" union:"member"`
	SourceSftpBulkParquetFormat              *SourceSftpBulkParquetFormat              `queryParam:"inline" union:"member"`
	SourceSftpBulkUnstructuredDocumentFormat *SourceSftpBulkUnstructuredDocumentFormat `queryParam:"inline" union:"member"`
	SourceSftpBulkExcelFormat                *SourceSftpBulkExcelFormat                `queryParam:"inline" union:"member"`

	Type SourceSftpBulkFormatType
}

func CreateSourceSftpBulkFormatSourceSftpBulkAvroFormat(sourceSftpBulkAvroFormat SourceSftpBulkAvroFormat) SourceSftpBulkFormat {
	typ := SourceSftpBulkFormatTypeSourceSftpBulkAvroFormat

	return SourceSftpBulkFormat{
		SourceSftpBulkAvroFormat: &sourceSftpBulkAvroFormat,
		Type:                     typ,
	}
}

func CreateSourceSftpBulkFormatSourceSftpBulkCSVFormat(sourceSftpBulkCSVFormat SourceSftpBulkCSVFormat) SourceSftpBulkFormat {
	typ := SourceSftpBulkFormatTypeSourceSftpBulkCSVFormat

	return SourceSftpBulkFormat{
		SourceSftpBulkCSVFormat: &sourceSftpBulkCSVFormat,
		Type:                    typ,
	}
}

func CreateSourceSftpBulkFormatSourceSftpBulkJsonlFormat(sourceSftpBulkJsonlFormat SourceSftpBulkJsonlFormat) SourceSftpBulkFormat {
	typ := SourceSftpBulkFormatTypeSourceSftpBulkJsonlFormat

	return SourceSftpBulkFormat{
		SourceSftpBulkJsonlFormat: &sourceSftpBulkJsonlFormat,
		Type:                      typ,
	}
}

func CreateSourceSftpBulkFormatSourceSftpBulkParquetFormat(sourceSftpBulkParquetFormat SourceSftpBulkParquetFormat) SourceSftpBulkFormat {
	typ := SourceSftpBulkFormatTypeSourceSftpBulkParquetFormat

	return SourceSftpBulkFormat{
		SourceSftpBulkParquetFormat: &sourceSftpBulkParquetFormat,
		Type:                        typ,
	}
}

func CreateSourceSftpBulkFormatSourceSftpBulkUnstructuredDocumentFormat(sourceSftpBulkUnstructuredDocumentFormat SourceSftpBulkUnstructuredDocumentFormat) SourceSftpBulkFormat {
	typ := SourceSftpBulkFormatTypeSourceSftpBulkUnstructuredDocumentFormat

	return SourceSftpBulkFormat{
		SourceSftpBulkUnstructuredDocumentFormat: &sourceSftpBulkUnstructuredDocumentFormat,
		Type:                                     typ,
	}
}

func CreateSourceSftpBulkFormatSourceSftpBulkExcelFormat(sourceSftpBulkExcelFormat SourceSftpBulkExcelFormat) SourceSftpBulkFormat {
	typ := SourceSftpBulkFormatTypeSourceSftpBulkExcelFormat

	return SourceSftpBulkFormat{
		SourceSftpBulkExcelFormat: &sourceSftpBulkExcelFormat,
		Type:                      typ,
	}
}

func (u *SourceSftpBulkFormat) UnmarshalJSON(data []byte) error {

	var candidates []utils.UnionCandidate

	// Collect all valid candidates
	var sourceSftpBulkAvroFormat SourceSftpBulkAvroFormat = SourceSftpBulkAvroFormat{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkAvroFormat, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkFormatTypeSourceSftpBulkAvroFormat,
			Value: &sourceSftpBulkAvroFormat,
		})
	}

	var sourceSftpBulkCSVFormat SourceSftpBulkCSVFormat = SourceSftpBulkCSVFormat{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkCSVFormat, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkFormatTypeSourceSftpBulkCSVFormat,
			Value: &sourceSftpBulkCSVFormat,
		})
	}

	var sourceSftpBulkJsonlFormat SourceSftpBulkJsonlFormat = SourceSftpBulkJsonlFormat{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkJsonlFormat, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkFormatTypeSourceSftpBulkJsonlFormat,
			Value: &sourceSftpBulkJsonlFormat,
		})
	}

	var sourceSftpBulkParquetFormat SourceSftpBulkParquetFormat = SourceSftpBulkParquetFormat{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkParquetFormat, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkFormatTypeSourceSftpBulkParquetFormat,
			Value: &sourceSftpBulkParquetFormat,
		})
	}

	var sourceSftpBulkUnstructuredDocumentFormat SourceSftpBulkUnstructuredDocumentFormat = SourceSftpBulkUnstructuredDocumentFormat{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkUnstructuredDocumentFormat, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkFormatTypeSourceSftpBulkUnstructuredDocumentFormat,
			Value: &sourceSftpBulkUnstructuredDocumentFormat,
		})
	}

	var sourceSftpBulkExcelFormat SourceSftpBulkExcelFormat = SourceSftpBulkExcelFormat{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkExcelFormat, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkFormatTypeSourceSftpBulkExcelFormat,
			Value: &sourceSftpBulkExcelFormat,
		})
	}

	if len(candidates) == 0 {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkFormat", string(data))
	}

	// Pick the best candidate using multi-stage filtering
	best := utils.PickBestUnionCandidate(candidates, data)
	if best == nil {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkFormat", string(data))
	}

	// Set the union type and value based on the best candidate
	u.Type = best.Type.(SourceSftpBulkFormatType)
	switch best.Type {
	case SourceSftpBulkFormatTypeSourceSftpBulkAvroFormat:
		u.SourceSftpBulkAvroFormat = best.Value.(*SourceSftpBulkAvroFormat)
		return nil
	case SourceSftpBulkFormatTypeSourceSftpBulkCSVFormat:
		u.SourceSftpBulkCSVFormat = best.Value.(*SourceSftpBulkCSVFormat)
		return nil
	case SourceSftpBulkFormatTypeSourceSftpBulkJsonlFormat:
		u.SourceSftpBulkJsonlFormat = best.Value.(*SourceSftpBulkJsonlFormat)
		return nil
	case SourceSftpBulkFormatTypeSourceSftpBulkParquetFormat:
		u.SourceSftpBulkParquetFormat = best.Value.(*SourceSftpBulkParquetFormat)
		return nil
	case SourceSftpBulkFormatTypeSourceSftpBulkUnstructuredDocumentFormat:
		u.SourceSftpBulkUnstructuredDocumentFormat = best.Value.(*SourceSftpBulkUnstructuredDocumentFormat)
		return nil
	case SourceSftpBulkFormatTypeSourceSftpBulkExcelFormat:
		u.SourceSftpBulkExcelFormat = best.Value.(*SourceSftpBulkExcelFormat)
		return nil
	}

	return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkFormat", string(data))
}

func (u SourceSftpBulkFormat) MarshalJSON() ([]byte, error) {
	if u.SourceSftpBulkAvroFormat != nil {
		return utils.MarshalJSON(u.SourceSftpBulkAvroFormat, "", true)
	}

	if u.SourceSftpBulkCSVFormat != nil {
		return utils.MarshalJSON(u.SourceSftpBulkCSVFormat, "", true)
	}

	if u.SourceSftpBulkJsonlFormat != nil {
		return utils.MarshalJSON(u.SourceSftpBulkJsonlFormat, "", true)
	}

	if u.SourceSftpBulkParquetFormat != nil {
		return utils.MarshalJSON(u.SourceSftpBulkParquetFormat, "", true)
	}

	if u.SourceSftpBulkUnstructuredDocumentFormat != nil {
		return utils.MarshalJSON(u.SourceSftpBulkUnstructuredDocumentFormat, "", true)
	}

	if u.SourceSftpBulkExcelFormat != nil {
		return utils.MarshalJSON(u.SourceSftpBulkExcelFormat, "", true)
	}

	return nil, errors.New("could not marshal union type SourceSftpBulkFormat: all fields are null")
}

// SourceSftpBulkValidationPolicy - The name of the validation policy that dictates sync behavior when a record does not adhere to the stream schema.
type SourceSftpBulkValidationPolicy string

const (
	SourceSftpBulkValidationPolicyEmitRecord      SourceSftpBulkValidationPolicy = "Emit Record"
	SourceSftpBulkValidationPolicySkipRecord      SourceSftpBulkValidationPolicy = "Skip Record"
	SourceSftpBulkValidationPolicyWaitForDiscover SourceSftpBulkValidationPolicy = "Wait for Discover"
)

func (e SourceSftpBulkValidationPolicy) ToPointer() *SourceSftpBulkValidationPolicy {
	return &e
}
func (e *SourceSftpBulkValidationPolicy) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "Emit Record":
		fallthrough
	case "Skip Record":
		fallthrough
	case "Wait for Discover":
		*e = SourceSftpBulkValidationPolicy(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceSftpBulkValidationPolicy: %v", v)
	}
}

type SourceSftpBulkFileBasedStreamConfig struct {
	// When the state history of the file store is full, syncs will only read files that were last modified in the provided day range.
	DaysToSyncIfHistoryIsFull *int64 `default:"3" json:"days_to_sync_if_history_is_full"`
	// The configuration options that are used to alter how to read incoming files that deviate from the standard formatting.
	Format SourceSftpBulkFormat `json:"format"`
	// The pattern used to specify which files should be selected from the file system. For more information on glob pattern matching look <a href="https://en.wikipedia.org/wiki/Glob_(programming)">here</a>.
	Globs []string `json:"globs,omitempty"`
	// The schema that will be used to validate records extracted from the file. This will override the stream schema that is auto-detected from incoming files.
	InputSchema *string `json:"input_schema,omitempty"`
	// The path prefix configured in v3 versions of the S3 connector. This option is deprecated in favor of a single glob.
	LegacyPrefix *string `json:"legacy_prefix,omitempty"`
	// The name of the stream.
	Name string `json:"name"`
	// The column or columns (for a composite key) that serves as the unique identifier of a record. If empty, the primary key will default to the parser's default primary key.
	PrimaryKey *string `json:"primary_key,omitempty"`
	// The number of resent files which will be used to discover the schema for this stream.
	RecentNFilesToReadForSchemaDiscovery *int64 `json:"recent_n_files_to_read_for_schema_discovery,omitempty"`
	// When enabled, syncs will not validate or structure records against the stream's schema.
	Schemaless *bool `default:"false" json:"schemaless"`
	// When enabled, the source will use the first found file for schema discovery. Helps to avoid long discovery step.
	UseFirstFoundFileForSchemaDiscovery *bool `default:"false" json:"use_first_found_file_for_schema_discovery"`
	// The name of the validation policy that dictates sync behavior when a record does not adhere to the stream schema.
	ValidationPolicy *SourceSftpBulkValidationPolicy `default:"Emit Record" json:"validation_policy"`
}

func (s SourceSftpBulkFileBasedStreamConfig) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkFileBasedStreamConfig) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkFileBasedStreamConfig) GetDaysToSyncIfHistoryIsFull() *int64 {
	if s == nil {
		return nil
	}
	return s.DaysToSyncIfHistoryIsFull
}

func (s *SourceSftpBulkFileBasedStreamConfig) GetFormat() SourceSftpBulkFormat {
	if s == nil {
		return SourceSftpBulkFormat{}
	}
	return s.Format
}

func (s *SourceSftpBulkFileBasedStreamConfig) GetGlobs() []string {
	if s == nil {
		return nil
	}
	return s.Globs
}

func (s *SourceSftpBulkFileBasedStreamConfig) GetInputSchema() *string {
	if s == nil {
		return nil
	}
	return s.InputSchema
}

func (s *SourceSftpBulkFileBasedStreamConfig) GetLegacyPrefix() *string {
	if s == nil {
		return nil
	}
	return s.LegacyPrefix
}

func (s *SourceSftpBulkFileBasedStreamConfig) GetName() string {
	if s == nil {
		return ""
	}
	return s.Name
}

func (s *SourceSftpBulkFileBasedStreamConfig) GetPrimaryKey() *string {
	if s == nil {
		return nil
	}
	return s.PrimaryKey
}

func (s *SourceSftpBulkFileBasedStreamConfig) GetRecentNFilesToReadForSchemaDiscovery() *int64 {
	if s == nil {
		return nil
	}
	return s.RecentNFilesToReadForSchemaDiscovery
}

func (s *SourceSftpBulkFileBasedStreamConfig) GetSchemaless() *bool {
	if s == nil {
		return nil
	}
	return s.Schemaless
}

func (s *SourceSftpBulkFileBasedStreamConfig) GetUseFirstFoundFileForSchemaDiscovery() *bool {
	if s == nil {
		return nil
	}
	return s.UseFirstFoundFileForSchemaDiscovery
}

func (s *SourceSftpBulkFileBasedStreamConfig) GetValidationPolicy() *SourceSftpBulkValidationPolicy {
	if s == nil {
		return nil
	}
	return s.ValidationPolicy
}

type SourceSftpBulkSourceType string

const (
	SourceSftpBulkSourceTypeSftpBulk SourceSftpBulkSourceType = "sftp-bulk"
)

func (e SourceSftpBulkSourceType) ToPointer() *SourceSftpBulkSourceType {
	return &e
}
func (e *SourceSftpBulkSourceType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "sftp-bulk":
		*e = SourceSftpBulkSourceType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceSftpBulkSourceType: %v", v)
	}
}

// SourceSftpBulk - Used during spec; allows the developer to configure the cloud provider specific options that are needed when users configure a file-based source.
type SourceSftpBulk struct {
	// Credentials for connecting to the SFTP Server
	Credentials    SourceSftpBulkAuthentication  `json:"credentials"`
	DeliveryMethod *SourceSftpBulkDeliveryMethod `json:"delivery_method,omitempty"`
	// The directory to search files for sync
	FolderPath *string `default:"/" json:"folder_path"`
	// The server host address
	Host string `json:"host"`
	// The server port
	Port *int64 `default:"22" json:"port"`
	// UTC date and time in the format 2017-01-25T00:00:00.000000Z. Any file modified before this date will not be replicated.
	StartDate *time.Time `json:"start_date,omitempty"`
	// Each instance of this configuration defines a <a href="https://docs.airbyte.com/cloud/core-concepts#stream">stream</a>. Use this to define which files belong in the stream, their format, and how they should be parsed and validated. When sending data to warehouse destination such as Snowflake or BigQuery, each stream is a separate table.
	Streams []SourceSftpBulkFileBasedStreamConfig `json:"streams"`
	// The server user
	Username   string                    `json:"username"`
	sourceType *SourceSftpBulkSourceType `const:"sftp-bulk" json:"sourceType"`
}

func (s SourceSftpBulk) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulk) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulk) GetCredentials() SourceSftpBulkAuthentication {
	if s == nil {
		return SourceSftpBulkAuthentication{}
	}
	return s.Credentials
}

func (s *SourceSftpBulk) GetDeliveryMethod() *SourceSftpBulkDeliveryMethod {
	if s == nil {
		return nil
	}
	return s.DeliveryMethod
}

func (s *SourceSftpBulk) GetFolderPath() *string {
	if s == nil {
		return nil
	}
	return s.FolderPath
}

func (s *SourceSftpBulk) GetHost() string {
	if s == nil {
		return ""
	}
	return s.Host
}

func (s *SourceSftpBulk) GetPort() *int64 {
	if s == nil {
		return nil
	}
	return s.Port
}

func (s *SourceSftpBulk) GetStartDate() *time.Time {
	if s == nil {
		return nil
	}
	return s.StartDate
}

func (s *SourceSftpBulk) GetStreams() []SourceSftpBulkFileBasedStreamConfig {
	if s == nil {
		return []SourceSftpBulkFileBasedStreamConfig{}
	}
	return s.Streams
}

func (s *SourceSftpBulk) GetUsername() string {
	if s == nil {
		return ""
	}
	return s.Username
}

func (s *SourceSftpBulk) GetSourceType() *SourceSftpBulkSourceType {
	return SourceSftpBulkSourceTypeSftpBulk.ToPointer()
}
