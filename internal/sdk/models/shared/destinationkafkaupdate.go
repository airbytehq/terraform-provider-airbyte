// Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.

package shared

import (
	"encoding/json"
	"errors"
	"fmt"
	"github.com/airbytehq/terraform-provider-airbyte/internal/sdk/internal/utils"
)

// DestinationKafkaUpdateACKs - The number of acknowledgments the producer requires the leader to have received before considering a request complete. This controls the  durability of records that are sent.
type DestinationKafkaUpdateACKs string

const (
	DestinationKafkaUpdateACKsZero DestinationKafkaUpdateACKs = "0"
	DestinationKafkaUpdateACKsOne  DestinationKafkaUpdateACKs = "1"
	DestinationKafkaUpdateACKsAll  DestinationKafkaUpdateACKs = "all"
)

func (e DestinationKafkaUpdateACKs) ToPointer() *DestinationKafkaUpdateACKs {
	return &e
}
func (e *DestinationKafkaUpdateACKs) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "0":
		fallthrough
	case "1":
		fallthrough
	case "all":
		*e = DestinationKafkaUpdateACKs(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationKafkaUpdateACKs: %v", v)
	}
}

// DestinationKafkaUpdateClientDNSLookup - Controls how the client uses DNS lookups. If set to use_all_dns_ips, connect to each returned IP address in sequence until a successful connection is established. After a disconnection, the next IP is used. Once all IPs have been used once, the client resolves the IP(s) from the hostname again. If set to resolve_canonical_bootstrap_servers_only, resolve each bootstrap address into a list of canonical names. After the bootstrap phase, this behaves the same as use_all_dns_ips. If set to default (deprecated), attempt to connect to the first IP address returned by the lookup, even if the lookup returns multiple IP addresses.
type DestinationKafkaUpdateClientDNSLookup string

const (
	DestinationKafkaUpdateClientDNSLookupDefault                              DestinationKafkaUpdateClientDNSLookup = "default"
	DestinationKafkaUpdateClientDNSLookupUseAllDNSIps                         DestinationKafkaUpdateClientDNSLookup = "use_all_dns_ips"
	DestinationKafkaUpdateClientDNSLookupResolveCanonicalBootstrapServersOnly DestinationKafkaUpdateClientDNSLookup = "resolve_canonical_bootstrap_servers_only"
)

func (e DestinationKafkaUpdateClientDNSLookup) ToPointer() *DestinationKafkaUpdateClientDNSLookup {
	return &e
}
func (e *DestinationKafkaUpdateClientDNSLookup) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "default":
		fallthrough
	case "use_all_dns_ips":
		fallthrough
	case "resolve_canonical_bootstrap_servers_only":
		*e = DestinationKafkaUpdateClientDNSLookup(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationKafkaUpdateClientDNSLookup: %v", v)
	}
}

// DestinationKafkaUpdateCompressionType - The compression type for all data generated by the producer.
type DestinationKafkaUpdateCompressionType string

const (
	DestinationKafkaUpdateCompressionTypeNone   DestinationKafkaUpdateCompressionType = "none"
	DestinationKafkaUpdateCompressionTypeGzip   DestinationKafkaUpdateCompressionType = "gzip"
	DestinationKafkaUpdateCompressionTypeSnappy DestinationKafkaUpdateCompressionType = "snappy"
	DestinationKafkaUpdateCompressionTypeLz4    DestinationKafkaUpdateCompressionType = "lz4"
	DestinationKafkaUpdateCompressionTypeZstd   DestinationKafkaUpdateCompressionType = "zstd"
)

func (e DestinationKafkaUpdateCompressionType) ToPointer() *DestinationKafkaUpdateCompressionType {
	return &e
}
func (e *DestinationKafkaUpdateCompressionType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		fallthrough
	case "snappy":
		fallthrough
	case "lz4":
		fallthrough
	case "zstd":
		*e = DestinationKafkaUpdateCompressionType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationKafkaUpdateCompressionType: %v", v)
	}
}

// DestinationKafkaUpdateSchemasSASLMechanism - SASL mechanism used for client connections. This may be any mechanism for which a security provider is available.
type DestinationKafkaUpdateSchemasSASLMechanism string

const (
	DestinationKafkaUpdateSchemasSASLMechanismGssapi      DestinationKafkaUpdateSchemasSASLMechanism = "GSSAPI"
	DestinationKafkaUpdateSchemasSASLMechanismOauthbearer DestinationKafkaUpdateSchemasSASLMechanism = "OAUTHBEARER"
	DestinationKafkaUpdateSchemasSASLMechanismScramSha256 DestinationKafkaUpdateSchemasSASLMechanism = "SCRAM-SHA-256"
	DestinationKafkaUpdateSchemasSASLMechanismScramSha512 DestinationKafkaUpdateSchemasSASLMechanism = "SCRAM-SHA-512"
	DestinationKafkaUpdateSchemasSASLMechanismPlain       DestinationKafkaUpdateSchemasSASLMechanism = "PLAIN"
)

func (e DestinationKafkaUpdateSchemasSASLMechanism) ToPointer() *DestinationKafkaUpdateSchemasSASLMechanism {
	return &e
}
func (e *DestinationKafkaUpdateSchemasSASLMechanism) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "GSSAPI":
		fallthrough
	case "OAUTHBEARER":
		fallthrough
	case "SCRAM-SHA-256":
		fallthrough
	case "SCRAM-SHA-512":
		fallthrough
	case "PLAIN":
		*e = DestinationKafkaUpdateSchemasSASLMechanism(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationKafkaUpdateSchemasSASLMechanism: %v", v)
	}
}

type DestinationKafkaUpdateSchemasProtocolSecurityProtocol string

const (
	DestinationKafkaUpdateSchemasProtocolSecurityProtocolSaslSsl DestinationKafkaUpdateSchemasProtocolSecurityProtocol = "SASL_SSL"
)

func (e DestinationKafkaUpdateSchemasProtocolSecurityProtocol) ToPointer() *DestinationKafkaUpdateSchemasProtocolSecurityProtocol {
	return &e
}
func (e *DestinationKafkaUpdateSchemasProtocolSecurityProtocol) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "SASL_SSL":
		*e = DestinationKafkaUpdateSchemasProtocolSecurityProtocol(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationKafkaUpdateSchemasProtocolSecurityProtocol: %v", v)
	}
}

type DestinationKafkaUpdateSASLSSL struct {
	// JAAS login context parameters for SASL connections in the format used by JAAS configuration files.
	SaslJaasConfig *string `default:"" json:"sasl_jaas_config"`
	// SASL mechanism used for client connections. This may be any mechanism for which a security provider is available.
	SaslMechanism    *DestinationKafkaUpdateSchemasSASLMechanism            `default:"GSSAPI" json:"sasl_mechanism"`
	SecurityProtocol *DestinationKafkaUpdateSchemasProtocolSecurityProtocol `default:"SASL_SSL" json:"security_protocol"`
}

func (d DestinationKafkaUpdateSASLSSL) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(d, "", false)
}

func (d *DestinationKafkaUpdateSASLSSL) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &d, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (d *DestinationKafkaUpdateSASLSSL) GetSaslJaasConfig() *string {
	if d == nil {
		return nil
	}
	return d.SaslJaasConfig
}

func (d *DestinationKafkaUpdateSASLSSL) GetSaslMechanism() *DestinationKafkaUpdateSchemasSASLMechanism {
	if d == nil {
		return nil
	}
	return d.SaslMechanism
}

func (d *DestinationKafkaUpdateSASLSSL) GetSecurityProtocol() *DestinationKafkaUpdateSchemasProtocolSecurityProtocol {
	if d == nil {
		return nil
	}
	return d.SecurityProtocol
}

// DestinationKafkaUpdateSASLMechanism - SASL mechanism used for client connections. This may be any mechanism for which a security provider is available.
type DestinationKafkaUpdateSASLMechanism string

const (
	DestinationKafkaUpdateSASLMechanismGssapi      DestinationKafkaUpdateSASLMechanism = "GSSAPI"
	DestinationKafkaUpdateSASLMechanismOauthbearer DestinationKafkaUpdateSASLMechanism = "OAUTHBEARER"
	DestinationKafkaUpdateSASLMechanismScramSha256 DestinationKafkaUpdateSASLMechanism = "SCRAM-SHA-256"
	DestinationKafkaUpdateSASLMechanismScramSha512 DestinationKafkaUpdateSASLMechanism = "SCRAM-SHA-512"
	DestinationKafkaUpdateSASLMechanismPlain       DestinationKafkaUpdateSASLMechanism = "PLAIN"
)

func (e DestinationKafkaUpdateSASLMechanism) ToPointer() *DestinationKafkaUpdateSASLMechanism {
	return &e
}
func (e *DestinationKafkaUpdateSASLMechanism) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "GSSAPI":
		fallthrough
	case "OAUTHBEARER":
		fallthrough
	case "SCRAM-SHA-256":
		fallthrough
	case "SCRAM-SHA-512":
		fallthrough
	case "PLAIN":
		*e = DestinationKafkaUpdateSASLMechanism(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationKafkaUpdateSASLMechanism: %v", v)
	}
}

type DestinationKafkaUpdateSchemasSecurityProtocol string

const (
	DestinationKafkaUpdateSchemasSecurityProtocolSaslPlaintext DestinationKafkaUpdateSchemasSecurityProtocol = "SASL_PLAINTEXT"
)

func (e DestinationKafkaUpdateSchemasSecurityProtocol) ToPointer() *DestinationKafkaUpdateSchemasSecurityProtocol {
	return &e
}
func (e *DestinationKafkaUpdateSchemasSecurityProtocol) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "SASL_PLAINTEXT":
		*e = DestinationKafkaUpdateSchemasSecurityProtocol(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationKafkaUpdateSchemasSecurityProtocol: %v", v)
	}
}

type DestinationKafkaUpdateSASLPLAINTEXT struct {
	// JAAS login context parameters for SASL connections in the format used by JAAS configuration files.
	SaslJaasConfig *string `default:"" json:"sasl_jaas_config"`
	// SASL mechanism used for client connections. This may be any mechanism for which a security provider is available.
	SaslMechanism    *DestinationKafkaUpdateSASLMechanism           `default:"PLAIN" json:"sasl_mechanism"`
	SecurityProtocol *DestinationKafkaUpdateSchemasSecurityProtocol `default:"SASL_PLAINTEXT" json:"security_protocol"`
}

func (d DestinationKafkaUpdateSASLPLAINTEXT) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(d, "", false)
}

func (d *DestinationKafkaUpdateSASLPLAINTEXT) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &d, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (d *DestinationKafkaUpdateSASLPLAINTEXT) GetSaslJaasConfig() *string {
	if d == nil {
		return nil
	}
	return d.SaslJaasConfig
}

func (d *DestinationKafkaUpdateSASLPLAINTEXT) GetSaslMechanism() *DestinationKafkaUpdateSASLMechanism {
	if d == nil {
		return nil
	}
	return d.SaslMechanism
}

func (d *DestinationKafkaUpdateSASLPLAINTEXT) GetSecurityProtocol() *DestinationKafkaUpdateSchemasSecurityProtocol {
	if d == nil {
		return nil
	}
	return d.SecurityProtocol
}

type DestinationKafkaUpdateSecurityProtocol string

const (
	DestinationKafkaUpdateSecurityProtocolPlaintext DestinationKafkaUpdateSecurityProtocol = "PLAINTEXT"
)

func (e DestinationKafkaUpdateSecurityProtocol) ToPointer() *DestinationKafkaUpdateSecurityProtocol {
	return &e
}
func (e *DestinationKafkaUpdateSecurityProtocol) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "PLAINTEXT":
		*e = DestinationKafkaUpdateSecurityProtocol(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationKafkaUpdateSecurityProtocol: %v", v)
	}
}

type DestinationKafkaUpdatePLAINTEXT struct {
	SecurityProtocol *DestinationKafkaUpdateSecurityProtocol `default:"PLAINTEXT" json:"security_protocol"`
}

func (d DestinationKafkaUpdatePLAINTEXT) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(d, "", false)
}

func (d *DestinationKafkaUpdatePLAINTEXT) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &d, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (d *DestinationKafkaUpdatePLAINTEXT) GetSecurityProtocol() *DestinationKafkaUpdateSecurityProtocol {
	if d == nil {
		return nil
	}
	return d.SecurityProtocol
}

type DestinationKafkaUpdateProtocolType string

const (
	DestinationKafkaUpdateProtocolTypeDestinationKafkaUpdatePLAINTEXT     DestinationKafkaUpdateProtocolType = "destination-kafka-update_PLAINTEXT"
	DestinationKafkaUpdateProtocolTypeDestinationKafkaUpdateSASLPLAINTEXT DestinationKafkaUpdateProtocolType = "destination-kafka-update_SASL PLAINTEXT"
	DestinationKafkaUpdateProtocolTypeDestinationKafkaUpdateSASLSSL       DestinationKafkaUpdateProtocolType = "destination-kafka-update_SASL SSL"
)

// DestinationKafkaUpdateProtocol - Protocol used to communicate with brokers.
type DestinationKafkaUpdateProtocol struct {
	DestinationKafkaUpdatePLAINTEXT     *DestinationKafkaUpdatePLAINTEXT     `queryParam:"inline" union:"member"`
	DestinationKafkaUpdateSASLPLAINTEXT *DestinationKafkaUpdateSASLPLAINTEXT `queryParam:"inline" union:"member"`
	DestinationKafkaUpdateSASLSSL       *DestinationKafkaUpdateSASLSSL       `queryParam:"inline" union:"member"`

	Type DestinationKafkaUpdateProtocolType
}

func CreateDestinationKafkaUpdateProtocolDestinationKafkaUpdatePLAINTEXT(destinationKafkaUpdatePLAINTEXT DestinationKafkaUpdatePLAINTEXT) DestinationKafkaUpdateProtocol {
	typ := DestinationKafkaUpdateProtocolTypeDestinationKafkaUpdatePLAINTEXT

	return DestinationKafkaUpdateProtocol{
		DestinationKafkaUpdatePLAINTEXT: &destinationKafkaUpdatePLAINTEXT,
		Type:                            typ,
	}
}

func CreateDestinationKafkaUpdateProtocolDestinationKafkaUpdateSASLPLAINTEXT(destinationKafkaUpdateSASLPLAINTEXT DestinationKafkaUpdateSASLPLAINTEXT) DestinationKafkaUpdateProtocol {
	typ := DestinationKafkaUpdateProtocolTypeDestinationKafkaUpdateSASLPLAINTEXT

	return DestinationKafkaUpdateProtocol{
		DestinationKafkaUpdateSASLPLAINTEXT: &destinationKafkaUpdateSASLPLAINTEXT,
		Type:                                typ,
	}
}

func CreateDestinationKafkaUpdateProtocolDestinationKafkaUpdateSASLSSL(destinationKafkaUpdateSASLSSL DestinationKafkaUpdateSASLSSL) DestinationKafkaUpdateProtocol {
	typ := DestinationKafkaUpdateProtocolTypeDestinationKafkaUpdateSASLSSL

	return DestinationKafkaUpdateProtocol{
		DestinationKafkaUpdateSASLSSL: &destinationKafkaUpdateSASLSSL,
		Type:                          typ,
	}
}

func (u *DestinationKafkaUpdateProtocol) UnmarshalJSON(data []byte) error {

	var candidates []utils.UnionCandidate

	// Collect all valid candidates
	var destinationKafkaUpdatePLAINTEXT DestinationKafkaUpdatePLAINTEXT = DestinationKafkaUpdatePLAINTEXT{}
	if err := utils.UnmarshalJSON(data, &destinationKafkaUpdatePLAINTEXT, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  DestinationKafkaUpdateProtocolTypeDestinationKafkaUpdatePLAINTEXT,
			Value: &destinationKafkaUpdatePLAINTEXT,
		})
	}

	var destinationKafkaUpdateSASLPLAINTEXT DestinationKafkaUpdateSASLPLAINTEXT = DestinationKafkaUpdateSASLPLAINTEXT{}
	if err := utils.UnmarshalJSON(data, &destinationKafkaUpdateSASLPLAINTEXT, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  DestinationKafkaUpdateProtocolTypeDestinationKafkaUpdateSASLPLAINTEXT,
			Value: &destinationKafkaUpdateSASLPLAINTEXT,
		})
	}

	var destinationKafkaUpdateSASLSSL DestinationKafkaUpdateSASLSSL = DestinationKafkaUpdateSASLSSL{}
	if err := utils.UnmarshalJSON(data, &destinationKafkaUpdateSASLSSL, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  DestinationKafkaUpdateProtocolTypeDestinationKafkaUpdateSASLSSL,
			Value: &destinationKafkaUpdateSASLSSL,
		})
	}

	if len(candidates) == 0 {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for DestinationKafkaUpdateProtocol", string(data))
	}

	// Pick the best candidate using multi-stage filtering
	best := utils.PickBestUnionCandidate(candidates, data)
	if best == nil {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for DestinationKafkaUpdateProtocol", string(data))
	}

	// Set the union type and value based on the best candidate
	u.Type = best.Type.(DestinationKafkaUpdateProtocolType)
	switch best.Type {
	case DestinationKafkaUpdateProtocolTypeDestinationKafkaUpdatePLAINTEXT:
		u.DestinationKafkaUpdatePLAINTEXT = best.Value.(*DestinationKafkaUpdatePLAINTEXT)
		return nil
	case DestinationKafkaUpdateProtocolTypeDestinationKafkaUpdateSASLPLAINTEXT:
		u.DestinationKafkaUpdateSASLPLAINTEXT = best.Value.(*DestinationKafkaUpdateSASLPLAINTEXT)
		return nil
	case DestinationKafkaUpdateProtocolTypeDestinationKafkaUpdateSASLSSL:
		u.DestinationKafkaUpdateSASLSSL = best.Value.(*DestinationKafkaUpdateSASLSSL)
		return nil
	}

	return fmt.Errorf("could not unmarshal `%s` into any supported union types for DestinationKafkaUpdateProtocol", string(data))
}

func (u DestinationKafkaUpdateProtocol) MarshalJSON() ([]byte, error) {
	if u.DestinationKafkaUpdatePLAINTEXT != nil {
		return utils.MarshalJSON(u.DestinationKafkaUpdatePLAINTEXT, "", true)
	}

	if u.DestinationKafkaUpdateSASLPLAINTEXT != nil {
		return utils.MarshalJSON(u.DestinationKafkaUpdateSASLPLAINTEXT, "", true)
	}

	if u.DestinationKafkaUpdateSASLSSL != nil {
		return utils.MarshalJSON(u.DestinationKafkaUpdateSASLSSL, "", true)
	}

	return nil, errors.New("could not marshal union type DestinationKafkaUpdateProtocol: all fields are null")
}

type DestinationKafkaUpdateDestinationType string

const (
	DestinationKafkaUpdateDestinationTypeKafka DestinationKafkaUpdateDestinationType = "kafka"
)

func (e DestinationKafkaUpdateDestinationType) ToPointer() *DestinationKafkaUpdateDestinationType {
	return &e
}
func (e *DestinationKafkaUpdateDestinationType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "kafka":
		*e = DestinationKafkaUpdateDestinationType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationKafkaUpdateDestinationType: %v", v)
	}
}

type DestinationKafkaUpdate struct {
	// The number of acknowledgments the producer requires the leader to have received before considering a request complete. This controls the  durability of records that are sent.
	Acks *DestinationKafkaUpdateACKs `default:"1" json:"acks"`
	// The producer will attempt to batch records together into fewer requests whenever multiple records are being sent to the same partition.
	BatchSize *int64 `json:"batch_size,omitempty"`
	// A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. The client will make use of all servers irrespective of which servers are specified here for bootstrapping&mdash;this list only impacts the initial hosts used to discover the full set of servers. This list should be in the form <code>host1:port1,host2:port2,...</code>. Since these servers are just used for the initial connection to discover the full cluster membership (which may change dynamically), this list need not contain the full set of servers (you may want more than one, though, in case a server is down).
	BootstrapServers *string `json:"bootstrap_servers,omitempty"`
	// The total bytes of memory the producer can use to buffer records waiting to be sent to the server.
	BufferMemory *string `json:"buffer_memory,omitempty"`
	// Controls how the client uses DNS lookups. If set to use_all_dns_ips, connect to each returned IP address in sequence until a successful connection is established. After a disconnection, the next IP is used. Once all IPs have been used once, the client resolves the IP(s) from the hostname again. If set to resolve_canonical_bootstrap_servers_only, resolve each bootstrap address into a list of canonical names. After the bootstrap phase, this behaves the same as use_all_dns_ips. If set to default (deprecated), attempt to connect to the first IP address returned by the lookup, even if the lookup returns multiple IP addresses.
	ClientDNSLookup *DestinationKafkaUpdateClientDNSLookup `default:"use_all_dns_ips" json:"client_dns_lookup"`
	// An ID string to pass to the server when making requests. The purpose of this is to be able to track the source of requests beyond just ip/port by allowing a logical application name to be included in server-side request logging.
	ClientID *string `json:"client_id,omitempty"`
	// The compression type for all data generated by the producer.
	CompressionType *DestinationKafkaUpdateCompressionType `default:"none" json:"compression_type"`
	// An upper bound on the time to report success or failure after a call to 'send()' returns.
	DeliveryTimeoutMs *int64 `json:"delivery_timeout_ms,omitempty"`
	// When set to 'true', the producer will ensure that exactly one copy of each message is written in the stream. If 'false', producer retries due to broker failures, etc., may write duplicates of the retried message in the stream.
	EnableIdempotence *bool `default:"false" json:"enable_idempotence"`
	// The producer groups together any records that arrive in between request transmissions into a single batched request.
	LingerMs *string `json:"linger_ms,omitempty"`
	// The configuration controls how long the KafkaProducer's send(), partitionsFor(), initTransactions(), sendOffsetsToTransaction(), commitTransaction() and abortTransaction() methods will block.
	MaxBlockMs *string `json:"max_block_ms,omitempty"`
	// The maximum number of unacknowledged requests the client will send on a single connection before blocking. Can be greater than 1, and the maximum value supported with idempotency is 5.
	MaxInFlightRequestsPerConnection *int64 `json:"max_in_flight_requests_per_connection,omitempty"`
	// The maximum size of a request in bytes.
	MaxRequestSize *int64 `json:"max_request_size,omitempty"`
	// Protocol used to communicate with brokers.
	Protocol *DestinationKafkaUpdateProtocol `json:"protocol,omitempty"`
	// The size of the TCP receive buffer (SO_RCVBUF) to use when reading data. If the value is -1, the OS default will be used.
	ReceiveBufferBytes *int64 `json:"receive_buffer_bytes,omitempty"`
	// The configuration controls the maximum amount of time the client will wait for the response of a request. If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted.
	RequestTimeoutMs *int64 `json:"request_timeout_ms,omitempty"`
	// Setting a value greater than zero will cause the client to resend any record whose send fails with a potentially transient error.
	Retries *int64 `json:"retries,omitempty"`
	// The size of the TCP send buffer (SO_SNDBUF) to use when sending data. If the value is -1, the OS default will be used.
	SendBufferBytes *int64 `json:"send_buffer_bytes,omitempty"`
	// The maximum amount of time the client will wait for the socket connection to be established. The connection setup timeout will increase exponentially for each consecutive connection failure up to this maximum.
	SocketConnectionSetupTimeoutMaxMs *string `json:"socket_connection_setup_timeout_max_ms,omitempty"`
	// The amount of time the client will wait for the socket connection to be established.
	SocketConnectionSetupTimeoutMs *string `json:"socket_connection_setup_timeout_ms,omitempty"`
	// Wait synchronously until the record has been sent to Kafka.
	SyncProducer *bool `default:"false" json:"sync_producer"`
	// Topic to test if Airbyte can produce messages.
	TestTopic *string `json:"test_topic,omitempty"`
	// Topic pattern in which the records will be sent. You can use patterns like '{namespace}' and/or '{stream}' to send the message to a specific topic based on these values. Notice that the topic name will be transformed to a standard naming convention.
	TopicPattern         *string                                `json:"topic_pattern,omitempty"`
	destinationType      *DestinationKafkaUpdateDestinationType `const:"kafka" json:"destinationType"`
	AdditionalProperties any                                    `additionalProperties:"true" json:"-"`
}

func (d DestinationKafkaUpdate) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(d, "", false)
}

func (d *DestinationKafkaUpdate) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &d, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (d *DestinationKafkaUpdate) GetAcks() *DestinationKafkaUpdateACKs {
	if d == nil {
		return nil
	}
	return d.Acks
}

func (d *DestinationKafkaUpdate) GetBatchSize() *int64 {
	if d == nil {
		return nil
	}
	return d.BatchSize
}

func (d *DestinationKafkaUpdate) GetBootstrapServers() *string {
	if d == nil {
		return nil
	}
	return d.BootstrapServers
}

func (d *DestinationKafkaUpdate) GetBufferMemory() *string {
	if d == nil {
		return nil
	}
	return d.BufferMemory
}

func (d *DestinationKafkaUpdate) GetClientDNSLookup() *DestinationKafkaUpdateClientDNSLookup {
	if d == nil {
		return nil
	}
	return d.ClientDNSLookup
}

func (d *DestinationKafkaUpdate) GetClientID() *string {
	if d == nil {
		return nil
	}
	return d.ClientID
}

func (d *DestinationKafkaUpdate) GetCompressionType() *DestinationKafkaUpdateCompressionType {
	if d == nil {
		return nil
	}
	return d.CompressionType
}

func (d *DestinationKafkaUpdate) GetDeliveryTimeoutMs() *int64 {
	if d == nil {
		return nil
	}
	return d.DeliveryTimeoutMs
}

func (d *DestinationKafkaUpdate) GetEnableIdempotence() *bool {
	if d == nil {
		return nil
	}
	return d.EnableIdempotence
}

func (d *DestinationKafkaUpdate) GetLingerMs() *string {
	if d == nil {
		return nil
	}
	return d.LingerMs
}

func (d *DestinationKafkaUpdate) GetMaxBlockMs() *string {
	if d == nil {
		return nil
	}
	return d.MaxBlockMs
}

func (d *DestinationKafkaUpdate) GetMaxInFlightRequestsPerConnection() *int64 {
	if d == nil {
		return nil
	}
	return d.MaxInFlightRequestsPerConnection
}

func (d *DestinationKafkaUpdate) GetMaxRequestSize() *int64 {
	if d == nil {
		return nil
	}
	return d.MaxRequestSize
}

func (d *DestinationKafkaUpdate) GetProtocol() *DestinationKafkaUpdateProtocol {
	if d == nil {
		return nil
	}
	return d.Protocol
}

func (d *DestinationKafkaUpdate) GetReceiveBufferBytes() *int64 {
	if d == nil {
		return nil
	}
	return d.ReceiveBufferBytes
}

func (d *DestinationKafkaUpdate) GetRequestTimeoutMs() *int64 {
	if d == nil {
		return nil
	}
	return d.RequestTimeoutMs
}

func (d *DestinationKafkaUpdate) GetRetries() *int64 {
	if d == nil {
		return nil
	}
	return d.Retries
}

func (d *DestinationKafkaUpdate) GetSendBufferBytes() *int64 {
	if d == nil {
		return nil
	}
	return d.SendBufferBytes
}

func (d *DestinationKafkaUpdate) GetSocketConnectionSetupTimeoutMaxMs() *string {
	if d == nil {
		return nil
	}
	return d.SocketConnectionSetupTimeoutMaxMs
}

func (d *DestinationKafkaUpdate) GetSocketConnectionSetupTimeoutMs() *string {
	if d == nil {
		return nil
	}
	return d.SocketConnectionSetupTimeoutMs
}

func (d *DestinationKafkaUpdate) GetSyncProducer() *bool {
	if d == nil {
		return nil
	}
	return d.SyncProducer
}

func (d *DestinationKafkaUpdate) GetTestTopic() *string {
	if d == nil {
		return nil
	}
	return d.TestTopic
}

func (d *DestinationKafkaUpdate) GetTopicPattern() *string {
	if d == nil {
		return nil
	}
	return d.TopicPattern
}

func (d *DestinationKafkaUpdate) GetDestinationType() *DestinationKafkaUpdateDestinationType {
	return DestinationKafkaUpdateDestinationTypeKafka.ToPointer()
}

func (d *DestinationKafkaUpdate) GetAdditionalProperties() any {
	if d == nil {
		return nil
	}
	return d.AdditionalProperties
}
