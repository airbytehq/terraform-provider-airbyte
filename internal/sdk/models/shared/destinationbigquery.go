// Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.

package shared

import (
	"encoding/json"
	"errors"
	"fmt"
	"github.com/airbytehq/terraform-provider-airbyte/internal/sdk/internal/utils"
)

// CDCDeletionMode - Whether to execute CDC deletions as hard deletes (i.e. propagate source deletions to the destination), or soft deletes (i.e. leave a tombstone record in the destination). Defaults to hard deletes.
type CDCDeletionMode string

const (
	CDCDeletionModeHardDelete CDCDeletionMode = "Hard delete"
	CDCDeletionModeSoftDelete CDCDeletionMode = "Soft delete"
)

func (e CDCDeletionMode) ToPointer() *CDCDeletionMode {
	return &e
}
func (e *CDCDeletionMode) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "Hard delete":
		fallthrough
	case "Soft delete":
		*e = CDCDeletionMode(v)
		return nil
	default:
		return fmt.Errorf("invalid value for CDCDeletionMode: %v", v)
	}
}

// DatasetLocation - The location of the dataset. Warning: Changes made after creation will not be applied. Read more <a href="https://cloud.google.com/bigquery/docs/locations">here</a>.
type DatasetLocation string

const (
	DatasetLocationEu                     DatasetLocation = "EU"
	DatasetLocationUs                     DatasetLocation = "US"
	DatasetLocationAfricaSouth1           DatasetLocation = "africa-south1"
	DatasetLocationAsiaEast1              DatasetLocation = "asia-east1"
	DatasetLocationAsiaEast2              DatasetLocation = "asia-east2"
	DatasetLocationAsiaNortheast1         DatasetLocation = "asia-northeast1"
	DatasetLocationAsiaNortheast2         DatasetLocation = "asia-northeast2"
	DatasetLocationAsiaNortheast3         DatasetLocation = "asia-northeast3"
	DatasetLocationAsiaSouth1             DatasetLocation = "asia-south1"
	DatasetLocationAsiaSouth2             DatasetLocation = "asia-south2"
	DatasetLocationAsiaSoutheast1         DatasetLocation = "asia-southeast1"
	DatasetLocationAsiaSoutheast2         DatasetLocation = "asia-southeast2"
	DatasetLocationAustraliaSoutheast1    DatasetLocation = "australia-southeast1"
	DatasetLocationAustraliaSoutheast2    DatasetLocation = "australia-southeast2"
	DatasetLocationEuropeCentral2         DatasetLocation = "europe-central2"
	DatasetLocationEuropeNorth1           DatasetLocation = "europe-north1"
	DatasetLocationEuropeNorth2           DatasetLocation = "europe-north2"
	DatasetLocationEuropeSouthwest1       DatasetLocation = "europe-southwest1"
	DatasetLocationEuropeWest1            DatasetLocation = "europe-west1"
	DatasetLocationEuropeWest2            DatasetLocation = "europe-west2"
	DatasetLocationEuropeWest3            DatasetLocation = "europe-west3"
	DatasetLocationEuropeWest4            DatasetLocation = "europe-west4"
	DatasetLocationEuropeWest6            DatasetLocation = "europe-west6"
	DatasetLocationEuropeWest8            DatasetLocation = "europe-west8"
	DatasetLocationEuropeWest9            DatasetLocation = "europe-west9"
	DatasetLocationEuropeWest10           DatasetLocation = "europe-west10"
	DatasetLocationEuropeWest12           DatasetLocation = "europe-west12"
	DatasetLocationMeCentral1             DatasetLocation = "me-central1"
	DatasetLocationMeCentral2             DatasetLocation = "me-central2"
	DatasetLocationMeWest1                DatasetLocation = "me-west1"
	DatasetLocationNorthamericaNortheast1 DatasetLocation = "northamerica-northeast1"
	DatasetLocationNorthamericaNortheast2 DatasetLocation = "northamerica-northeast2"
	DatasetLocationNorthamericaSouth1     DatasetLocation = "northamerica-south1"
	DatasetLocationSouthamericaEast1      DatasetLocation = "southamerica-east1"
	DatasetLocationSouthamericaWest1      DatasetLocation = "southamerica-west1"
	DatasetLocationUsCentral1             DatasetLocation = "us-central1"
	DatasetLocationUsEast1                DatasetLocation = "us-east1"
	DatasetLocationUsEast4                DatasetLocation = "us-east4"
	DatasetLocationUsEast5                DatasetLocation = "us-east5"
	DatasetLocationUsSouth1               DatasetLocation = "us-south1"
	DatasetLocationUsWest1                DatasetLocation = "us-west1"
	DatasetLocationUsWest2                DatasetLocation = "us-west2"
	DatasetLocationUsWest3                DatasetLocation = "us-west3"
	DatasetLocationUsWest4                DatasetLocation = "us-west4"
)

func (e DatasetLocation) ToPointer() *DatasetLocation {
	return &e
}
func (e *DatasetLocation) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "EU":
		fallthrough
	case "US":
		fallthrough
	case "africa-south1":
		fallthrough
	case "asia-east1":
		fallthrough
	case "asia-east2":
		fallthrough
	case "asia-northeast1":
		fallthrough
	case "asia-northeast2":
		fallthrough
	case "asia-northeast3":
		fallthrough
	case "asia-south1":
		fallthrough
	case "asia-south2":
		fallthrough
	case "asia-southeast1":
		fallthrough
	case "asia-southeast2":
		fallthrough
	case "australia-southeast1":
		fallthrough
	case "australia-southeast2":
		fallthrough
	case "europe-central2":
		fallthrough
	case "europe-north1":
		fallthrough
	case "europe-north2":
		fallthrough
	case "europe-southwest1":
		fallthrough
	case "europe-west1":
		fallthrough
	case "europe-west2":
		fallthrough
	case "europe-west3":
		fallthrough
	case "europe-west4":
		fallthrough
	case "europe-west6":
		fallthrough
	case "europe-west8":
		fallthrough
	case "europe-west9":
		fallthrough
	case "europe-west10":
		fallthrough
	case "europe-west12":
		fallthrough
	case "me-central1":
		fallthrough
	case "me-central2":
		fallthrough
	case "me-west1":
		fallthrough
	case "northamerica-northeast1":
		fallthrough
	case "northamerica-northeast2":
		fallthrough
	case "northamerica-south1":
		fallthrough
	case "southamerica-east1":
		fallthrough
	case "southamerica-west1":
		fallthrough
	case "us-central1":
		fallthrough
	case "us-east1":
		fallthrough
	case "us-east4":
		fallthrough
	case "us-east5":
		fallthrough
	case "us-south1":
		fallthrough
	case "us-west1":
		fallthrough
	case "us-west2":
		fallthrough
	case "us-west3":
		fallthrough
	case "us-west4":
		*e = DatasetLocation(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DatasetLocation: %v", v)
	}
}

type DestinationBigqueryCredentialType string

const (
	DestinationBigqueryCredentialTypeHmacKey DestinationBigqueryCredentialType = "HMAC_KEY"
)

func (e DestinationBigqueryCredentialType) ToPointer() *DestinationBigqueryCredentialType {
	return &e
}
func (e *DestinationBigqueryCredentialType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "HMAC_KEY":
		*e = DestinationBigqueryCredentialType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryCredentialType: %v", v)
	}
}

type DestinationBigqueryHMACKey struct {
	CredentialType *DestinationBigqueryCredentialType `default:"HMAC_KEY" json:"credential_type"`
	// HMAC key access ID. When linked to a service account, this ID is 61 characters long; when linked to a user account, it is 24 characters long.
	HmacKeyAccessID string `json:"hmac_key_access_id"`
	// The corresponding secret for the access ID. It is a 40-character base-64 encoded string.
	HmacKeySecret        string `json:"hmac_key_secret"`
	AdditionalProperties any    `additionalProperties:"true" json:"-"`
}

func (d DestinationBigqueryHMACKey) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(d, "", false)
}

func (d *DestinationBigqueryHMACKey) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &d, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (d *DestinationBigqueryHMACKey) GetCredentialType() *DestinationBigqueryCredentialType {
	if d == nil {
		return nil
	}
	return d.CredentialType
}

func (d *DestinationBigqueryHMACKey) GetHmacKeyAccessID() string {
	if d == nil {
		return ""
	}
	return d.HmacKeyAccessID
}

func (d *DestinationBigqueryHMACKey) GetHmacKeySecret() string {
	if d == nil {
		return ""
	}
	return d.HmacKeySecret
}

func (d *DestinationBigqueryHMACKey) GetAdditionalProperties() any {
	if d == nil {
		return nil
	}
	return d.AdditionalProperties
}

type CredentialUnionType string

const (
	CredentialUnionTypeDestinationBigqueryHMACKey CredentialUnionType = "destination-bigquery_HMAC key"
)

// Credential - An HMAC key is a type of credential and can be associated with a service account or a user account in Cloud Storage. Read more <a href="https://cloud.google.com/storage/docs/authentication/hmackeys">here</a>.
type Credential struct {
	DestinationBigqueryHMACKey *DestinationBigqueryHMACKey `queryParam:"inline" union:"member"`

	Type CredentialUnionType
}

func CreateCredentialDestinationBigqueryHMACKey(destinationBigqueryHMACKey DestinationBigqueryHMACKey) Credential {
	typ := CredentialUnionTypeDestinationBigqueryHMACKey

	return Credential{
		DestinationBigqueryHMACKey: &destinationBigqueryHMACKey,
		Type:                       typ,
	}
}

func (u *Credential) UnmarshalJSON(data []byte) error {

	var candidates []utils.UnionCandidate

	// Collect all valid candidates
	var destinationBigqueryHMACKey DestinationBigqueryHMACKey = DestinationBigqueryHMACKey{}
	if err := utils.UnmarshalJSON(data, &destinationBigqueryHMACKey, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  CredentialUnionTypeDestinationBigqueryHMACKey,
			Value: &destinationBigqueryHMACKey,
		})
	}

	if len(candidates) == 0 {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for Credential", string(data))
	}

	// Pick the best candidate using multi-stage filtering
	best := utils.PickBestUnionCandidate(candidates, data)
	if best == nil {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for Credential", string(data))
	}

	// Set the union type and value based on the best candidate
	u.Type = best.Type.(CredentialUnionType)
	switch best.Type {
	case CredentialUnionTypeDestinationBigqueryHMACKey:
		u.DestinationBigqueryHMACKey = best.Value.(*DestinationBigqueryHMACKey)
		return nil
	}

	return fmt.Errorf("could not unmarshal `%s` into any supported union types for Credential", string(data))
}

func (u Credential) MarshalJSON() ([]byte, error) {
	if u.DestinationBigqueryHMACKey != nil {
		return utils.MarshalJSON(u.DestinationBigqueryHMACKey, "", true)
	}

	return nil, errors.New("could not marshal union type Credential: all fields are null")
}

// GCSTmpFilesPostProcessing - This upload method is supposed to temporary store records in GCS bucket. By this select you can chose if these records should be removed from GCS when migration has finished. The default "Delete all tmp files from GCS" value is used if not set explicitly.
type GCSTmpFilesPostProcessing string

const (
	GCSTmpFilesPostProcessingDeleteAllTmpFilesFromGcs GCSTmpFilesPostProcessing = "Delete all tmp files from GCS"
	GCSTmpFilesPostProcessingKeepAllTmpFilesInGcs     GCSTmpFilesPostProcessing = "Keep all tmp files in GCS"
)

func (e GCSTmpFilesPostProcessing) ToPointer() *GCSTmpFilesPostProcessing {
	return &e
}
func (e *GCSTmpFilesPostProcessing) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "Delete all tmp files from GCS":
		fallthrough
	case "Keep all tmp files in GCS":
		*e = GCSTmpFilesPostProcessing(v)
		return nil
	default:
		return fmt.Errorf("invalid value for GCSTmpFilesPostProcessing: %v", v)
	}
}

type DestinationBigquerySchemasMethod string

const (
	DestinationBigquerySchemasMethodGcsStaging DestinationBigquerySchemasMethod = "GCS Staging"
)

func (e DestinationBigquerySchemasMethod) ToPointer() *DestinationBigquerySchemasMethod {
	return &e
}
func (e *DestinationBigquerySchemasMethod) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "GCS Staging":
		*e = DestinationBigquerySchemasMethod(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigquerySchemasMethod: %v", v)
	}
}

// GCSStaging - Writes large batches of records to a file, uploads the file to GCS, then uses COPY INTO to load your data into BigQuery.
type GCSStaging struct {
	// An HMAC key is a type of credential and can be associated with a service account or a user account in Cloud Storage. Read more <a href="https://cloud.google.com/storage/docs/authentication/hmackeys">here</a>.
	Credential Credential `json:"credential"`
	// The name of the GCS bucket. Read more <a href="https://cloud.google.com/storage/docs/naming-buckets">here</a>.
	GcsBucketName string `json:"gcs_bucket_name"`
	// Directory under the GCS bucket where data will be written.
	GcsBucketPath string `json:"gcs_bucket_path"`
	// This upload method is supposed to temporary store records in GCS bucket. By this select you can chose if these records should be removed from GCS when migration has finished. The default "Delete all tmp files from GCS" value is used if not set explicitly.
	KeepFilesInGcsBucket *GCSTmpFilesPostProcessing        `default:"Delete all tmp files from GCS" json:"keep_files_in_gcs-bucket"`
	Method               *DestinationBigquerySchemasMethod `default:"GCS Staging" json:"method"`
	AdditionalProperties any                               `additionalProperties:"true" json:"-"`
}

func (g GCSStaging) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(g, "", false)
}

func (g *GCSStaging) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &g, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (g *GCSStaging) GetCredential() Credential {
	if g == nil {
		return Credential{}
	}
	return g.Credential
}

func (g *GCSStaging) GetGcsBucketName() string {
	if g == nil {
		return ""
	}
	return g.GcsBucketName
}

func (g *GCSStaging) GetGcsBucketPath() string {
	if g == nil {
		return ""
	}
	return g.GcsBucketPath
}

func (g *GCSStaging) GetKeepFilesInGcsBucket() *GCSTmpFilesPostProcessing {
	if g == nil {
		return nil
	}
	return g.KeepFilesInGcsBucket
}

func (g *GCSStaging) GetMethod() *DestinationBigquerySchemasMethod {
	if g == nil {
		return nil
	}
	return g.Method
}

func (g *GCSStaging) GetAdditionalProperties() any {
	if g == nil {
		return nil
	}
	return g.AdditionalProperties
}

type DestinationBigqueryMethod string

const (
	DestinationBigqueryMethodStandard DestinationBigqueryMethod = "Standard"
)

func (e DestinationBigqueryMethod) ToPointer() *DestinationBigqueryMethod {
	return &e
}
func (e *DestinationBigqueryMethod) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "Standard":
		*e = DestinationBigqueryMethod(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryMethod: %v", v)
	}
}

// BatchedStandardInserts - Direct loading using batched SQL INSERT statements. This method uses the BigQuery driver to convert large INSERT statements into file uploads automatically.
type BatchedStandardInserts struct {
	Method               *DestinationBigqueryMethod `default:"Standard" json:"method"`
	AdditionalProperties any                        `additionalProperties:"true" json:"-"`
}

func (b BatchedStandardInserts) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(b, "", false)
}

func (b *BatchedStandardInserts) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &b, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (b *BatchedStandardInserts) GetMethod() *DestinationBigqueryMethod {
	if b == nil {
		return nil
	}
	return b.Method
}

func (b *BatchedStandardInserts) GetAdditionalProperties() any {
	if b == nil {
		return nil
	}
	return b.AdditionalProperties
}

type LoadingMethodType string

const (
	LoadingMethodTypeBatchedStandardInserts LoadingMethodType = "Batched Standard Inserts"
	LoadingMethodTypeGCSStaging             LoadingMethodType = "GCS Staging"
)

// LoadingMethod - The way data will be uploaded to BigQuery.
type LoadingMethod struct {
	BatchedStandardInserts *BatchedStandardInserts `queryParam:"inline" union:"member"`
	GCSStaging             *GCSStaging             `queryParam:"inline" union:"member"`

	Type LoadingMethodType
}

func CreateLoadingMethodBatchedStandardInserts(batchedStandardInserts BatchedStandardInserts) LoadingMethod {
	typ := LoadingMethodTypeBatchedStandardInserts

	return LoadingMethod{
		BatchedStandardInserts: &batchedStandardInserts,
		Type:                   typ,
	}
}

func CreateLoadingMethodGCSStaging(gcsStaging GCSStaging) LoadingMethod {
	typ := LoadingMethodTypeGCSStaging

	return LoadingMethod{
		GCSStaging: &gcsStaging,
		Type:       typ,
	}
}

func (u *LoadingMethod) UnmarshalJSON(data []byte) error {

	var candidates []utils.UnionCandidate

	// Collect all valid candidates
	var batchedStandardInserts BatchedStandardInserts = BatchedStandardInserts{}
	if err := utils.UnmarshalJSON(data, &batchedStandardInserts, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  LoadingMethodTypeBatchedStandardInserts,
			Value: &batchedStandardInserts,
		})
	}

	var gcsStaging GCSStaging = GCSStaging{}
	if err := utils.UnmarshalJSON(data, &gcsStaging, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  LoadingMethodTypeGCSStaging,
			Value: &gcsStaging,
		})
	}

	if len(candidates) == 0 {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for LoadingMethod", string(data))
	}

	// Pick the best candidate using multi-stage filtering
	best := utils.PickBestUnionCandidate(candidates, data)
	if best == nil {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for LoadingMethod", string(data))
	}

	// Set the union type and value based on the best candidate
	u.Type = best.Type.(LoadingMethodType)
	switch best.Type {
	case LoadingMethodTypeBatchedStandardInserts:
		u.BatchedStandardInserts = best.Value.(*BatchedStandardInserts)
		return nil
	case LoadingMethodTypeGCSStaging:
		u.GCSStaging = best.Value.(*GCSStaging)
		return nil
	}

	return fmt.Errorf("could not unmarshal `%s` into any supported union types for LoadingMethod", string(data))
}

func (u LoadingMethod) MarshalJSON() ([]byte, error) {
	if u.BatchedStandardInserts != nil {
		return utils.MarshalJSON(u.BatchedStandardInserts, "", true)
	}

	if u.GCSStaging != nil {
		return utils.MarshalJSON(u.GCSStaging, "", true)
	}

	return nil, errors.New("could not marshal union type LoadingMethod: all fields are null")
}

type DestinationBigqueryDestinationType string

const (
	DestinationBigqueryDestinationTypeBigquery DestinationBigqueryDestinationType = "bigquery"
)

func (e DestinationBigqueryDestinationType) ToPointer() *DestinationBigqueryDestinationType {
	return &e
}
func (e *DestinationBigqueryDestinationType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "bigquery":
		*e = DestinationBigqueryDestinationType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryDestinationType: %v", v)
	}
}

type DestinationBigquery struct {
	// Whether to execute CDC deletions as hard deletes (i.e. propagate source deletions to the destination), or soft deletes (i.e. leave a tombstone record in the destination). Defaults to hard deletes.
	CdcDeletionMode *CDCDeletionMode `default:"Hard delete" json:"cdc_deletion_mode"`
	// The contents of the JSON service account key. Check out the <a href="https://docs.airbyte.com/integrations/destinations/bigquery#service-account-key">docs</a> if you need help generating this key. Default credentials will be used if this field is left empty.
	CredentialsJSON *string `json:"credentials_json,omitempty"`
	// The default BigQuery Dataset ID that tables are replicated to if the source does not specify a namespace. Read more <a href="https://cloud.google.com/bigquery/docs/datasets#create-dataset">here</a>.
	DatasetID string `json:"dataset_id"`
	// The location of the dataset. Warning: Changes made after creation will not be applied. Read more <a href="https://cloud.google.com/bigquery/docs/locations">here</a>.
	DatasetLocation DatasetLocation `json:"dataset_location"`
	// Write the legacy "raw tables" format, to enable backwards compatibility with older versions of this connector.
	DisableTypeDedupe *bool `default:"false" json:"disable_type_dedupe"`
	// The way data will be uploaded to BigQuery.
	LoadingMethod *LoadingMethod `json:"loading_method,omitempty"`
	// The GCP project ID for the project containing the target BigQuery dataset. Read more <a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects">here</a>.
	ProjectID string `json:"project_id"`
	// Airbyte will use this dataset for various internal tables. In legacy raw tables mode, the raw tables will be stored in this dataset. Defaults to "airbyte_internal".
	RawDataDataset       *string                             `json:"raw_data_dataset,omitempty"`
	destinationType      *DestinationBigqueryDestinationType `const:"bigquery" json:"destinationType"`
	AdditionalProperties any                                 `additionalProperties:"true" json:"-"`
}

func (d DestinationBigquery) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(d, "", false)
}

func (d *DestinationBigquery) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &d, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (d *DestinationBigquery) GetCdcDeletionMode() *CDCDeletionMode {
	if d == nil {
		return nil
	}
	return d.CdcDeletionMode
}

func (d *DestinationBigquery) GetCredentialsJSON() *string {
	if d == nil {
		return nil
	}
	return d.CredentialsJSON
}

func (d *DestinationBigquery) GetDatasetID() string {
	if d == nil {
		return ""
	}
	return d.DatasetID
}

func (d *DestinationBigquery) GetDatasetLocation() DatasetLocation {
	if d == nil {
		return DatasetLocation("")
	}
	return d.DatasetLocation
}

func (d *DestinationBigquery) GetDisableTypeDedupe() *bool {
	if d == nil {
		return nil
	}
	return d.DisableTypeDedupe
}

func (d *DestinationBigquery) GetLoadingMethod() *LoadingMethod {
	if d == nil {
		return nil
	}
	return d.LoadingMethod
}

func (d *DestinationBigquery) GetProjectID() string {
	if d == nil {
		return ""
	}
	return d.ProjectID
}

func (d *DestinationBigquery) GetRawDataDataset() *string {
	if d == nil {
		return nil
	}
	return d.RawDataDataset
}

func (d *DestinationBigquery) GetDestinationType() *DestinationBigqueryDestinationType {
	return DestinationBigqueryDestinationTypeBigquery.ToPointer()
}

func (d *DestinationBigquery) GetAdditionalProperties() any {
	if d == nil {
		return nil
	}
	return d.AdditionalProperties
}
