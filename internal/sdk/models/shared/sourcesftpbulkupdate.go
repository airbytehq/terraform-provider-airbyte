// Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.

package shared

import (
	"encoding/json"
	"errors"
	"fmt"
	"github.com/airbytehq/terraform-provider-airbyte/internal/sdk/internal/utils"
	"github.com/airbytehq/terraform-provider-airbyte/internal/sdk/types"
	"time"
)

type SourceSftpBulkUpdateSchemasAuthType string

const (
	SourceSftpBulkUpdateSchemasAuthTypePrivateKey SourceSftpBulkUpdateSchemasAuthType = "private_key"
)

func (e SourceSftpBulkUpdateSchemasAuthType) ToPointer() *SourceSftpBulkUpdateSchemasAuthType {
	return &e
}
func (e *SourceSftpBulkUpdateSchemasAuthType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "private_key":
		*e = SourceSftpBulkUpdateSchemasAuthType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceSftpBulkUpdateSchemasAuthType: %v", v)
	}
}

type SourceSftpBulkUpdateAuthenticateViaPrivateKey struct {
	authType *SourceSftpBulkUpdateSchemasAuthType `const:"private_key" json:"auth_type"`
	// The Private key
	PrivateKey *string `json:"private_key,omitempty"`
}

func (s SourceSftpBulkUpdateAuthenticateViaPrivateKey) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkUpdateAuthenticateViaPrivateKey) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkUpdateAuthenticateViaPrivateKey) GetAuthType() *SourceSftpBulkUpdateSchemasAuthType {
	return SourceSftpBulkUpdateSchemasAuthTypePrivateKey.ToPointer()
}

func (s *SourceSftpBulkUpdateAuthenticateViaPrivateKey) GetPrivateKey() *string {
	if s == nil {
		return nil
	}
	return s.PrivateKey
}

type SourceSftpBulkUpdateAuthType string

const (
	SourceSftpBulkUpdateAuthTypePassword SourceSftpBulkUpdateAuthType = "password"
)

func (e SourceSftpBulkUpdateAuthType) ToPointer() *SourceSftpBulkUpdateAuthType {
	return &e
}
func (e *SourceSftpBulkUpdateAuthType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "password":
		*e = SourceSftpBulkUpdateAuthType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceSftpBulkUpdateAuthType: %v", v)
	}
}

type SourceSftpBulkUpdateAuthenticateViaPassword struct {
	authType *SourceSftpBulkUpdateAuthType `const:"password" json:"auth_type"`
	// Password
	Password *string `json:"password,omitempty"`
}

func (s SourceSftpBulkUpdateAuthenticateViaPassword) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkUpdateAuthenticateViaPassword) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkUpdateAuthenticateViaPassword) GetAuthType() *SourceSftpBulkUpdateAuthType {
	return SourceSftpBulkUpdateAuthTypePassword.ToPointer()
}

func (s *SourceSftpBulkUpdateAuthenticateViaPassword) GetPassword() *string {
	if s == nil {
		return nil
	}
	return s.Password
}

type SourceSftpBulkUpdateAuthenticationType string

const (
	SourceSftpBulkUpdateAuthenticationTypeSourceSftpBulkUpdateAuthenticateViaPassword   SourceSftpBulkUpdateAuthenticationType = "source-sftp-bulk-update_Authenticate via Password"
	SourceSftpBulkUpdateAuthenticationTypeSourceSftpBulkUpdateAuthenticateViaPrivateKey SourceSftpBulkUpdateAuthenticationType = "source-sftp-bulk-update_Authenticate via Private Key"
)

// SourceSftpBulkUpdateAuthentication - Credentials for connecting to the SFTP Server
type SourceSftpBulkUpdateAuthentication struct {
	SourceSftpBulkUpdateAuthenticateViaPassword   *SourceSftpBulkUpdateAuthenticateViaPassword   `queryParam:"inline" union:"member"`
	SourceSftpBulkUpdateAuthenticateViaPrivateKey *SourceSftpBulkUpdateAuthenticateViaPrivateKey `queryParam:"inline" union:"member"`

	Type SourceSftpBulkUpdateAuthenticationType
}

func CreateSourceSftpBulkUpdateAuthenticationSourceSftpBulkUpdateAuthenticateViaPassword(sourceSftpBulkUpdateAuthenticateViaPassword SourceSftpBulkUpdateAuthenticateViaPassword) SourceSftpBulkUpdateAuthentication {
	typ := SourceSftpBulkUpdateAuthenticationTypeSourceSftpBulkUpdateAuthenticateViaPassword

	return SourceSftpBulkUpdateAuthentication{
		SourceSftpBulkUpdateAuthenticateViaPassword: &sourceSftpBulkUpdateAuthenticateViaPassword,
		Type: typ,
	}
}

func CreateSourceSftpBulkUpdateAuthenticationSourceSftpBulkUpdateAuthenticateViaPrivateKey(sourceSftpBulkUpdateAuthenticateViaPrivateKey SourceSftpBulkUpdateAuthenticateViaPrivateKey) SourceSftpBulkUpdateAuthentication {
	typ := SourceSftpBulkUpdateAuthenticationTypeSourceSftpBulkUpdateAuthenticateViaPrivateKey

	return SourceSftpBulkUpdateAuthentication{
		SourceSftpBulkUpdateAuthenticateViaPrivateKey: &sourceSftpBulkUpdateAuthenticateViaPrivateKey,
		Type: typ,
	}
}

func (u *SourceSftpBulkUpdateAuthentication) UnmarshalJSON(data []byte) error {

	var candidates []utils.UnionCandidate

	// Collect all valid candidates
	var sourceSftpBulkUpdateAuthenticateViaPassword SourceSftpBulkUpdateAuthenticateViaPassword = SourceSftpBulkUpdateAuthenticateViaPassword{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkUpdateAuthenticateViaPassword, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkUpdateAuthenticationTypeSourceSftpBulkUpdateAuthenticateViaPassword,
			Value: &sourceSftpBulkUpdateAuthenticateViaPassword,
		})
	}

	var sourceSftpBulkUpdateAuthenticateViaPrivateKey SourceSftpBulkUpdateAuthenticateViaPrivateKey = SourceSftpBulkUpdateAuthenticateViaPrivateKey{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkUpdateAuthenticateViaPrivateKey, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkUpdateAuthenticationTypeSourceSftpBulkUpdateAuthenticateViaPrivateKey,
			Value: &sourceSftpBulkUpdateAuthenticateViaPrivateKey,
		})
	}

	if len(candidates) == 0 {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkUpdateAuthentication", string(data))
	}

	// Pick the best candidate using multi-stage filtering
	best := utils.PickBestUnionCandidate(candidates, data)
	if best == nil {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkUpdateAuthentication", string(data))
	}

	// Set the union type and value based on the best candidate
	u.Type = best.Type.(SourceSftpBulkUpdateAuthenticationType)
	switch best.Type {
	case SourceSftpBulkUpdateAuthenticationTypeSourceSftpBulkUpdateAuthenticateViaPassword:
		u.SourceSftpBulkUpdateAuthenticateViaPassword = best.Value.(*SourceSftpBulkUpdateAuthenticateViaPassword)
		return nil
	case SourceSftpBulkUpdateAuthenticationTypeSourceSftpBulkUpdateAuthenticateViaPrivateKey:
		u.SourceSftpBulkUpdateAuthenticateViaPrivateKey = best.Value.(*SourceSftpBulkUpdateAuthenticateViaPrivateKey)
		return nil
	}

	return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkUpdateAuthentication", string(data))
}

func (u SourceSftpBulkUpdateAuthentication) MarshalJSON() ([]byte, error) {
	if u.SourceSftpBulkUpdateAuthenticateViaPassword != nil {
		return utils.MarshalJSON(u.SourceSftpBulkUpdateAuthenticateViaPassword, "", true)
	}

	if u.SourceSftpBulkUpdateAuthenticateViaPrivateKey != nil {
		return utils.MarshalJSON(u.SourceSftpBulkUpdateAuthenticateViaPrivateKey, "", true)
	}

	return nil, errors.New("could not marshal union type SourceSftpBulkUpdateAuthentication: all fields are null")
}

type SourceSftpBulkUpdateSchemasDeliveryType string

const (
	SourceSftpBulkUpdateSchemasDeliveryTypeUseFileTransfer SourceSftpBulkUpdateSchemasDeliveryType = "use_file_transfer"
)

func (e SourceSftpBulkUpdateSchemasDeliveryType) ToPointer() *SourceSftpBulkUpdateSchemasDeliveryType {
	return &e
}
func (e *SourceSftpBulkUpdateSchemasDeliveryType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "use_file_transfer":
		*e = SourceSftpBulkUpdateSchemasDeliveryType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceSftpBulkUpdateSchemasDeliveryType: %v", v)
	}
}

// SourceSftpBulkUpdateCopyRawFiles - Copy raw files without parsing their contents. Bits are copied into the destination exactly as they appeared in the source. Recommended for use with unstructured text data, non-text and compressed files.
type SourceSftpBulkUpdateCopyRawFiles struct {
	deliveryType *SourceSftpBulkUpdateSchemasDeliveryType `const:"use_file_transfer" json:"delivery_type"`
	// If enabled, sends subdirectory folder structure along with source file names to the destination. Otherwise, files will be synced by their names only. This option is ignored when file-based replication is not enabled.
	PreserveDirectoryStructure *bool `default:"true" json:"preserve_directory_structure"`
}

func (s SourceSftpBulkUpdateCopyRawFiles) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkUpdateCopyRawFiles) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkUpdateCopyRawFiles) GetDeliveryType() *SourceSftpBulkUpdateSchemasDeliveryType {
	return SourceSftpBulkUpdateSchemasDeliveryTypeUseFileTransfer.ToPointer()
}

func (s *SourceSftpBulkUpdateCopyRawFiles) GetPreserveDirectoryStructure() *bool {
	if s == nil {
		return nil
	}
	return s.PreserveDirectoryStructure
}

type SourceSftpBulkUpdateDeliveryType string

const (
	SourceSftpBulkUpdateDeliveryTypeUseRecordsTransfer SourceSftpBulkUpdateDeliveryType = "use_records_transfer"
)

func (e SourceSftpBulkUpdateDeliveryType) ToPointer() *SourceSftpBulkUpdateDeliveryType {
	return &e
}
func (e *SourceSftpBulkUpdateDeliveryType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "use_records_transfer":
		*e = SourceSftpBulkUpdateDeliveryType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceSftpBulkUpdateDeliveryType: %v", v)
	}
}

// SourceSftpBulkUpdateReplicateRecords - Recommended - Extract and load structured records into your destination of choice. This is the classic method of moving data in Airbyte. It allows for blocking and hashing individual fields or files from a structured schema. Data can be flattened, typed and deduped depending on the destination.
type SourceSftpBulkUpdateReplicateRecords struct {
	deliveryType *SourceSftpBulkUpdateDeliveryType `const:"use_records_transfer" json:"delivery_type"`
}

func (s SourceSftpBulkUpdateReplicateRecords) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkUpdateReplicateRecords) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkUpdateReplicateRecords) GetDeliveryType() *SourceSftpBulkUpdateDeliveryType {
	return SourceSftpBulkUpdateDeliveryTypeUseRecordsTransfer.ToPointer()
}

type SourceSftpBulkUpdateDeliveryMethodType string

const (
	SourceSftpBulkUpdateDeliveryMethodTypeSourceSftpBulkUpdateReplicateRecords SourceSftpBulkUpdateDeliveryMethodType = "source-sftp-bulk-update_Replicate Records"
	SourceSftpBulkUpdateDeliveryMethodTypeSourceSftpBulkUpdateCopyRawFiles     SourceSftpBulkUpdateDeliveryMethodType = "source-sftp-bulk-update_Copy Raw Files"
)

type SourceSftpBulkUpdateDeliveryMethod struct {
	SourceSftpBulkUpdateReplicateRecords *SourceSftpBulkUpdateReplicateRecords `queryParam:"inline" union:"member"`
	SourceSftpBulkUpdateCopyRawFiles     *SourceSftpBulkUpdateCopyRawFiles     `queryParam:"inline" union:"member"`

	Type SourceSftpBulkUpdateDeliveryMethodType
}

func CreateSourceSftpBulkUpdateDeliveryMethodSourceSftpBulkUpdateReplicateRecords(sourceSftpBulkUpdateReplicateRecords SourceSftpBulkUpdateReplicateRecords) SourceSftpBulkUpdateDeliveryMethod {
	typ := SourceSftpBulkUpdateDeliveryMethodTypeSourceSftpBulkUpdateReplicateRecords

	return SourceSftpBulkUpdateDeliveryMethod{
		SourceSftpBulkUpdateReplicateRecords: &sourceSftpBulkUpdateReplicateRecords,
		Type:                                 typ,
	}
}

func CreateSourceSftpBulkUpdateDeliveryMethodSourceSftpBulkUpdateCopyRawFiles(sourceSftpBulkUpdateCopyRawFiles SourceSftpBulkUpdateCopyRawFiles) SourceSftpBulkUpdateDeliveryMethod {
	typ := SourceSftpBulkUpdateDeliveryMethodTypeSourceSftpBulkUpdateCopyRawFiles

	return SourceSftpBulkUpdateDeliveryMethod{
		SourceSftpBulkUpdateCopyRawFiles: &sourceSftpBulkUpdateCopyRawFiles,
		Type:                             typ,
	}
}

func (u *SourceSftpBulkUpdateDeliveryMethod) UnmarshalJSON(data []byte) error {

	var candidates []utils.UnionCandidate

	// Collect all valid candidates
	var sourceSftpBulkUpdateReplicateRecords SourceSftpBulkUpdateReplicateRecords = SourceSftpBulkUpdateReplicateRecords{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkUpdateReplicateRecords, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkUpdateDeliveryMethodTypeSourceSftpBulkUpdateReplicateRecords,
			Value: &sourceSftpBulkUpdateReplicateRecords,
		})
	}

	var sourceSftpBulkUpdateCopyRawFiles SourceSftpBulkUpdateCopyRawFiles = SourceSftpBulkUpdateCopyRawFiles{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkUpdateCopyRawFiles, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkUpdateDeliveryMethodTypeSourceSftpBulkUpdateCopyRawFiles,
			Value: &sourceSftpBulkUpdateCopyRawFiles,
		})
	}

	if len(candidates) == 0 {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkUpdateDeliveryMethod", string(data))
	}

	// Pick the best candidate using multi-stage filtering
	best := utils.PickBestUnionCandidate(candidates, data)
	if best == nil {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkUpdateDeliveryMethod", string(data))
	}

	// Set the union type and value based on the best candidate
	u.Type = best.Type.(SourceSftpBulkUpdateDeliveryMethodType)
	switch best.Type {
	case SourceSftpBulkUpdateDeliveryMethodTypeSourceSftpBulkUpdateReplicateRecords:
		u.SourceSftpBulkUpdateReplicateRecords = best.Value.(*SourceSftpBulkUpdateReplicateRecords)
		return nil
	case SourceSftpBulkUpdateDeliveryMethodTypeSourceSftpBulkUpdateCopyRawFiles:
		u.SourceSftpBulkUpdateCopyRawFiles = best.Value.(*SourceSftpBulkUpdateCopyRawFiles)
		return nil
	}

	return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkUpdateDeliveryMethod", string(data))
}

func (u SourceSftpBulkUpdateDeliveryMethod) MarshalJSON() ([]byte, error) {
	if u.SourceSftpBulkUpdateReplicateRecords != nil {
		return utils.MarshalJSON(u.SourceSftpBulkUpdateReplicateRecords, "", true)
	}

	if u.SourceSftpBulkUpdateCopyRawFiles != nil {
		return utils.MarshalJSON(u.SourceSftpBulkUpdateCopyRawFiles, "", true)
	}

	return nil, errors.New("could not marshal union type SourceSftpBulkUpdateDeliveryMethod: all fields are null")
}

type SourceSftpBulkUpdateExcelFormat struct {
	filetype *string `const:"excel" json:"filetype"`
}

func (s SourceSftpBulkUpdateExcelFormat) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkUpdateExcelFormat) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkUpdateExcelFormat) GetFiletype() *string {
	return types.Pointer("excel")
}

type SourceSftpBulkUpdateSchemasMode string

const (
	SourceSftpBulkUpdateSchemasModeAPI SourceSftpBulkUpdateSchemasMode = "api"
)

func (e SourceSftpBulkUpdateSchemasMode) ToPointer() *SourceSftpBulkUpdateSchemasMode {
	return &e
}
func (e *SourceSftpBulkUpdateSchemasMode) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "api":
		*e = SourceSftpBulkUpdateSchemasMode(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceSftpBulkUpdateSchemasMode: %v", v)
	}
}

type SourceSftpBulkUpdateAPIParameterConfigModel struct {
	// The name of the unstructured API parameter to use
	Name *string `json:"name,omitempty"`
	// The value of the parameter
	Value *string `json:"value,omitempty"`
}

func (s SourceSftpBulkUpdateAPIParameterConfigModel) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkUpdateAPIParameterConfigModel) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkUpdateAPIParameterConfigModel) GetName() *string {
	if s == nil {
		return nil
	}
	return s.Name
}

func (s *SourceSftpBulkUpdateAPIParameterConfigModel) GetValue() *string {
	if s == nil {
		return nil
	}
	return s.Value
}

// SourceSftpBulkUpdateViaAPI - Process files via an API, using the `hi_res` mode. This option is useful for increased performance and accuracy, but requires an API key and a hosted instance of unstructured.
type SourceSftpBulkUpdateViaAPI struct {
	// The API key to use matching the environment
	APIKey *string `default:"" json:"api_key"`
	// The URL of the unstructured API to use
	APIURL *string                          `default:"https://api.unstructured.io" json:"api_url"`
	mode   *SourceSftpBulkUpdateSchemasMode `const:"api" json:"mode"`
	// List of parameters send to the API
	Parameters []SourceSftpBulkUpdateAPIParameterConfigModel `json:"parameters,omitempty"`
}

func (s SourceSftpBulkUpdateViaAPI) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkUpdateViaAPI) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkUpdateViaAPI) GetAPIKey() *string {
	if s == nil {
		return nil
	}
	return s.APIKey
}

func (s *SourceSftpBulkUpdateViaAPI) GetAPIURL() *string {
	if s == nil {
		return nil
	}
	return s.APIURL
}

func (s *SourceSftpBulkUpdateViaAPI) GetMode() *SourceSftpBulkUpdateSchemasMode {
	return SourceSftpBulkUpdateSchemasModeAPI.ToPointer()
}

func (s *SourceSftpBulkUpdateViaAPI) GetParameters() []SourceSftpBulkUpdateAPIParameterConfigModel {
	if s == nil {
		return nil
	}
	return s.Parameters
}

type SourceSftpBulkUpdateMode string

const (
	SourceSftpBulkUpdateModeLocal SourceSftpBulkUpdateMode = "local"
)

func (e SourceSftpBulkUpdateMode) ToPointer() *SourceSftpBulkUpdateMode {
	return &e
}
func (e *SourceSftpBulkUpdateMode) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "local":
		*e = SourceSftpBulkUpdateMode(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceSftpBulkUpdateMode: %v", v)
	}
}

// SourceSftpBulkUpdateLocal - Process files locally, supporting `fast` and `ocr` modes. This is the default option.
type SourceSftpBulkUpdateLocal struct {
	mode *SourceSftpBulkUpdateMode `const:"local" json:"mode"`
}

func (s SourceSftpBulkUpdateLocal) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkUpdateLocal) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkUpdateLocal) GetMode() *SourceSftpBulkUpdateMode {
	return SourceSftpBulkUpdateModeLocal.ToPointer()
}

type SourceSftpBulkUpdateProcessingType string

const (
	SourceSftpBulkUpdateProcessingTypeSourceSftpBulkUpdateLocal  SourceSftpBulkUpdateProcessingType = "source-sftp-bulk-update_Local"
	SourceSftpBulkUpdateProcessingTypeSourceSftpBulkUpdateViaAPI SourceSftpBulkUpdateProcessingType = "source-sftp-bulk-update_via API"
)

// SourceSftpBulkUpdateProcessing - Processing configuration
type SourceSftpBulkUpdateProcessing struct {
	SourceSftpBulkUpdateLocal  *SourceSftpBulkUpdateLocal  `queryParam:"inline" union:"member"`
	SourceSftpBulkUpdateViaAPI *SourceSftpBulkUpdateViaAPI `queryParam:"inline" union:"member"`

	Type SourceSftpBulkUpdateProcessingType
}

func CreateSourceSftpBulkUpdateProcessingSourceSftpBulkUpdateLocal(sourceSftpBulkUpdateLocal SourceSftpBulkUpdateLocal) SourceSftpBulkUpdateProcessing {
	typ := SourceSftpBulkUpdateProcessingTypeSourceSftpBulkUpdateLocal

	return SourceSftpBulkUpdateProcessing{
		SourceSftpBulkUpdateLocal: &sourceSftpBulkUpdateLocal,
		Type:                      typ,
	}
}

func CreateSourceSftpBulkUpdateProcessingSourceSftpBulkUpdateViaAPI(sourceSftpBulkUpdateViaAPI SourceSftpBulkUpdateViaAPI) SourceSftpBulkUpdateProcessing {
	typ := SourceSftpBulkUpdateProcessingTypeSourceSftpBulkUpdateViaAPI

	return SourceSftpBulkUpdateProcessing{
		SourceSftpBulkUpdateViaAPI: &sourceSftpBulkUpdateViaAPI,
		Type:                       typ,
	}
}

func (u *SourceSftpBulkUpdateProcessing) UnmarshalJSON(data []byte) error {

	var candidates []utils.UnionCandidate

	// Collect all valid candidates
	var sourceSftpBulkUpdateLocal SourceSftpBulkUpdateLocal = SourceSftpBulkUpdateLocal{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkUpdateLocal, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkUpdateProcessingTypeSourceSftpBulkUpdateLocal,
			Value: &sourceSftpBulkUpdateLocal,
		})
	}

	var sourceSftpBulkUpdateViaAPI SourceSftpBulkUpdateViaAPI = SourceSftpBulkUpdateViaAPI{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkUpdateViaAPI, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkUpdateProcessingTypeSourceSftpBulkUpdateViaAPI,
			Value: &sourceSftpBulkUpdateViaAPI,
		})
	}

	if len(candidates) == 0 {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkUpdateProcessing", string(data))
	}

	// Pick the best candidate using multi-stage filtering
	best := utils.PickBestUnionCandidate(candidates, data)
	if best == nil {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkUpdateProcessing", string(data))
	}

	// Set the union type and value based on the best candidate
	u.Type = best.Type.(SourceSftpBulkUpdateProcessingType)
	switch best.Type {
	case SourceSftpBulkUpdateProcessingTypeSourceSftpBulkUpdateLocal:
		u.SourceSftpBulkUpdateLocal = best.Value.(*SourceSftpBulkUpdateLocal)
		return nil
	case SourceSftpBulkUpdateProcessingTypeSourceSftpBulkUpdateViaAPI:
		u.SourceSftpBulkUpdateViaAPI = best.Value.(*SourceSftpBulkUpdateViaAPI)
		return nil
	}

	return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkUpdateProcessing", string(data))
}

func (u SourceSftpBulkUpdateProcessing) MarshalJSON() ([]byte, error) {
	if u.SourceSftpBulkUpdateLocal != nil {
		return utils.MarshalJSON(u.SourceSftpBulkUpdateLocal, "", true)
	}

	if u.SourceSftpBulkUpdateViaAPI != nil {
		return utils.MarshalJSON(u.SourceSftpBulkUpdateViaAPI, "", true)
	}

	return nil, errors.New("could not marshal union type SourceSftpBulkUpdateProcessing: all fields are null")
}

// SourceSftpBulkUpdateParsingStrategy - The strategy used to parse documents. `fast` extracts text directly from the document which doesn't work for all files. `ocr_only` is more reliable, but slower. `hi_res` is the most reliable, but requires an API key and a hosted instance of unstructured and can't be used with local mode. See the unstructured.io documentation for more details: https://unstructured-io.github.io/unstructured/core/partition.html#partition-pdf
type SourceSftpBulkUpdateParsingStrategy string

const (
	SourceSftpBulkUpdateParsingStrategyAuto    SourceSftpBulkUpdateParsingStrategy = "auto"
	SourceSftpBulkUpdateParsingStrategyFast    SourceSftpBulkUpdateParsingStrategy = "fast"
	SourceSftpBulkUpdateParsingStrategyOcrOnly SourceSftpBulkUpdateParsingStrategy = "ocr_only"
	SourceSftpBulkUpdateParsingStrategyHiRes   SourceSftpBulkUpdateParsingStrategy = "hi_res"
)

func (e SourceSftpBulkUpdateParsingStrategy) ToPointer() *SourceSftpBulkUpdateParsingStrategy {
	return &e
}
func (e *SourceSftpBulkUpdateParsingStrategy) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "auto":
		fallthrough
	case "fast":
		fallthrough
	case "ocr_only":
		fallthrough
	case "hi_res":
		*e = SourceSftpBulkUpdateParsingStrategy(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceSftpBulkUpdateParsingStrategy: %v", v)
	}
}

// SourceSftpBulkUpdateUnstructuredDocumentFormat - Extract text from document formats (.pdf, .docx, .md, .pptx) and emit as one record per file.
type SourceSftpBulkUpdateUnstructuredDocumentFormat struct {
	filetype *string `const:"unstructured" json:"filetype"`
	// Processing configuration
	Processing *SourceSftpBulkUpdateProcessing `json:"processing,omitempty"`
	// If true, skip files that cannot be parsed and pass the error message along as the _ab_source_file_parse_error field. If false, fail the sync.
	SkipUnprocessableFiles *bool `default:"true" json:"skip_unprocessable_files"`
	// The strategy used to parse documents. `fast` extracts text directly from the document which doesn't work for all files. `ocr_only` is more reliable, but slower. `hi_res` is the most reliable, but requires an API key and a hosted instance of unstructured and can't be used with local mode. See the unstructured.io documentation for more details: https://unstructured-io.github.io/unstructured/core/partition.html#partition-pdf
	Strategy *SourceSftpBulkUpdateParsingStrategy `default:"auto" json:"strategy"`
}

func (s SourceSftpBulkUpdateUnstructuredDocumentFormat) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkUpdateUnstructuredDocumentFormat) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkUpdateUnstructuredDocumentFormat) GetFiletype() *string {
	return types.Pointer("unstructured")
}

func (s *SourceSftpBulkUpdateUnstructuredDocumentFormat) GetProcessing() *SourceSftpBulkUpdateProcessing {
	if s == nil {
		return nil
	}
	return s.Processing
}

func (s *SourceSftpBulkUpdateUnstructuredDocumentFormat) GetSkipUnprocessableFiles() *bool {
	if s == nil {
		return nil
	}
	return s.SkipUnprocessableFiles
}

func (s *SourceSftpBulkUpdateUnstructuredDocumentFormat) GetStrategy() *SourceSftpBulkUpdateParsingStrategy {
	if s == nil {
		return nil
	}
	return s.Strategy
}

type SourceSftpBulkUpdateParquetFormat struct {
	// Whether to convert decimal fields to floats. There is a loss of precision when converting decimals to floats, so this is not recommended.
	DecimalAsFloat *bool   `default:"false" json:"decimal_as_float"`
	filetype       *string `const:"parquet" json:"filetype"`
}

func (s SourceSftpBulkUpdateParquetFormat) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkUpdateParquetFormat) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkUpdateParquetFormat) GetDecimalAsFloat() *bool {
	if s == nil {
		return nil
	}
	return s.DecimalAsFloat
}

func (s *SourceSftpBulkUpdateParquetFormat) GetFiletype() *string {
	return types.Pointer("parquet")
}

type SourceSftpBulkUpdateJsonlFormat struct {
	filetype *string `const:"jsonl" json:"filetype"`
}

func (s SourceSftpBulkUpdateJsonlFormat) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkUpdateJsonlFormat) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkUpdateJsonlFormat) GetFiletype() *string {
	return types.Pointer("jsonl")
}

type SourceSftpBulkUpdateUserProvided struct {
	// The column names that will be used while emitting the CSV records
	ColumnNames          []string `json:"column_names,omitempty"`
	headerDefinitionType *string  `const:"User Provided" json:"header_definition_type"`
}

func (s SourceSftpBulkUpdateUserProvided) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkUpdateUserProvided) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkUpdateUserProvided) GetColumnNames() []string {
	if s == nil {
		return nil
	}
	return s.ColumnNames
}

func (s *SourceSftpBulkUpdateUserProvided) GetHeaderDefinitionType() *string {
	return types.Pointer("User Provided")
}

type SourceSftpBulkUpdateAutogenerated struct {
	headerDefinitionType *string `const:"Autogenerated" json:"header_definition_type"`
}

func (s SourceSftpBulkUpdateAutogenerated) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkUpdateAutogenerated) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkUpdateAutogenerated) GetHeaderDefinitionType() *string {
	return types.Pointer("Autogenerated")
}

type SourceSftpBulkUpdateFromCSV struct {
	headerDefinitionType *string `const:"From CSV" json:"header_definition_type"`
}

func (s SourceSftpBulkUpdateFromCSV) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkUpdateFromCSV) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkUpdateFromCSV) GetHeaderDefinitionType() *string {
	return types.Pointer("From CSV")
}

type SourceSftpBulkUpdateCSVHeaderDefinitionType string

const (
	SourceSftpBulkUpdateCSVHeaderDefinitionTypeSourceSftpBulkUpdateFromCSV       SourceSftpBulkUpdateCSVHeaderDefinitionType = "source-sftp-bulk-update_From CSV"
	SourceSftpBulkUpdateCSVHeaderDefinitionTypeSourceSftpBulkUpdateAutogenerated SourceSftpBulkUpdateCSVHeaderDefinitionType = "source-sftp-bulk-update_Autogenerated"
	SourceSftpBulkUpdateCSVHeaderDefinitionTypeSourceSftpBulkUpdateUserProvided  SourceSftpBulkUpdateCSVHeaderDefinitionType = "source-sftp-bulk-update_User Provided"
)

// SourceSftpBulkUpdateCSVHeaderDefinition - How headers will be defined. `User Provided` assumes the CSV does not have a header row and uses the headers provided and `Autogenerated` assumes the CSV does not have a header row and the CDK will generate headers using for `f{i}` where `i` is the index starting from 0. Else, the default behavior is to use the header from the CSV file. If a user wants to autogenerate or provide column names for a CSV having headers, they can skip rows.
type SourceSftpBulkUpdateCSVHeaderDefinition struct {
	SourceSftpBulkUpdateFromCSV       *SourceSftpBulkUpdateFromCSV       `queryParam:"inline" union:"member"`
	SourceSftpBulkUpdateAutogenerated *SourceSftpBulkUpdateAutogenerated `queryParam:"inline" union:"member"`
	SourceSftpBulkUpdateUserProvided  *SourceSftpBulkUpdateUserProvided  `queryParam:"inline" union:"member"`

	Type SourceSftpBulkUpdateCSVHeaderDefinitionType
}

func CreateSourceSftpBulkUpdateCSVHeaderDefinitionSourceSftpBulkUpdateFromCSV(sourceSftpBulkUpdateFromCSV SourceSftpBulkUpdateFromCSV) SourceSftpBulkUpdateCSVHeaderDefinition {
	typ := SourceSftpBulkUpdateCSVHeaderDefinitionTypeSourceSftpBulkUpdateFromCSV

	return SourceSftpBulkUpdateCSVHeaderDefinition{
		SourceSftpBulkUpdateFromCSV: &sourceSftpBulkUpdateFromCSV,
		Type:                        typ,
	}
}

func CreateSourceSftpBulkUpdateCSVHeaderDefinitionSourceSftpBulkUpdateAutogenerated(sourceSftpBulkUpdateAutogenerated SourceSftpBulkUpdateAutogenerated) SourceSftpBulkUpdateCSVHeaderDefinition {
	typ := SourceSftpBulkUpdateCSVHeaderDefinitionTypeSourceSftpBulkUpdateAutogenerated

	return SourceSftpBulkUpdateCSVHeaderDefinition{
		SourceSftpBulkUpdateAutogenerated: &sourceSftpBulkUpdateAutogenerated,
		Type:                              typ,
	}
}

func CreateSourceSftpBulkUpdateCSVHeaderDefinitionSourceSftpBulkUpdateUserProvided(sourceSftpBulkUpdateUserProvided SourceSftpBulkUpdateUserProvided) SourceSftpBulkUpdateCSVHeaderDefinition {
	typ := SourceSftpBulkUpdateCSVHeaderDefinitionTypeSourceSftpBulkUpdateUserProvided

	return SourceSftpBulkUpdateCSVHeaderDefinition{
		SourceSftpBulkUpdateUserProvided: &sourceSftpBulkUpdateUserProvided,
		Type:                             typ,
	}
}

func (u *SourceSftpBulkUpdateCSVHeaderDefinition) UnmarshalJSON(data []byte) error {

	var candidates []utils.UnionCandidate

	// Collect all valid candidates
	var sourceSftpBulkUpdateFromCSV SourceSftpBulkUpdateFromCSV = SourceSftpBulkUpdateFromCSV{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkUpdateFromCSV, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkUpdateCSVHeaderDefinitionTypeSourceSftpBulkUpdateFromCSV,
			Value: &sourceSftpBulkUpdateFromCSV,
		})
	}

	var sourceSftpBulkUpdateAutogenerated SourceSftpBulkUpdateAutogenerated = SourceSftpBulkUpdateAutogenerated{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkUpdateAutogenerated, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkUpdateCSVHeaderDefinitionTypeSourceSftpBulkUpdateAutogenerated,
			Value: &sourceSftpBulkUpdateAutogenerated,
		})
	}

	var sourceSftpBulkUpdateUserProvided SourceSftpBulkUpdateUserProvided = SourceSftpBulkUpdateUserProvided{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkUpdateUserProvided, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkUpdateCSVHeaderDefinitionTypeSourceSftpBulkUpdateUserProvided,
			Value: &sourceSftpBulkUpdateUserProvided,
		})
	}

	if len(candidates) == 0 {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkUpdateCSVHeaderDefinition", string(data))
	}

	// Pick the best candidate using multi-stage filtering
	best := utils.PickBestUnionCandidate(candidates, data)
	if best == nil {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkUpdateCSVHeaderDefinition", string(data))
	}

	// Set the union type and value based on the best candidate
	u.Type = best.Type.(SourceSftpBulkUpdateCSVHeaderDefinitionType)
	switch best.Type {
	case SourceSftpBulkUpdateCSVHeaderDefinitionTypeSourceSftpBulkUpdateFromCSV:
		u.SourceSftpBulkUpdateFromCSV = best.Value.(*SourceSftpBulkUpdateFromCSV)
		return nil
	case SourceSftpBulkUpdateCSVHeaderDefinitionTypeSourceSftpBulkUpdateAutogenerated:
		u.SourceSftpBulkUpdateAutogenerated = best.Value.(*SourceSftpBulkUpdateAutogenerated)
		return nil
	case SourceSftpBulkUpdateCSVHeaderDefinitionTypeSourceSftpBulkUpdateUserProvided:
		u.SourceSftpBulkUpdateUserProvided = best.Value.(*SourceSftpBulkUpdateUserProvided)
		return nil
	}

	return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkUpdateCSVHeaderDefinition", string(data))
}

func (u SourceSftpBulkUpdateCSVHeaderDefinition) MarshalJSON() ([]byte, error) {
	if u.SourceSftpBulkUpdateFromCSV != nil {
		return utils.MarshalJSON(u.SourceSftpBulkUpdateFromCSV, "", true)
	}

	if u.SourceSftpBulkUpdateAutogenerated != nil {
		return utils.MarshalJSON(u.SourceSftpBulkUpdateAutogenerated, "", true)
	}

	if u.SourceSftpBulkUpdateUserProvided != nil {
		return utils.MarshalJSON(u.SourceSftpBulkUpdateUserProvided, "", true)
	}

	return nil, errors.New("could not marshal union type SourceSftpBulkUpdateCSVHeaderDefinition: all fields are null")
}

// SourceSftpBulkUpdateInferenceType - How to infer the types of the columns. If none, inference default to strings.
type SourceSftpBulkUpdateInferenceType string

const (
	SourceSftpBulkUpdateInferenceTypeNone               SourceSftpBulkUpdateInferenceType = "None"
	SourceSftpBulkUpdateInferenceTypePrimitiveTypesOnly SourceSftpBulkUpdateInferenceType = "Primitive Types Only"
)

func (e SourceSftpBulkUpdateInferenceType) ToPointer() *SourceSftpBulkUpdateInferenceType {
	return &e
}
func (e *SourceSftpBulkUpdateInferenceType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "None":
		fallthrough
	case "Primitive Types Only":
		*e = SourceSftpBulkUpdateInferenceType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceSftpBulkUpdateInferenceType: %v", v)
	}
}

type SourceSftpBulkUpdateCSVFormat struct {
	// The character delimiting individual cells in the CSV data. This may only be a 1-character string. For tab-delimited data enter '\t'.
	Delimiter *string `default:"," json:"delimiter"`
	// Whether two quotes in a quoted CSV value denote a single quote in the data.
	DoubleQuote *bool `default:"true" json:"double_quote"`
	// The character encoding of the CSV data. Leave blank to default to <strong>UTF8</strong>. See <a href="https://docs.python.org/3/library/codecs.html#standard-encodings" target="_blank">list of python encodings</a> for allowable options.
	Encoding *string `default:"utf8" json:"encoding"`
	// The character used for escaping special characters. To disallow escaping, leave this field blank.
	EscapeChar *string `json:"escape_char,omitempty"`
	// A set of case-sensitive strings that should be interpreted as false values.
	FalseValues []string `json:"false_values,omitempty"`
	filetype    *string  `const:"csv" json:"filetype"`
	// How headers will be defined. `User Provided` assumes the CSV does not have a header row and uses the headers provided and `Autogenerated` assumes the CSV does not have a header row and the CDK will generate headers using for `f{i}` where `i` is the index starting from 0. Else, the default behavior is to use the header from the CSV file. If a user wants to autogenerate or provide column names for a CSV having headers, they can skip rows.
	HeaderDefinition *SourceSftpBulkUpdateCSVHeaderDefinition `json:"header_definition,omitempty"`
	// Whether to ignore errors that occur when the number of fields in the CSV does not match the number of columns in the schema.
	IgnoreErrorsOnFieldsMismatch *bool `default:"false" json:"ignore_errors_on_fields_mismatch"`
	// How to infer the types of the columns. If none, inference default to strings.
	InferenceType *SourceSftpBulkUpdateInferenceType `json:"inference_type,omitempty"`
	// A set of case-sensitive strings that should be interpreted as null values. For example, if the value 'NA' should be interpreted as null, enter 'NA' in this field.
	NullValues []string `json:"null_values,omitempty"`
	// The character used for quoting CSV values. To disallow quoting, make this field blank.
	QuoteChar *string `default:"\"" json:"quote_char"`
	// The number of rows to skip after the header row.
	SkipRowsAfterHeader *int64 `default:"0" json:"skip_rows_after_header"`
	// The number of rows to skip before the header row. For example, if the header row is on the 3rd row, enter 2 in this field.
	SkipRowsBeforeHeader *int64 `default:"0" json:"skip_rows_before_header"`
	// Whether strings can be interpreted as null values. If true, strings that match the null_values set will be interpreted as null. If false, strings that match the null_values set will be interpreted as the string itself.
	StringsCanBeNull *bool `default:"true" json:"strings_can_be_null"`
	// A set of case-sensitive strings that should be interpreted as true values.
	TrueValues []string `json:"true_values,omitempty"`
}

func (s SourceSftpBulkUpdateCSVFormat) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkUpdateCSVFormat) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkUpdateCSVFormat) GetDelimiter() *string {
	if s == nil {
		return nil
	}
	return s.Delimiter
}

func (s *SourceSftpBulkUpdateCSVFormat) GetDoubleQuote() *bool {
	if s == nil {
		return nil
	}
	return s.DoubleQuote
}

func (s *SourceSftpBulkUpdateCSVFormat) GetEncoding() *string {
	if s == nil {
		return nil
	}
	return s.Encoding
}

func (s *SourceSftpBulkUpdateCSVFormat) GetEscapeChar() *string {
	if s == nil {
		return nil
	}
	return s.EscapeChar
}

func (s *SourceSftpBulkUpdateCSVFormat) GetFalseValues() []string {
	if s == nil {
		return nil
	}
	return s.FalseValues
}

func (s *SourceSftpBulkUpdateCSVFormat) GetFiletype() *string {
	return types.Pointer("csv")
}

func (s *SourceSftpBulkUpdateCSVFormat) GetHeaderDefinition() *SourceSftpBulkUpdateCSVHeaderDefinition {
	if s == nil {
		return nil
	}
	return s.HeaderDefinition
}

func (s *SourceSftpBulkUpdateCSVFormat) GetIgnoreErrorsOnFieldsMismatch() *bool {
	if s == nil {
		return nil
	}
	return s.IgnoreErrorsOnFieldsMismatch
}

func (s *SourceSftpBulkUpdateCSVFormat) GetInferenceType() *SourceSftpBulkUpdateInferenceType {
	if s == nil {
		return nil
	}
	return s.InferenceType
}

func (s *SourceSftpBulkUpdateCSVFormat) GetNullValues() []string {
	if s == nil {
		return nil
	}
	return s.NullValues
}

func (s *SourceSftpBulkUpdateCSVFormat) GetQuoteChar() *string {
	if s == nil {
		return nil
	}
	return s.QuoteChar
}

func (s *SourceSftpBulkUpdateCSVFormat) GetSkipRowsAfterHeader() *int64 {
	if s == nil {
		return nil
	}
	return s.SkipRowsAfterHeader
}

func (s *SourceSftpBulkUpdateCSVFormat) GetSkipRowsBeforeHeader() *int64 {
	if s == nil {
		return nil
	}
	return s.SkipRowsBeforeHeader
}

func (s *SourceSftpBulkUpdateCSVFormat) GetStringsCanBeNull() *bool {
	if s == nil {
		return nil
	}
	return s.StringsCanBeNull
}

func (s *SourceSftpBulkUpdateCSVFormat) GetTrueValues() []string {
	if s == nil {
		return nil
	}
	return s.TrueValues
}

type SourceSftpBulkUpdateAvroFormat struct {
	// Whether to convert double fields to strings. This is recommended if you have decimal numbers with a high degree of precision because there can be a loss precision when handling floating point numbers.
	DoubleAsString *bool   `default:"false" json:"double_as_string"`
	filetype       *string `const:"avro" json:"filetype"`
}

func (s SourceSftpBulkUpdateAvroFormat) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkUpdateAvroFormat) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkUpdateAvroFormat) GetDoubleAsString() *bool {
	if s == nil {
		return nil
	}
	return s.DoubleAsString
}

func (s *SourceSftpBulkUpdateAvroFormat) GetFiletype() *string {
	return types.Pointer("avro")
}

type SourceSftpBulkUpdateFormatType string

const (
	SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateAvroFormat                 SourceSftpBulkUpdateFormatType = "source-sftp-bulk-update_Avro Format"
	SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateCSVFormat                  SourceSftpBulkUpdateFormatType = "source-sftp-bulk-update_CSV Format"
	SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateJsonlFormat                SourceSftpBulkUpdateFormatType = "source-sftp-bulk-update_Jsonl Format"
	SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateParquetFormat              SourceSftpBulkUpdateFormatType = "source-sftp-bulk-update_Parquet Format"
	SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateUnstructuredDocumentFormat SourceSftpBulkUpdateFormatType = "source-sftp-bulk-update_Unstructured Document Format"
	SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateExcelFormat                SourceSftpBulkUpdateFormatType = "source-sftp-bulk-update_Excel Format"
)

// SourceSftpBulkUpdateFormat - The configuration options that are used to alter how to read incoming files that deviate from the standard formatting.
type SourceSftpBulkUpdateFormat struct {
	SourceSftpBulkUpdateAvroFormat                 *SourceSftpBulkUpdateAvroFormat                 `queryParam:"inline" union:"member"`
	SourceSftpBulkUpdateCSVFormat                  *SourceSftpBulkUpdateCSVFormat                  `queryParam:"inline" union:"member"`
	SourceSftpBulkUpdateJsonlFormat                *SourceSftpBulkUpdateJsonlFormat                `queryParam:"inline" union:"member"`
	SourceSftpBulkUpdateParquetFormat              *SourceSftpBulkUpdateParquetFormat              `queryParam:"inline" union:"member"`
	SourceSftpBulkUpdateUnstructuredDocumentFormat *SourceSftpBulkUpdateUnstructuredDocumentFormat `queryParam:"inline" union:"member"`
	SourceSftpBulkUpdateExcelFormat                *SourceSftpBulkUpdateExcelFormat                `queryParam:"inline" union:"member"`

	Type SourceSftpBulkUpdateFormatType
}

func CreateSourceSftpBulkUpdateFormatSourceSftpBulkUpdateAvroFormat(sourceSftpBulkUpdateAvroFormat SourceSftpBulkUpdateAvroFormat) SourceSftpBulkUpdateFormat {
	typ := SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateAvroFormat

	return SourceSftpBulkUpdateFormat{
		SourceSftpBulkUpdateAvroFormat: &sourceSftpBulkUpdateAvroFormat,
		Type:                           typ,
	}
}

func CreateSourceSftpBulkUpdateFormatSourceSftpBulkUpdateCSVFormat(sourceSftpBulkUpdateCSVFormat SourceSftpBulkUpdateCSVFormat) SourceSftpBulkUpdateFormat {
	typ := SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateCSVFormat

	return SourceSftpBulkUpdateFormat{
		SourceSftpBulkUpdateCSVFormat: &sourceSftpBulkUpdateCSVFormat,
		Type:                          typ,
	}
}

func CreateSourceSftpBulkUpdateFormatSourceSftpBulkUpdateJsonlFormat(sourceSftpBulkUpdateJsonlFormat SourceSftpBulkUpdateJsonlFormat) SourceSftpBulkUpdateFormat {
	typ := SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateJsonlFormat

	return SourceSftpBulkUpdateFormat{
		SourceSftpBulkUpdateJsonlFormat: &sourceSftpBulkUpdateJsonlFormat,
		Type:                            typ,
	}
}

func CreateSourceSftpBulkUpdateFormatSourceSftpBulkUpdateParquetFormat(sourceSftpBulkUpdateParquetFormat SourceSftpBulkUpdateParquetFormat) SourceSftpBulkUpdateFormat {
	typ := SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateParquetFormat

	return SourceSftpBulkUpdateFormat{
		SourceSftpBulkUpdateParquetFormat: &sourceSftpBulkUpdateParquetFormat,
		Type:                              typ,
	}
}

func CreateSourceSftpBulkUpdateFormatSourceSftpBulkUpdateUnstructuredDocumentFormat(sourceSftpBulkUpdateUnstructuredDocumentFormat SourceSftpBulkUpdateUnstructuredDocumentFormat) SourceSftpBulkUpdateFormat {
	typ := SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateUnstructuredDocumentFormat

	return SourceSftpBulkUpdateFormat{
		SourceSftpBulkUpdateUnstructuredDocumentFormat: &sourceSftpBulkUpdateUnstructuredDocumentFormat,
		Type: typ,
	}
}

func CreateSourceSftpBulkUpdateFormatSourceSftpBulkUpdateExcelFormat(sourceSftpBulkUpdateExcelFormat SourceSftpBulkUpdateExcelFormat) SourceSftpBulkUpdateFormat {
	typ := SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateExcelFormat

	return SourceSftpBulkUpdateFormat{
		SourceSftpBulkUpdateExcelFormat: &sourceSftpBulkUpdateExcelFormat,
		Type:                            typ,
	}
}

func (u *SourceSftpBulkUpdateFormat) UnmarshalJSON(data []byte) error {

	var candidates []utils.UnionCandidate

	// Collect all valid candidates
	var sourceSftpBulkUpdateAvroFormat SourceSftpBulkUpdateAvroFormat = SourceSftpBulkUpdateAvroFormat{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkUpdateAvroFormat, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateAvroFormat,
			Value: &sourceSftpBulkUpdateAvroFormat,
		})
	}

	var sourceSftpBulkUpdateCSVFormat SourceSftpBulkUpdateCSVFormat = SourceSftpBulkUpdateCSVFormat{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkUpdateCSVFormat, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateCSVFormat,
			Value: &sourceSftpBulkUpdateCSVFormat,
		})
	}

	var sourceSftpBulkUpdateJsonlFormat SourceSftpBulkUpdateJsonlFormat = SourceSftpBulkUpdateJsonlFormat{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkUpdateJsonlFormat, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateJsonlFormat,
			Value: &sourceSftpBulkUpdateJsonlFormat,
		})
	}

	var sourceSftpBulkUpdateParquetFormat SourceSftpBulkUpdateParquetFormat = SourceSftpBulkUpdateParquetFormat{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkUpdateParquetFormat, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateParquetFormat,
			Value: &sourceSftpBulkUpdateParquetFormat,
		})
	}

	var sourceSftpBulkUpdateUnstructuredDocumentFormat SourceSftpBulkUpdateUnstructuredDocumentFormat = SourceSftpBulkUpdateUnstructuredDocumentFormat{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkUpdateUnstructuredDocumentFormat, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateUnstructuredDocumentFormat,
			Value: &sourceSftpBulkUpdateUnstructuredDocumentFormat,
		})
	}

	var sourceSftpBulkUpdateExcelFormat SourceSftpBulkUpdateExcelFormat = SourceSftpBulkUpdateExcelFormat{}
	if err := utils.UnmarshalJSON(data, &sourceSftpBulkUpdateExcelFormat, "", true, nil); err == nil {
		candidates = append(candidates, utils.UnionCandidate{
			Type:  SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateExcelFormat,
			Value: &sourceSftpBulkUpdateExcelFormat,
		})
	}

	if len(candidates) == 0 {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkUpdateFormat", string(data))
	}

	// Pick the best candidate using multi-stage filtering
	best := utils.PickBestUnionCandidate(candidates, data)
	if best == nil {
		return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkUpdateFormat", string(data))
	}

	// Set the union type and value based on the best candidate
	u.Type = best.Type.(SourceSftpBulkUpdateFormatType)
	switch best.Type {
	case SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateAvroFormat:
		u.SourceSftpBulkUpdateAvroFormat = best.Value.(*SourceSftpBulkUpdateAvroFormat)
		return nil
	case SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateCSVFormat:
		u.SourceSftpBulkUpdateCSVFormat = best.Value.(*SourceSftpBulkUpdateCSVFormat)
		return nil
	case SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateJsonlFormat:
		u.SourceSftpBulkUpdateJsonlFormat = best.Value.(*SourceSftpBulkUpdateJsonlFormat)
		return nil
	case SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateParquetFormat:
		u.SourceSftpBulkUpdateParquetFormat = best.Value.(*SourceSftpBulkUpdateParquetFormat)
		return nil
	case SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateUnstructuredDocumentFormat:
		u.SourceSftpBulkUpdateUnstructuredDocumentFormat = best.Value.(*SourceSftpBulkUpdateUnstructuredDocumentFormat)
		return nil
	case SourceSftpBulkUpdateFormatTypeSourceSftpBulkUpdateExcelFormat:
		u.SourceSftpBulkUpdateExcelFormat = best.Value.(*SourceSftpBulkUpdateExcelFormat)
		return nil
	}

	return fmt.Errorf("could not unmarshal `%s` into any supported union types for SourceSftpBulkUpdateFormat", string(data))
}

func (u SourceSftpBulkUpdateFormat) MarshalJSON() ([]byte, error) {
	if u.SourceSftpBulkUpdateAvroFormat != nil {
		return utils.MarshalJSON(u.SourceSftpBulkUpdateAvroFormat, "", true)
	}

	if u.SourceSftpBulkUpdateCSVFormat != nil {
		return utils.MarshalJSON(u.SourceSftpBulkUpdateCSVFormat, "", true)
	}

	if u.SourceSftpBulkUpdateJsonlFormat != nil {
		return utils.MarshalJSON(u.SourceSftpBulkUpdateJsonlFormat, "", true)
	}

	if u.SourceSftpBulkUpdateParquetFormat != nil {
		return utils.MarshalJSON(u.SourceSftpBulkUpdateParquetFormat, "", true)
	}

	if u.SourceSftpBulkUpdateUnstructuredDocumentFormat != nil {
		return utils.MarshalJSON(u.SourceSftpBulkUpdateUnstructuredDocumentFormat, "", true)
	}

	if u.SourceSftpBulkUpdateExcelFormat != nil {
		return utils.MarshalJSON(u.SourceSftpBulkUpdateExcelFormat, "", true)
	}

	return nil, errors.New("could not marshal union type SourceSftpBulkUpdateFormat: all fields are null")
}

// SourceSftpBulkUpdateValidationPolicy - The name of the validation policy that dictates sync behavior when a record does not adhere to the stream schema.
type SourceSftpBulkUpdateValidationPolicy string

const (
	SourceSftpBulkUpdateValidationPolicyEmitRecord      SourceSftpBulkUpdateValidationPolicy = "Emit Record"
	SourceSftpBulkUpdateValidationPolicySkipRecord      SourceSftpBulkUpdateValidationPolicy = "Skip Record"
	SourceSftpBulkUpdateValidationPolicyWaitForDiscover SourceSftpBulkUpdateValidationPolicy = "Wait for Discover"
)

func (e SourceSftpBulkUpdateValidationPolicy) ToPointer() *SourceSftpBulkUpdateValidationPolicy {
	return &e
}
func (e *SourceSftpBulkUpdateValidationPolicy) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "Emit Record":
		fallthrough
	case "Skip Record":
		fallthrough
	case "Wait for Discover":
		*e = SourceSftpBulkUpdateValidationPolicy(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceSftpBulkUpdateValidationPolicy: %v", v)
	}
}

type SourceSftpBulkUpdateFileBasedStreamConfig struct {
	// When the state history of the file store is full, syncs will only read files that were last modified in the provided day range.
	DaysToSyncIfHistoryIsFull *int64 `default:"3" json:"days_to_sync_if_history_is_full"`
	// The configuration options that are used to alter how to read incoming files that deviate from the standard formatting.
	Format *SourceSftpBulkUpdateFormat `json:"format,omitempty"`
	// The pattern used to specify which files should be selected from the file system. For more information on glob pattern matching look <a href="https://en.wikipedia.org/wiki/Glob_(programming)">here</a>.
	Globs []string `json:"globs,omitempty"`
	// The schema that will be used to validate records extracted from the file. This will override the stream schema that is auto-detected from incoming files.
	InputSchema *string `json:"input_schema,omitempty"`
	// The path prefix configured in v3 versions of the S3 connector. This option is deprecated in favor of a single glob.
	LegacyPrefix *string `json:"legacy_prefix,omitempty"`
	// The name of the stream.
	Name *string `json:"name,omitempty"`
	// The column or columns (for a composite key) that serves as the unique identifier of a record. If empty, the primary key will default to the parser's default primary key.
	PrimaryKey *string `json:"primary_key,omitempty"`
	// The number of resent files which will be used to discover the schema for this stream.
	RecentNFilesToReadForSchemaDiscovery *int64 `json:"recent_n_files_to_read_for_schema_discovery,omitempty"`
	// When enabled, syncs will not validate or structure records against the stream's schema.
	Schemaless *bool `default:"false" json:"schemaless"`
	// When enabled, the source will use the first found file for schema discovery. Helps to avoid long discovery step.
	UseFirstFoundFileForSchemaDiscovery *bool `default:"false" json:"use_first_found_file_for_schema_discovery"`
	// The name of the validation policy that dictates sync behavior when a record does not adhere to the stream schema.
	ValidationPolicy *SourceSftpBulkUpdateValidationPolicy `default:"Emit Record" json:"validation_policy"`
}

func (s SourceSftpBulkUpdateFileBasedStreamConfig) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkUpdateFileBasedStreamConfig) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkUpdateFileBasedStreamConfig) GetDaysToSyncIfHistoryIsFull() *int64 {
	if s == nil {
		return nil
	}
	return s.DaysToSyncIfHistoryIsFull
}

func (s *SourceSftpBulkUpdateFileBasedStreamConfig) GetFormat() *SourceSftpBulkUpdateFormat {
	if s == nil {
		return nil
	}
	return s.Format
}

func (s *SourceSftpBulkUpdateFileBasedStreamConfig) GetGlobs() []string {
	if s == nil {
		return nil
	}
	return s.Globs
}

func (s *SourceSftpBulkUpdateFileBasedStreamConfig) GetInputSchema() *string {
	if s == nil {
		return nil
	}
	return s.InputSchema
}

func (s *SourceSftpBulkUpdateFileBasedStreamConfig) GetLegacyPrefix() *string {
	if s == nil {
		return nil
	}
	return s.LegacyPrefix
}

func (s *SourceSftpBulkUpdateFileBasedStreamConfig) GetName() *string {
	if s == nil {
		return nil
	}
	return s.Name
}

func (s *SourceSftpBulkUpdateFileBasedStreamConfig) GetPrimaryKey() *string {
	if s == nil {
		return nil
	}
	return s.PrimaryKey
}

func (s *SourceSftpBulkUpdateFileBasedStreamConfig) GetRecentNFilesToReadForSchemaDiscovery() *int64 {
	if s == nil {
		return nil
	}
	return s.RecentNFilesToReadForSchemaDiscovery
}

func (s *SourceSftpBulkUpdateFileBasedStreamConfig) GetSchemaless() *bool {
	if s == nil {
		return nil
	}
	return s.Schemaless
}

func (s *SourceSftpBulkUpdateFileBasedStreamConfig) GetUseFirstFoundFileForSchemaDiscovery() *bool {
	if s == nil {
		return nil
	}
	return s.UseFirstFoundFileForSchemaDiscovery
}

func (s *SourceSftpBulkUpdateFileBasedStreamConfig) GetValidationPolicy() *SourceSftpBulkUpdateValidationPolicy {
	if s == nil {
		return nil
	}
	return s.ValidationPolicy
}

type SourceSftpBulkUpdateSourceType string

const (
	SourceSftpBulkUpdateSourceTypeSftpBulk SourceSftpBulkUpdateSourceType = "sftp-bulk"
)

func (e SourceSftpBulkUpdateSourceType) ToPointer() *SourceSftpBulkUpdateSourceType {
	return &e
}
func (e *SourceSftpBulkUpdateSourceType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "sftp-bulk":
		*e = SourceSftpBulkUpdateSourceType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceSftpBulkUpdateSourceType: %v", v)
	}
}

// SourceSftpBulkUpdate - Used during spec; allows the developer to configure the cloud provider specific options that are needed when users configure a file-based source.
type SourceSftpBulkUpdate struct {
	// Credentials for connecting to the SFTP Server
	Credentials    *SourceSftpBulkUpdateAuthentication `json:"credentials,omitempty"`
	DeliveryMethod *SourceSftpBulkUpdateDeliveryMethod `json:"delivery_method,omitempty"`
	// The directory to search files for sync
	FolderPath *string `default:"/" json:"folder_path"`
	// The server host address
	Host *string `json:"host,omitempty"`
	// The server port
	Port *int64 `default:"22" json:"port"`
	// UTC date and time in the format 2017-01-25T00:00:00.000000Z. Any file modified before this date will not be replicated.
	StartDate *time.Time `json:"start_date,omitempty"`
	// Each instance of this configuration defines a <a href="https://docs.airbyte.com/cloud/core-concepts#stream">stream</a>. Use this to define which files belong in the stream, their format, and how they should be parsed and validated. When sending data to warehouse destination such as Snowflake or BigQuery, each stream is a separate table.
	Streams []SourceSftpBulkUpdateFileBasedStreamConfig `json:"streams,omitempty"`
	// The server user
	Username   *string                         `json:"username,omitempty"`
	sourceType *SourceSftpBulkUpdateSourceType `const:"sftp-bulk" json:"sourceType"`
}

func (s SourceSftpBulkUpdate) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *SourceSftpBulkUpdate) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, nil); err != nil {
		return err
	}
	return nil
}

func (s *SourceSftpBulkUpdate) GetCredentials() *SourceSftpBulkUpdateAuthentication {
	if s == nil {
		return nil
	}
	return s.Credentials
}

func (s *SourceSftpBulkUpdate) GetDeliveryMethod() *SourceSftpBulkUpdateDeliveryMethod {
	if s == nil {
		return nil
	}
	return s.DeliveryMethod
}

func (s *SourceSftpBulkUpdate) GetFolderPath() *string {
	if s == nil {
		return nil
	}
	return s.FolderPath
}

func (s *SourceSftpBulkUpdate) GetHost() *string {
	if s == nil {
		return nil
	}
	return s.Host
}

func (s *SourceSftpBulkUpdate) GetPort() *int64 {
	if s == nil {
		return nil
	}
	return s.Port
}

func (s *SourceSftpBulkUpdate) GetStartDate() *time.Time {
	if s == nil {
		return nil
	}
	return s.StartDate
}

func (s *SourceSftpBulkUpdate) GetStreams() []SourceSftpBulkUpdateFileBasedStreamConfig {
	if s == nil {
		return nil
	}
	return s.Streams
}

func (s *SourceSftpBulkUpdate) GetUsername() *string {
	if s == nil {
		return nil
	}
	return s.Username
}

func (s *SourceSftpBulkUpdate) GetSourceType() *SourceSftpBulkUpdateSourceType {
	return SourceSftpBulkUpdateSourceTypeSftpBulk.ToPointer()
}
