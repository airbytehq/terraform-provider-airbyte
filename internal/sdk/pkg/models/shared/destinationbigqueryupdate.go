// Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.

package shared

import (
	"encoding/json"
	"errors"
	"fmt"
	"github.com/airbytehq/terraform-provider-airbyte/internal/sdk/pkg/utils"
)

// DestinationBigqueryUpdateDatasetLocation - The location of the dataset. Warning: Changes made after creation will not be applied. Read more <a href="https://cloud.google.com/bigquery/docs/locations">here</a>.
type DestinationBigqueryUpdateDatasetLocation string

const (
	DestinationBigqueryUpdateDatasetLocationUs                     DestinationBigqueryUpdateDatasetLocation = "US"
	DestinationBigqueryUpdateDatasetLocationEu                     DestinationBigqueryUpdateDatasetLocation = "EU"
	DestinationBigqueryUpdateDatasetLocationAsiaEast1              DestinationBigqueryUpdateDatasetLocation = "asia-east1"
	DestinationBigqueryUpdateDatasetLocationAsiaEast2              DestinationBigqueryUpdateDatasetLocation = "asia-east2"
	DestinationBigqueryUpdateDatasetLocationAsiaNortheast1         DestinationBigqueryUpdateDatasetLocation = "asia-northeast1"
	DestinationBigqueryUpdateDatasetLocationAsiaNortheast2         DestinationBigqueryUpdateDatasetLocation = "asia-northeast2"
	DestinationBigqueryUpdateDatasetLocationAsiaNortheast3         DestinationBigqueryUpdateDatasetLocation = "asia-northeast3"
	DestinationBigqueryUpdateDatasetLocationAsiaSouth1             DestinationBigqueryUpdateDatasetLocation = "asia-south1"
	DestinationBigqueryUpdateDatasetLocationAsiaSouth2             DestinationBigqueryUpdateDatasetLocation = "asia-south2"
	DestinationBigqueryUpdateDatasetLocationAsiaSoutheast1         DestinationBigqueryUpdateDatasetLocation = "asia-southeast1"
	DestinationBigqueryUpdateDatasetLocationAsiaSoutheast2         DestinationBigqueryUpdateDatasetLocation = "asia-southeast2"
	DestinationBigqueryUpdateDatasetLocationAustraliaSoutheast1    DestinationBigqueryUpdateDatasetLocation = "australia-southeast1"
	DestinationBigqueryUpdateDatasetLocationAustraliaSoutheast2    DestinationBigqueryUpdateDatasetLocation = "australia-southeast2"
	DestinationBigqueryUpdateDatasetLocationEuropeCentral1         DestinationBigqueryUpdateDatasetLocation = "europe-central1"
	DestinationBigqueryUpdateDatasetLocationEuropeCentral2         DestinationBigqueryUpdateDatasetLocation = "europe-central2"
	DestinationBigqueryUpdateDatasetLocationEuropeNorth1           DestinationBigqueryUpdateDatasetLocation = "europe-north1"
	DestinationBigqueryUpdateDatasetLocationEuropeSouthwest1       DestinationBigqueryUpdateDatasetLocation = "europe-southwest1"
	DestinationBigqueryUpdateDatasetLocationEuropeWest1            DestinationBigqueryUpdateDatasetLocation = "europe-west1"
	DestinationBigqueryUpdateDatasetLocationEuropeWest2            DestinationBigqueryUpdateDatasetLocation = "europe-west2"
	DestinationBigqueryUpdateDatasetLocationEuropeWest3            DestinationBigqueryUpdateDatasetLocation = "europe-west3"
	DestinationBigqueryUpdateDatasetLocationEuropeWest4            DestinationBigqueryUpdateDatasetLocation = "europe-west4"
	DestinationBigqueryUpdateDatasetLocationEuropeWest6            DestinationBigqueryUpdateDatasetLocation = "europe-west6"
	DestinationBigqueryUpdateDatasetLocationEuropeWest7            DestinationBigqueryUpdateDatasetLocation = "europe-west7"
	DestinationBigqueryUpdateDatasetLocationEuropeWest8            DestinationBigqueryUpdateDatasetLocation = "europe-west8"
	DestinationBigqueryUpdateDatasetLocationEuropeWest9            DestinationBigqueryUpdateDatasetLocation = "europe-west9"
	DestinationBigqueryUpdateDatasetLocationMeWest1                DestinationBigqueryUpdateDatasetLocation = "me-west1"
	DestinationBigqueryUpdateDatasetLocationNorthamericaNortheast1 DestinationBigqueryUpdateDatasetLocation = "northamerica-northeast1"
	DestinationBigqueryUpdateDatasetLocationNorthamericaNortheast2 DestinationBigqueryUpdateDatasetLocation = "northamerica-northeast2"
	DestinationBigqueryUpdateDatasetLocationSouthamericaEast1      DestinationBigqueryUpdateDatasetLocation = "southamerica-east1"
	DestinationBigqueryUpdateDatasetLocationSouthamericaWest1      DestinationBigqueryUpdateDatasetLocation = "southamerica-west1"
	DestinationBigqueryUpdateDatasetLocationUsCentral1             DestinationBigqueryUpdateDatasetLocation = "us-central1"
	DestinationBigqueryUpdateDatasetLocationUsEast1                DestinationBigqueryUpdateDatasetLocation = "us-east1"
	DestinationBigqueryUpdateDatasetLocationUsEast2                DestinationBigqueryUpdateDatasetLocation = "us-east2"
	DestinationBigqueryUpdateDatasetLocationUsEast3                DestinationBigqueryUpdateDatasetLocation = "us-east3"
	DestinationBigqueryUpdateDatasetLocationUsEast4                DestinationBigqueryUpdateDatasetLocation = "us-east4"
	DestinationBigqueryUpdateDatasetLocationUsEast5                DestinationBigqueryUpdateDatasetLocation = "us-east5"
	DestinationBigqueryUpdateDatasetLocationUsWest1                DestinationBigqueryUpdateDatasetLocation = "us-west1"
	DestinationBigqueryUpdateDatasetLocationUsWest2                DestinationBigqueryUpdateDatasetLocation = "us-west2"
	DestinationBigqueryUpdateDatasetLocationUsWest3                DestinationBigqueryUpdateDatasetLocation = "us-west3"
	DestinationBigqueryUpdateDatasetLocationUsWest4                DestinationBigqueryUpdateDatasetLocation = "us-west4"
)

func (e DestinationBigqueryUpdateDatasetLocation) ToPointer() *DestinationBigqueryUpdateDatasetLocation {
	return &e
}

func (e *DestinationBigqueryUpdateDatasetLocation) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "US":
		fallthrough
	case "EU":
		fallthrough
	case "asia-east1":
		fallthrough
	case "asia-east2":
		fallthrough
	case "asia-northeast1":
		fallthrough
	case "asia-northeast2":
		fallthrough
	case "asia-northeast3":
		fallthrough
	case "asia-south1":
		fallthrough
	case "asia-south2":
		fallthrough
	case "asia-southeast1":
		fallthrough
	case "asia-southeast2":
		fallthrough
	case "australia-southeast1":
		fallthrough
	case "australia-southeast2":
		fallthrough
	case "europe-central1":
		fallthrough
	case "europe-central2":
		fallthrough
	case "europe-north1":
		fallthrough
	case "europe-southwest1":
		fallthrough
	case "europe-west1":
		fallthrough
	case "europe-west2":
		fallthrough
	case "europe-west3":
		fallthrough
	case "europe-west4":
		fallthrough
	case "europe-west6":
		fallthrough
	case "europe-west7":
		fallthrough
	case "europe-west8":
		fallthrough
	case "europe-west9":
		fallthrough
	case "me-west1":
		fallthrough
	case "northamerica-northeast1":
		fallthrough
	case "northamerica-northeast2":
		fallthrough
	case "southamerica-east1":
		fallthrough
	case "southamerica-west1":
		fallthrough
	case "us-central1":
		fallthrough
	case "us-east1":
		fallthrough
	case "us-east2":
		fallthrough
	case "us-east3":
		fallthrough
	case "us-east4":
		fallthrough
	case "us-east5":
		fallthrough
	case "us-west1":
		fallthrough
	case "us-west2":
		fallthrough
	case "us-west3":
		fallthrough
	case "us-west4":
		*e = DestinationBigqueryUpdateDatasetLocation(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryUpdateDatasetLocation: %v", v)
	}
}

type DestinationBigqueryUpdateCredentialType string

const (
	DestinationBigqueryUpdateCredentialTypeHmacKey DestinationBigqueryUpdateCredentialType = "HMAC_KEY"
)

func (e DestinationBigqueryUpdateCredentialType) ToPointer() *DestinationBigqueryUpdateCredentialType {
	return &e
}

func (e *DestinationBigqueryUpdateCredentialType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "HMAC_KEY":
		*e = DestinationBigqueryUpdateCredentialType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryUpdateCredentialType: %v", v)
	}
}

// DestinationBigqueryUpdateHMACKey - An HMAC key is a type of credential and can be associated with a service account or a user account in Cloud Storage. Read more <a href="https://cloud.google.com/storage/docs/authentication/hmackeys">here</a>.
type DestinationBigqueryUpdateHMACKey struct {
	credentialType DestinationBigqueryUpdateCredentialType `const:"HMAC_KEY" json:"credential_type"`
	// HMAC key access ID. When linked to a service account, this ID is 61 characters long; when linked to a user account, it is 24 characters long.
	HmacKeyAccessID string `json:"hmac_key_access_id"`
	// The corresponding secret for the access ID. It is a 40-character base-64 encoded string.
	HmacKeySecret string `json:"hmac_key_secret"`
}

func (d DestinationBigqueryUpdateHMACKey) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(d, "", false)
}

func (d *DestinationBigqueryUpdateHMACKey) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &d, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *DestinationBigqueryUpdateHMACKey) GetCredentialType() DestinationBigqueryUpdateCredentialType {
	return DestinationBigqueryUpdateCredentialTypeHmacKey
}

func (o *DestinationBigqueryUpdateHMACKey) GetHmacKeyAccessID() string {
	if o == nil {
		return ""
	}
	return o.HmacKeyAccessID
}

func (o *DestinationBigqueryUpdateHMACKey) GetHmacKeySecret() string {
	if o == nil {
		return ""
	}
	return o.HmacKeySecret
}

type DestinationBigqueryUpdateCredentialUnionType string

const (
	DestinationBigqueryUpdateCredentialUnionTypeDestinationBigqueryUpdateHMACKey DestinationBigqueryUpdateCredentialUnionType = "destination-bigquery-update_HMAC key"
)

type DestinationBigqueryUpdateCredential struct {
	DestinationBigqueryUpdateHMACKey *DestinationBigqueryUpdateHMACKey

	Type DestinationBigqueryUpdateCredentialUnionType
}

func CreateDestinationBigqueryUpdateCredentialDestinationBigqueryUpdateHMACKey(destinationBigqueryUpdateHMACKey DestinationBigqueryUpdateHMACKey) DestinationBigqueryUpdateCredential {
	typ := DestinationBigqueryUpdateCredentialUnionTypeDestinationBigqueryUpdateHMACKey

	return DestinationBigqueryUpdateCredential{
		DestinationBigqueryUpdateHMACKey: &destinationBigqueryUpdateHMACKey,
		Type:                             typ,
	}
}

func (u *DestinationBigqueryUpdateCredential) UnmarshalJSON(data []byte) error {

	destinationBigqueryUpdateHMACKey := new(DestinationBigqueryUpdateHMACKey)
	if err := utils.UnmarshalJSON(data, &destinationBigqueryUpdateHMACKey, "", true, true); err == nil {
		u.DestinationBigqueryUpdateHMACKey = destinationBigqueryUpdateHMACKey
		u.Type = DestinationBigqueryUpdateCredentialUnionTypeDestinationBigqueryUpdateHMACKey
		return nil
	}

	return errors.New("could not unmarshal into supported union types")
}

func (u DestinationBigqueryUpdateCredential) MarshalJSON() ([]byte, error) {
	if u.DestinationBigqueryUpdateHMACKey != nil {
		return utils.MarshalJSON(u.DestinationBigqueryUpdateHMACKey, "", true)
	}

	return nil, errors.New("could not marshal union type: all fields are null")
}

// DestinationBigqueryUpdateGCSTmpFilesAfterwardProcessing - This upload method is supposed to temporary store records in GCS bucket. By this select you can chose if these records should be removed from GCS when migration has finished. The default "Delete all tmp files from GCS" value is used if not set explicitly.
type DestinationBigqueryUpdateGCSTmpFilesAfterwardProcessing string

const (
	DestinationBigqueryUpdateGCSTmpFilesAfterwardProcessingDeleteAllTmpFilesFromGcs DestinationBigqueryUpdateGCSTmpFilesAfterwardProcessing = "Delete all tmp files from GCS"
	DestinationBigqueryUpdateGCSTmpFilesAfterwardProcessingKeepAllTmpFilesInGcs     DestinationBigqueryUpdateGCSTmpFilesAfterwardProcessing = "Keep all tmp files in GCS"
)

func (e DestinationBigqueryUpdateGCSTmpFilesAfterwardProcessing) ToPointer() *DestinationBigqueryUpdateGCSTmpFilesAfterwardProcessing {
	return &e
}

func (e *DestinationBigqueryUpdateGCSTmpFilesAfterwardProcessing) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "Delete all tmp files from GCS":
		fallthrough
	case "Keep all tmp files in GCS":
		*e = DestinationBigqueryUpdateGCSTmpFilesAfterwardProcessing(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryUpdateGCSTmpFilesAfterwardProcessing: %v", v)
	}
}

type DestinationBigqueryUpdateSchemasMethod string

const (
	DestinationBigqueryUpdateSchemasMethodGcsStaging DestinationBigqueryUpdateSchemasMethod = "GCS Staging"
)

func (e DestinationBigqueryUpdateSchemasMethod) ToPointer() *DestinationBigqueryUpdateSchemasMethod {
	return &e
}

func (e *DestinationBigqueryUpdateSchemasMethod) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "GCS Staging":
		*e = DestinationBigqueryUpdateSchemasMethod(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryUpdateSchemasMethod: %v", v)
	}
}

// DestinationBigqueryUpdateGCSStaging - Loading method used to send select the way data will be uploaded to BigQuery. <br/><b>Standard Inserts</b> - Direct uploading using SQL INSERT statements. This method is extremely inefficient and provided only for quick testing. In almost all cases, you should use staging. <br/><b>GCS Staging</b> - Writes large batches of records to a file, uploads the file to GCS, then uses <b>COPY INTO table</b> to upload the file. Recommended for most workloads for better speed and scalability. Read more about GCS Staging <a href="https://docs.airbyte.com/integrations/destinations/bigquery#gcs-staging">here</a>.
type DestinationBigqueryUpdateGCSStaging struct {
	// An HMAC key is a type of credential and can be associated with a service account or a user account in Cloud Storage. Read more <a href="https://cloud.google.com/storage/docs/authentication/hmackeys">here</a>.
	Credential DestinationBigqueryUpdateCredential `json:"credential"`
	// Number of file buffers allocated for writing data. Increasing this number is beneficial for connections using Change Data Capture (CDC) and up to the number of streams within a connection. Increasing the number of file buffers past the maximum number of streams has deteriorating effects
	FileBufferCount *int64 `default:"10" json:"file_buffer_count"`
	// The name of the GCS bucket. Read more <a href="https://cloud.google.com/storage/docs/naming-buckets">here</a>.
	GcsBucketName string `json:"gcs_bucket_name"`
	// Directory under the GCS bucket where data will be written.
	GcsBucketPath string `json:"gcs_bucket_path"`
	// This upload method is supposed to temporary store records in GCS bucket. By this select you can chose if these records should be removed from GCS when migration has finished. The default "Delete all tmp files from GCS" value is used if not set explicitly.
	KeepFilesInGcsBucket *DestinationBigqueryUpdateGCSTmpFilesAfterwardProcessing `default:"Delete all tmp files from GCS" json:"keep_files_in_gcs-bucket"`
	method               DestinationBigqueryUpdateSchemasMethod                   `const:"GCS Staging" json:"method"`
}

func (d DestinationBigqueryUpdateGCSStaging) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(d, "", false)
}

func (d *DestinationBigqueryUpdateGCSStaging) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &d, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *DestinationBigqueryUpdateGCSStaging) GetCredential() DestinationBigqueryUpdateCredential {
	if o == nil {
		return DestinationBigqueryUpdateCredential{}
	}
	return o.Credential
}

func (o *DestinationBigqueryUpdateGCSStaging) GetFileBufferCount() *int64 {
	if o == nil {
		return nil
	}
	return o.FileBufferCount
}

func (o *DestinationBigqueryUpdateGCSStaging) GetGcsBucketName() string {
	if o == nil {
		return ""
	}
	return o.GcsBucketName
}

func (o *DestinationBigqueryUpdateGCSStaging) GetGcsBucketPath() string {
	if o == nil {
		return ""
	}
	return o.GcsBucketPath
}

func (o *DestinationBigqueryUpdateGCSStaging) GetKeepFilesInGcsBucket() *DestinationBigqueryUpdateGCSTmpFilesAfterwardProcessing {
	if o == nil {
		return nil
	}
	return o.KeepFilesInGcsBucket
}

func (o *DestinationBigqueryUpdateGCSStaging) GetMethod() DestinationBigqueryUpdateSchemasMethod {
	return DestinationBigqueryUpdateSchemasMethodGcsStaging
}

type DestinationBigqueryUpdateMethod string

const (
	DestinationBigqueryUpdateMethodStandard DestinationBigqueryUpdateMethod = "Standard"
)

func (e DestinationBigqueryUpdateMethod) ToPointer() *DestinationBigqueryUpdateMethod {
	return &e
}

func (e *DestinationBigqueryUpdateMethod) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "Standard":
		*e = DestinationBigqueryUpdateMethod(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryUpdateMethod: %v", v)
	}
}

// DestinationBigqueryUpdateStandardInserts - Loading method used to send select the way data will be uploaded to BigQuery. <br/><b>Standard Inserts</b> - Direct uploading using SQL INSERT statements. This method is extremely inefficient and provided only for quick testing. In almost all cases, you should use staging. <br/><b>GCS Staging</b> - Writes large batches of records to a file, uploads the file to GCS, then uses <b>COPY INTO table</b> to upload the file. Recommended for most workloads for better speed and scalability. Read more about GCS Staging <a href="https://docs.airbyte.com/integrations/destinations/bigquery#gcs-staging">here</a>.
type DestinationBigqueryUpdateStandardInserts struct {
	method DestinationBigqueryUpdateMethod `const:"Standard" json:"method"`
}

func (d DestinationBigqueryUpdateStandardInserts) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(d, "", false)
}

func (d *DestinationBigqueryUpdateStandardInserts) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &d, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *DestinationBigqueryUpdateStandardInserts) GetMethod() DestinationBigqueryUpdateMethod {
	return DestinationBigqueryUpdateMethodStandard
}

type DestinationBigqueryUpdateLoadingMethodType string

const (
	DestinationBigqueryUpdateLoadingMethodTypeDestinationBigqueryUpdateStandardInserts DestinationBigqueryUpdateLoadingMethodType = "destination-bigquery-update_Standard Inserts"
	DestinationBigqueryUpdateLoadingMethodTypeDestinationBigqueryUpdateGCSStaging      DestinationBigqueryUpdateLoadingMethodType = "destination-bigquery-update_GCS Staging"
)

type DestinationBigqueryUpdateLoadingMethod struct {
	DestinationBigqueryUpdateStandardInserts *DestinationBigqueryUpdateStandardInserts
	DestinationBigqueryUpdateGCSStaging      *DestinationBigqueryUpdateGCSStaging

	Type DestinationBigqueryUpdateLoadingMethodType
}

func CreateDestinationBigqueryUpdateLoadingMethodDestinationBigqueryUpdateStandardInserts(destinationBigqueryUpdateStandardInserts DestinationBigqueryUpdateStandardInserts) DestinationBigqueryUpdateLoadingMethod {
	typ := DestinationBigqueryUpdateLoadingMethodTypeDestinationBigqueryUpdateStandardInserts

	return DestinationBigqueryUpdateLoadingMethod{
		DestinationBigqueryUpdateStandardInserts: &destinationBigqueryUpdateStandardInserts,
		Type:                                     typ,
	}
}

func CreateDestinationBigqueryUpdateLoadingMethodDestinationBigqueryUpdateGCSStaging(destinationBigqueryUpdateGCSStaging DestinationBigqueryUpdateGCSStaging) DestinationBigqueryUpdateLoadingMethod {
	typ := DestinationBigqueryUpdateLoadingMethodTypeDestinationBigqueryUpdateGCSStaging

	return DestinationBigqueryUpdateLoadingMethod{
		DestinationBigqueryUpdateGCSStaging: &destinationBigqueryUpdateGCSStaging,
		Type:                                typ,
	}
}

func (u *DestinationBigqueryUpdateLoadingMethod) UnmarshalJSON(data []byte) error {

	destinationBigqueryUpdateStandardInserts := new(DestinationBigqueryUpdateStandardInserts)
	if err := utils.UnmarshalJSON(data, &destinationBigqueryUpdateStandardInserts, "", true, true); err == nil {
		u.DestinationBigqueryUpdateStandardInserts = destinationBigqueryUpdateStandardInserts
		u.Type = DestinationBigqueryUpdateLoadingMethodTypeDestinationBigqueryUpdateStandardInserts
		return nil
	}

	destinationBigqueryUpdateGCSStaging := new(DestinationBigqueryUpdateGCSStaging)
	if err := utils.UnmarshalJSON(data, &destinationBigqueryUpdateGCSStaging, "", true, true); err == nil {
		u.DestinationBigqueryUpdateGCSStaging = destinationBigqueryUpdateGCSStaging
		u.Type = DestinationBigqueryUpdateLoadingMethodTypeDestinationBigqueryUpdateGCSStaging
		return nil
	}

	return errors.New("could not unmarshal into supported union types")
}

func (u DestinationBigqueryUpdateLoadingMethod) MarshalJSON() ([]byte, error) {
	if u.DestinationBigqueryUpdateStandardInserts != nil {
		return utils.MarshalJSON(u.DestinationBigqueryUpdateStandardInserts, "", true)
	}

	if u.DestinationBigqueryUpdateGCSStaging != nil {
		return utils.MarshalJSON(u.DestinationBigqueryUpdateGCSStaging, "", true)
	}

	return nil, errors.New("could not marshal union type: all fields are null")
}

// TransformationQueryRunType - Interactive run type means that the query is executed as soon as possible, and these queries count towards concurrent rate limit and daily limit. Read more about interactive run type <a href="https://cloud.google.com/bigquery/docs/running-queries#queries">here</a>. Batch queries are queued and started as soon as idle resources are available in the BigQuery shared resource pool, which usually occurs within a few minutes. Batch queries don’t count towards your concurrent rate limit. Read more about batch queries <a href="https://cloud.google.com/bigquery/docs/running-queries#batch">here</a>. The default "interactive" value is used if not set explicitly.
type TransformationQueryRunType string

const (
	TransformationQueryRunTypeInteractive TransformationQueryRunType = "interactive"
	TransformationQueryRunTypeBatch       TransformationQueryRunType = "batch"
)

func (e TransformationQueryRunType) ToPointer() *TransformationQueryRunType {
	return &e
}

func (e *TransformationQueryRunType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "interactive":
		fallthrough
	case "batch":
		*e = TransformationQueryRunType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for TransformationQueryRunType: %v", v)
	}
}

type DestinationBigqueryUpdate struct {
	// Google BigQuery client's chunk (buffer) size (MIN=1, MAX = 15) for each table. The size that will be written by a single RPC. Written data will be buffered and only flushed upon reaching this size or closing the channel. The default 15MB value is used if not set explicitly. Read more <a href="https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.client.Client.html">here</a>.
	BigQueryClientBufferSizeMb *int64 `default:"15" json:"big_query_client_buffer_size_mb"`
	// The contents of the JSON service account key. Check out the <a href="https://docs.airbyte.com/integrations/destinations/bigquery#service-account-key">docs</a> if you need help generating this key. Default credentials will be used if this field is left empty.
	CredentialsJSON *string `json:"credentials_json,omitempty"`
	// The default BigQuery Dataset ID that tables are replicated to if the source does not specify a namespace. Read more <a href="https://cloud.google.com/bigquery/docs/datasets#create-dataset">here</a>.
	DatasetID string `json:"dataset_id"`
	// The location of the dataset. Warning: Changes made after creation will not be applied. Read more <a href="https://cloud.google.com/bigquery/docs/locations">here</a>.
	DatasetLocation DestinationBigqueryUpdateDatasetLocation `json:"dataset_location"`
	// Loading method used to send select the way data will be uploaded to BigQuery. <br/><b>Standard Inserts</b> - Direct uploading using SQL INSERT statements. This method is extremely inefficient and provided only for quick testing. In almost all cases, you should use staging. <br/><b>GCS Staging</b> - Writes large batches of records to a file, uploads the file to GCS, then uses <b>COPY INTO table</b> to upload the file. Recommended for most workloads for better speed and scalability. Read more about GCS Staging <a href="https://docs.airbyte.com/integrations/destinations/bigquery#gcs-staging">here</a>.
	LoadingMethod *DestinationBigqueryUpdateLoadingMethod `json:"loading_method,omitempty"`
	// The GCP project ID for the project containing the target BigQuery dataset. Read more <a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects">here</a>.
	ProjectID string `json:"project_id"`
	// The dataset to write raw tables into
	RawDataDataset *string `json:"raw_data_dataset,omitempty"`
	// Interactive run type means that the query is executed as soon as possible, and these queries count towards concurrent rate limit and daily limit. Read more about interactive run type <a href="https://cloud.google.com/bigquery/docs/running-queries#queries">here</a>. Batch queries are queued and started as soon as idle resources are available in the BigQuery shared resource pool, which usually occurs within a few minutes. Batch queries don’t count towards your concurrent rate limit. Read more about batch queries <a href="https://cloud.google.com/bigquery/docs/running-queries#batch">here</a>. The default "interactive" value is used if not set explicitly.
	TransformationPriority *TransformationQueryRunType `default:"interactive" json:"transformation_priority"`
}

func (d DestinationBigqueryUpdate) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(d, "", false)
}

func (d *DestinationBigqueryUpdate) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &d, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *DestinationBigqueryUpdate) GetBigQueryClientBufferSizeMb() *int64 {
	if o == nil {
		return nil
	}
	return o.BigQueryClientBufferSizeMb
}

func (o *DestinationBigqueryUpdate) GetCredentialsJSON() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsJSON
}

func (o *DestinationBigqueryUpdate) GetDatasetID() string {
	if o == nil {
		return ""
	}
	return o.DatasetID
}

func (o *DestinationBigqueryUpdate) GetDatasetLocation() DestinationBigqueryUpdateDatasetLocation {
	if o == nil {
		return DestinationBigqueryUpdateDatasetLocation("")
	}
	return o.DatasetLocation
}

func (o *DestinationBigqueryUpdate) GetLoadingMethod() *DestinationBigqueryUpdateLoadingMethod {
	if o == nil {
		return nil
	}
	return o.LoadingMethod
}

func (o *DestinationBigqueryUpdate) GetProjectID() string {
	if o == nil {
		return ""
	}
	return o.ProjectID
}

func (o *DestinationBigqueryUpdate) GetRawDataDataset() *string {
	if o == nil {
		return nil
	}
	return o.RawDataDataset
}

func (o *DestinationBigqueryUpdate) GetTransformationPriority() *TransformationQueryRunType {
	if o == nil {
		return nil
	}
	return o.TransformationPriority
}
