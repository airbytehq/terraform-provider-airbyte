// Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.

package shared

import (
	"encoding/json"
	"errors"
	"fmt"
	"github.com/airbytehq/terraform-provider-airbyte/internal/sdk/pkg/utils"
)

// DatasetLocation - The location of the dataset. Warning: Changes made after creation will not be applied. The default "US" value is used if not set explicitly. Read more <a href="https://cloud.google.com/bigquery/docs/locations">here</a>.
type DatasetLocation string

const (
	DatasetLocationUs                     DatasetLocation = "US"
	DatasetLocationEu                     DatasetLocation = "EU"
	DatasetLocationAsiaEast1              DatasetLocation = "asia-east1"
	DatasetLocationAsiaEast2              DatasetLocation = "asia-east2"
	DatasetLocationAsiaNortheast1         DatasetLocation = "asia-northeast1"
	DatasetLocationAsiaNortheast2         DatasetLocation = "asia-northeast2"
	DatasetLocationAsiaNortheast3         DatasetLocation = "asia-northeast3"
	DatasetLocationAsiaSouth1             DatasetLocation = "asia-south1"
	DatasetLocationAsiaSouth2             DatasetLocation = "asia-south2"
	DatasetLocationAsiaSoutheast1         DatasetLocation = "asia-southeast1"
	DatasetLocationAsiaSoutheast2         DatasetLocation = "asia-southeast2"
	DatasetLocationAustraliaSoutheast1    DatasetLocation = "australia-southeast1"
	DatasetLocationAustraliaSoutheast2    DatasetLocation = "australia-southeast2"
	DatasetLocationEuropeCentral1         DatasetLocation = "europe-central1"
	DatasetLocationEuropeCentral2         DatasetLocation = "europe-central2"
	DatasetLocationEuropeNorth1           DatasetLocation = "europe-north1"
	DatasetLocationEuropeSouthwest1       DatasetLocation = "europe-southwest1"
	DatasetLocationEuropeWest1            DatasetLocation = "europe-west1"
	DatasetLocationEuropeWest2            DatasetLocation = "europe-west2"
	DatasetLocationEuropeWest3            DatasetLocation = "europe-west3"
	DatasetLocationEuropeWest4            DatasetLocation = "europe-west4"
	DatasetLocationEuropeWest6            DatasetLocation = "europe-west6"
	DatasetLocationEuropeWest7            DatasetLocation = "europe-west7"
	DatasetLocationEuropeWest8            DatasetLocation = "europe-west8"
	DatasetLocationEuropeWest9            DatasetLocation = "europe-west9"
	DatasetLocationMeWest1                DatasetLocation = "me-west1"
	DatasetLocationNorthamericaNortheast1 DatasetLocation = "northamerica-northeast1"
	DatasetLocationNorthamericaNortheast2 DatasetLocation = "northamerica-northeast2"
	DatasetLocationSouthamericaEast1      DatasetLocation = "southamerica-east1"
	DatasetLocationSouthamericaWest1      DatasetLocation = "southamerica-west1"
	DatasetLocationUsCentral1             DatasetLocation = "us-central1"
	DatasetLocationUsEast1                DatasetLocation = "us-east1"
	DatasetLocationUsEast2                DatasetLocation = "us-east2"
	DatasetLocationUsEast3                DatasetLocation = "us-east3"
	DatasetLocationUsEast4                DatasetLocation = "us-east4"
	DatasetLocationUsEast5                DatasetLocation = "us-east5"
	DatasetLocationUsWest1                DatasetLocation = "us-west1"
	DatasetLocationUsWest2                DatasetLocation = "us-west2"
	DatasetLocationUsWest3                DatasetLocation = "us-west3"
	DatasetLocationUsWest4                DatasetLocation = "us-west4"
)

func (e DatasetLocation) ToPointer() *DatasetLocation {
	return &e
}

func (e *DatasetLocation) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "US":
		fallthrough
	case "EU":
		fallthrough
	case "asia-east1":
		fallthrough
	case "asia-east2":
		fallthrough
	case "asia-northeast1":
		fallthrough
	case "asia-northeast2":
		fallthrough
	case "asia-northeast3":
		fallthrough
	case "asia-south1":
		fallthrough
	case "asia-south2":
		fallthrough
	case "asia-southeast1":
		fallthrough
	case "asia-southeast2":
		fallthrough
	case "australia-southeast1":
		fallthrough
	case "australia-southeast2":
		fallthrough
	case "europe-central1":
		fallthrough
	case "europe-central2":
		fallthrough
	case "europe-north1":
		fallthrough
	case "europe-southwest1":
		fallthrough
	case "europe-west1":
		fallthrough
	case "europe-west2":
		fallthrough
	case "europe-west3":
		fallthrough
	case "europe-west4":
		fallthrough
	case "europe-west6":
		fallthrough
	case "europe-west7":
		fallthrough
	case "europe-west8":
		fallthrough
	case "europe-west9":
		fallthrough
	case "me-west1":
		fallthrough
	case "northamerica-northeast1":
		fallthrough
	case "northamerica-northeast2":
		fallthrough
	case "southamerica-east1":
		fallthrough
	case "southamerica-west1":
		fallthrough
	case "us-central1":
		fallthrough
	case "us-east1":
		fallthrough
	case "us-east2":
		fallthrough
	case "us-east3":
		fallthrough
	case "us-east4":
		fallthrough
	case "us-east5":
		fallthrough
	case "us-west1":
		fallthrough
	case "us-west2":
		fallthrough
	case "us-west3":
		fallthrough
	case "us-west4":
		*e = DatasetLocation(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DatasetLocation: %v", v)
	}
}

type DestinationBigqueryDenormalizedUpdateCredentialType string

const (
	DestinationBigqueryDenormalizedUpdateCredentialTypeHmacKey DestinationBigqueryDenormalizedUpdateCredentialType = "HMAC_KEY"
)

func (e DestinationBigqueryDenormalizedUpdateCredentialType) ToPointer() *DestinationBigqueryDenormalizedUpdateCredentialType {
	return &e
}

func (e *DestinationBigqueryDenormalizedUpdateCredentialType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "HMAC_KEY":
		*e = DestinationBigqueryDenormalizedUpdateCredentialType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryDenormalizedUpdateCredentialType: %v", v)
	}
}

// DestinationBigqueryDenormalizedUpdateHMACKey - An HMAC key is a type of credential and can be associated with a service account or a user account in Cloud Storage. Read more <a href="https://cloud.google.com/storage/docs/authentication/hmackeys">here</a>.
type DestinationBigqueryDenormalizedUpdateHMACKey struct {
	credentialType DestinationBigqueryDenormalizedUpdateCredentialType `const:"HMAC_KEY" json:"credential_type"`
	// HMAC key access ID. When linked to a service account, this ID is 61 characters long; when linked to a user account, it is 24 characters long.
	HmacKeyAccessID string `json:"hmac_key_access_id"`
	// The corresponding secret for the access ID. It is a 40-character base-64 encoded string.
	HmacKeySecret string `json:"hmac_key_secret"`
}

func (d DestinationBigqueryDenormalizedUpdateHMACKey) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(d, "", false)
}

func (d *DestinationBigqueryDenormalizedUpdateHMACKey) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &d, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *DestinationBigqueryDenormalizedUpdateHMACKey) GetCredentialType() DestinationBigqueryDenormalizedUpdateCredentialType {
	return DestinationBigqueryDenormalizedUpdateCredentialTypeHmacKey
}

func (o *DestinationBigqueryDenormalizedUpdateHMACKey) GetHmacKeyAccessID() string {
	if o == nil {
		return ""
	}
	return o.HmacKeyAccessID
}

func (o *DestinationBigqueryDenormalizedUpdateHMACKey) GetHmacKeySecret() string {
	if o == nil {
		return ""
	}
	return o.HmacKeySecret
}

type CredentialUnionType string

const (
	CredentialUnionTypeDestinationBigqueryDenormalizedUpdateHMACKey CredentialUnionType = "destination-bigquery-denormalized-update_HMAC key"
)

type Credential struct {
	DestinationBigqueryDenormalizedUpdateHMACKey *DestinationBigqueryDenormalizedUpdateHMACKey

	Type CredentialUnionType
}

func CreateCredentialDestinationBigqueryDenormalizedUpdateHMACKey(destinationBigqueryDenormalizedUpdateHMACKey DestinationBigqueryDenormalizedUpdateHMACKey) Credential {
	typ := CredentialUnionTypeDestinationBigqueryDenormalizedUpdateHMACKey

	return Credential{
		DestinationBigqueryDenormalizedUpdateHMACKey: &destinationBigqueryDenormalizedUpdateHMACKey,
		Type: typ,
	}
}

func (u *Credential) UnmarshalJSON(data []byte) error {

	destinationBigqueryDenormalizedUpdateHMACKey := new(DestinationBigqueryDenormalizedUpdateHMACKey)
	if err := utils.UnmarshalJSON(data, &destinationBigqueryDenormalizedUpdateHMACKey, "", true, true); err == nil {
		u.DestinationBigqueryDenormalizedUpdateHMACKey = destinationBigqueryDenormalizedUpdateHMACKey
		u.Type = CredentialUnionTypeDestinationBigqueryDenormalizedUpdateHMACKey
		return nil
	}

	return errors.New("could not unmarshal into supported union types")
}

func (u Credential) MarshalJSON() ([]byte, error) {
	if u.DestinationBigqueryDenormalizedUpdateHMACKey != nil {
		return utils.MarshalJSON(u.DestinationBigqueryDenormalizedUpdateHMACKey, "", true)
	}

	return nil, errors.New("could not marshal union type: all fields are null")
}

// GCSTmpFilesAfterwardProcessing - This upload method is supposed to temporary store records in GCS bucket. By this select you can chose if these records should be removed from GCS when migration has finished. The default "Delete all tmp files from GCS" value is used if not set explicitly.
type GCSTmpFilesAfterwardProcessing string

const (
	GCSTmpFilesAfterwardProcessingDeleteAllTmpFilesFromGcs GCSTmpFilesAfterwardProcessing = "Delete all tmp files from GCS"
	GCSTmpFilesAfterwardProcessingKeepAllTmpFilesInGcs     GCSTmpFilesAfterwardProcessing = "Keep all tmp files in GCS"
)

func (e GCSTmpFilesAfterwardProcessing) ToPointer() *GCSTmpFilesAfterwardProcessing {
	return &e
}

func (e *GCSTmpFilesAfterwardProcessing) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "Delete all tmp files from GCS":
		fallthrough
	case "Keep all tmp files in GCS":
		*e = GCSTmpFilesAfterwardProcessing(v)
		return nil
	default:
		return fmt.Errorf("invalid value for GCSTmpFilesAfterwardProcessing: %v", v)
	}
}

type DestinationBigqueryDenormalizedUpdateMethod string

const (
	DestinationBigqueryDenormalizedUpdateMethodGcsStaging DestinationBigqueryDenormalizedUpdateMethod = "GCS Staging"
)

func (e DestinationBigqueryDenormalizedUpdateMethod) ToPointer() *DestinationBigqueryDenormalizedUpdateMethod {
	return &e
}

func (e *DestinationBigqueryDenormalizedUpdateMethod) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "GCS Staging":
		*e = DestinationBigqueryDenormalizedUpdateMethod(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryDenormalizedUpdateMethod: %v", v)
	}
}

// GCSStaging - Loading method used to send select the way data will be uploaded to BigQuery. <br/><b>Standard Inserts</b> - Direct uploading using SQL INSERT statements. This method is extremely inefficient and provided only for quick testing. In almost all cases, you should use staging. <br/><b>GCS Staging</b> - Writes large batches of records to a file, uploads the file to GCS, then uses <b>COPY INTO table</b> to upload the file. Recommended for most workloads for better speed and scalability. Read more about GCS Staging <a href="https://docs.airbyte.com/integrations/destinations/bigquery#gcs-staging">here</a>.
type GCSStaging struct {
	// An HMAC key is a type of credential and can be associated with a service account or a user account in Cloud Storage. Read more <a href="https://cloud.google.com/storage/docs/authentication/hmackeys">here</a>.
	Credential Credential `json:"credential"`
	// Number of file buffers allocated for writing data. Increasing this number is beneficial for connections using Change Data Capture (CDC) and up to the number of streams within a connection. Increasing the number of file buffers past the maximum number of streams has deteriorating effects
	FileBufferCount *int64 `default:"10" json:"file_buffer_count"`
	// The name of the GCS bucket. Read more <a href="https://cloud.google.com/storage/docs/naming-buckets">here</a>.
	GcsBucketName string `json:"gcs_bucket_name"`
	// Directory under the GCS bucket where data will be written. Read more <a href="https://cloud.google.com/storage/docs/locations">here</a>.
	GcsBucketPath string `json:"gcs_bucket_path"`
	// This upload method is supposed to temporary store records in GCS bucket. By this select you can chose if these records should be removed from GCS when migration has finished. The default "Delete all tmp files from GCS" value is used if not set explicitly.
	KeepFilesInGcsBucket *GCSTmpFilesAfterwardProcessing             `default:"Delete all tmp files from GCS" json:"keep_files_in_gcs-bucket"`
	method               DestinationBigqueryDenormalizedUpdateMethod `const:"GCS Staging" json:"method"`
}

func (g GCSStaging) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(g, "", false)
}

func (g *GCSStaging) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &g, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *GCSStaging) GetCredential() Credential {
	if o == nil {
		return Credential{}
	}
	return o.Credential
}

func (o *GCSStaging) GetFileBufferCount() *int64 {
	if o == nil {
		return nil
	}
	return o.FileBufferCount
}

func (o *GCSStaging) GetGcsBucketName() string {
	if o == nil {
		return ""
	}
	return o.GcsBucketName
}

func (o *GCSStaging) GetGcsBucketPath() string {
	if o == nil {
		return ""
	}
	return o.GcsBucketPath
}

func (o *GCSStaging) GetKeepFilesInGcsBucket() *GCSTmpFilesAfterwardProcessing {
	if o == nil {
		return nil
	}
	return o.KeepFilesInGcsBucket
}

func (o *GCSStaging) GetMethod() DestinationBigqueryDenormalizedUpdateMethod {
	return DestinationBigqueryDenormalizedUpdateMethodGcsStaging
}

type Method string

const (
	MethodStandard Method = "Standard"
)

func (e Method) ToPointer() *Method {
	return &e
}

func (e *Method) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "Standard":
		*e = Method(v)
		return nil
	default:
		return fmt.Errorf("invalid value for Method: %v", v)
	}
}

// StandardInserts - Loading method used to send select the way data will be uploaded to BigQuery. <br/><b>Standard Inserts</b> - Direct uploading using SQL INSERT statements. This method is extremely inefficient and provided only for quick testing. In almost all cases, you should use staging. <br/><b>GCS Staging</b> - Writes large batches of records to a file, uploads the file to GCS, then uses <b>COPY INTO table</b> to upload the file. Recommended for most workloads for better speed and scalability. Read more about GCS Staging <a href="https://docs.airbyte.com/integrations/destinations/bigquery#gcs-staging">here</a>.
type StandardInserts struct {
	method Method `const:"Standard" json:"method"`
}

func (s StandardInserts) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(s, "", false)
}

func (s *StandardInserts) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &s, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *StandardInserts) GetMethod() Method {
	return MethodStandard
}

type LoadingMethodType string

const (
	LoadingMethodTypeStandardInserts LoadingMethodType = "Standard Inserts"
	LoadingMethodTypeGCSStaging      LoadingMethodType = "GCS Staging"
)

type LoadingMethod struct {
	StandardInserts *StandardInserts
	GCSStaging      *GCSStaging

	Type LoadingMethodType
}

func CreateLoadingMethodStandardInserts(standardInserts StandardInserts) LoadingMethod {
	typ := LoadingMethodTypeStandardInserts

	return LoadingMethod{
		StandardInserts: &standardInserts,
		Type:            typ,
	}
}

func CreateLoadingMethodGCSStaging(gcsStaging GCSStaging) LoadingMethod {
	typ := LoadingMethodTypeGCSStaging

	return LoadingMethod{
		GCSStaging: &gcsStaging,
		Type:       typ,
	}
}

func (u *LoadingMethod) UnmarshalJSON(data []byte) error {

	standardInserts := new(StandardInserts)
	if err := utils.UnmarshalJSON(data, &standardInserts, "", true, true); err == nil {
		u.StandardInserts = standardInserts
		u.Type = LoadingMethodTypeStandardInserts
		return nil
	}

	gcsStaging := new(GCSStaging)
	if err := utils.UnmarshalJSON(data, &gcsStaging, "", true, true); err == nil {
		u.GCSStaging = gcsStaging
		u.Type = LoadingMethodTypeGCSStaging
		return nil
	}

	return errors.New("could not unmarshal into supported union types")
}

func (u LoadingMethod) MarshalJSON() ([]byte, error) {
	if u.StandardInserts != nil {
		return utils.MarshalJSON(u.StandardInserts, "", true)
	}

	if u.GCSStaging != nil {
		return utils.MarshalJSON(u.GCSStaging, "", true)
	}

	return nil, errors.New("could not marshal union type: all fields are null")
}

type DestinationBigqueryDenormalizedUpdate struct {
	// Google BigQuery client's chunk (buffer) size (MIN=1, MAX = 15) for each table. The size that will be written by a single RPC. Written data will be buffered and only flushed upon reaching this size or closing the channel. The default 15MB value is used if not set explicitly. Read more <a href="https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.client.Client.html">here</a>.
	BigQueryClientBufferSizeMb *int64 `default:"15" json:"big_query_client_buffer_size_mb"`
	// The contents of the JSON service account key. Check out the <a href="https://docs.airbyte.com/integrations/destinations/bigquery#service-account-key">docs</a> if you need help generating this key. Default credentials will be used if this field is left empty.
	CredentialsJSON *string `json:"credentials_json,omitempty"`
	// The default BigQuery Dataset ID that tables are replicated to if the source does not specify a namespace. Read more <a href="https://cloud.google.com/bigquery/docs/datasets#create-dataset">here</a>.
	DatasetID string `json:"dataset_id"`
	// The location of the dataset. Warning: Changes made after creation will not be applied. The default "US" value is used if not set explicitly. Read more <a href="https://cloud.google.com/bigquery/docs/locations">here</a>.
	DatasetLocation *DatasetLocation `default:"US" json:"dataset_location"`
	// Loading method used to send select the way data will be uploaded to BigQuery. <br/><b>Standard Inserts</b> - Direct uploading using SQL INSERT statements. This method is extremely inefficient and provided only for quick testing. In almost all cases, you should use staging. <br/><b>GCS Staging</b> - Writes large batches of records to a file, uploads the file to GCS, then uses <b>COPY INTO table</b> to upload the file. Recommended for most workloads for better speed and scalability. Read more about GCS Staging <a href="https://docs.airbyte.com/integrations/destinations/bigquery#gcs-staging">here</a>.
	LoadingMethod *LoadingMethod `json:"loading_method,omitempty"`
	// The GCP project ID for the project containing the target BigQuery dataset. Read more <a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects">here</a>.
	ProjectID string `json:"project_id"`
}

func (d DestinationBigqueryDenormalizedUpdate) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(d, "", false)
}

func (d *DestinationBigqueryDenormalizedUpdate) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &d, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *DestinationBigqueryDenormalizedUpdate) GetBigQueryClientBufferSizeMb() *int64 {
	if o == nil {
		return nil
	}
	return o.BigQueryClientBufferSizeMb
}

func (o *DestinationBigqueryDenormalizedUpdate) GetCredentialsJSON() *string {
	if o == nil {
		return nil
	}
	return o.CredentialsJSON
}

func (o *DestinationBigqueryDenormalizedUpdate) GetDatasetID() string {
	if o == nil {
		return ""
	}
	return o.DatasetID
}

func (o *DestinationBigqueryDenormalizedUpdate) GetDatasetLocation() *DatasetLocation {
	if o == nil {
		return nil
	}
	return o.DatasetLocation
}

func (o *DestinationBigqueryDenormalizedUpdate) GetLoadingMethod() *LoadingMethod {
	if o == nil {
		return nil
	}
	return o.LoadingMethod
}

func (o *DestinationBigqueryDenormalizedUpdate) GetProjectID() string {
	if o == nil {
		return ""
	}
	return o.ProjectID
}
