// Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.

package shared

import (
	"bytes"
	"encoding/json"
	"errors"
	"fmt"
)

type DestinationDatabricksDataSourceAzureBlobStorageDataSourceType string

const (
	DestinationDatabricksDataSourceAzureBlobStorageDataSourceTypeAzureBlobStorage DestinationDatabricksDataSourceAzureBlobStorageDataSourceType = "AZURE_BLOB_STORAGE"
)

func (e DestinationDatabricksDataSourceAzureBlobStorageDataSourceType) ToPointer() *DestinationDatabricksDataSourceAzureBlobStorageDataSourceType {
	return &e
}

func (e *DestinationDatabricksDataSourceAzureBlobStorageDataSourceType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "AZURE_BLOB_STORAGE":
		*e = DestinationDatabricksDataSourceAzureBlobStorageDataSourceType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationDatabricksDataSourceAzureBlobStorageDataSourceType: %v", v)
	}
}

// DestinationDatabricksDataSourceAzureBlobStorage - Storage on which the delta lake is built.
type DestinationDatabricksDataSourceAzureBlobStorage struct {
	// The account's name of the Azure Blob Storage.
	AzureBlobStorageAccountName string `json:"azure_blob_storage_account_name"`
	// The name of the Azure blob storage container.
	AzureBlobStorageContainerName string `json:"azure_blob_storage_container_name"`
	// This is Azure Blob Storage endpoint domain name. Leave default value (or leave it empty if run container from command line) to use Microsoft native from example.
	AzureBlobStorageEndpointDomainName *string `json:"azure_blob_storage_endpoint_domain_name,omitempty"`
	// Shared access signature (SAS) token to grant limited access to objects in your storage account.
	AzureBlobStorageSasToken string                                                        `json:"azure_blob_storage_sas_token"`
	DataSourceType           DestinationDatabricksDataSourceAzureBlobStorageDataSourceType `json:"data_source_type"`
}

type DestinationDatabricksDataSourceAmazonS3DataSourceType string

const (
	DestinationDatabricksDataSourceAmazonS3DataSourceTypeS3Storage DestinationDatabricksDataSourceAmazonS3DataSourceType = "S3_STORAGE"
)

func (e DestinationDatabricksDataSourceAmazonS3DataSourceType) ToPointer() *DestinationDatabricksDataSourceAmazonS3DataSourceType {
	return &e
}

func (e *DestinationDatabricksDataSourceAmazonS3DataSourceType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "S3_STORAGE":
		*e = DestinationDatabricksDataSourceAmazonS3DataSourceType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationDatabricksDataSourceAmazonS3DataSourceType: %v", v)
	}
}

// DestinationDatabricksDataSourceAmazonS3S3BucketRegion - The region of the S3 staging bucket to use if utilising a copy strategy.
type DestinationDatabricksDataSourceAmazonS3S3BucketRegion string

const (
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionUnknown      DestinationDatabricksDataSourceAmazonS3S3BucketRegion = ""
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionUsEast1      DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "us-east-1"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionUsEast2      DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "us-east-2"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionUsWest1      DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "us-west-1"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionUsWest2      DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "us-west-2"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionAfSouth1     DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "af-south-1"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionApEast1      DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "ap-east-1"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionApSouth1     DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "ap-south-1"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionApNortheast1 DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "ap-northeast-1"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionApNortheast2 DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "ap-northeast-2"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionApNortheast3 DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "ap-northeast-3"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionApSoutheast1 DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "ap-southeast-1"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionApSoutheast2 DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "ap-southeast-2"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionCaCentral1   DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "ca-central-1"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionCnNorth1     DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "cn-north-1"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionCnNorthwest1 DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "cn-northwest-1"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionEuCentral1   DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "eu-central-1"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionEuNorth1     DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "eu-north-1"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionEuSouth1     DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "eu-south-1"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionEuWest1      DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "eu-west-1"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionEuWest2      DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "eu-west-2"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionEuWest3      DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "eu-west-3"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionSaEast1      DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "sa-east-1"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionMeSouth1     DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "me-south-1"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionUsGovEast1   DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "us-gov-east-1"
	DestinationDatabricksDataSourceAmazonS3S3BucketRegionUsGovWest1   DestinationDatabricksDataSourceAmazonS3S3BucketRegion = "us-gov-west-1"
)

func (e DestinationDatabricksDataSourceAmazonS3S3BucketRegion) ToPointer() *DestinationDatabricksDataSourceAmazonS3S3BucketRegion {
	return &e
}

func (e *DestinationDatabricksDataSourceAmazonS3S3BucketRegion) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "":
		fallthrough
	case "us-east-1":
		fallthrough
	case "us-east-2":
		fallthrough
	case "us-west-1":
		fallthrough
	case "us-west-2":
		fallthrough
	case "af-south-1":
		fallthrough
	case "ap-east-1":
		fallthrough
	case "ap-south-1":
		fallthrough
	case "ap-northeast-1":
		fallthrough
	case "ap-northeast-2":
		fallthrough
	case "ap-northeast-3":
		fallthrough
	case "ap-southeast-1":
		fallthrough
	case "ap-southeast-2":
		fallthrough
	case "ca-central-1":
		fallthrough
	case "cn-north-1":
		fallthrough
	case "cn-northwest-1":
		fallthrough
	case "eu-central-1":
		fallthrough
	case "eu-north-1":
		fallthrough
	case "eu-south-1":
		fallthrough
	case "eu-west-1":
		fallthrough
	case "eu-west-2":
		fallthrough
	case "eu-west-3":
		fallthrough
	case "sa-east-1":
		fallthrough
	case "me-south-1":
		fallthrough
	case "us-gov-east-1":
		fallthrough
	case "us-gov-west-1":
		*e = DestinationDatabricksDataSourceAmazonS3S3BucketRegion(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationDatabricksDataSourceAmazonS3S3BucketRegion: %v", v)
	}
}

// DestinationDatabricksDataSourceAmazonS3 - Storage on which the delta lake is built.
type DestinationDatabricksDataSourceAmazonS3 struct {
	DataSourceType DestinationDatabricksDataSourceAmazonS3DataSourceType `json:"data_source_type"`
	// The pattern allows you to set the file-name format for the S3 staging file(s)
	FileNamePattern *string `json:"file_name_pattern,omitempty"`
	// The Access Key Id granting allow one to access the above S3 staging bucket. Airbyte requires Read and Write permissions to the given bucket.
	S3AccessKeyID string `json:"s3_access_key_id"`
	// The name of the S3 bucket to use for intermittent staging of the data.
	S3BucketName string `json:"s3_bucket_name"`
	// The directory under the S3 bucket where data will be written.
	S3BucketPath string `json:"s3_bucket_path"`
	// The region of the S3 staging bucket to use if utilising a copy strategy.
	S3BucketRegion DestinationDatabricksDataSourceAmazonS3S3BucketRegion `json:"s3_bucket_region"`
	// The corresponding secret to the above access key id.
	S3SecretAccessKey string `json:"s3_secret_access_key"`
}

type DestinationDatabricksDataSourceRecommendedManagedTablesDataSourceType string

const (
	DestinationDatabricksDataSourceRecommendedManagedTablesDataSourceTypeManagedTablesStorage DestinationDatabricksDataSourceRecommendedManagedTablesDataSourceType = "MANAGED_TABLES_STORAGE"
)

func (e DestinationDatabricksDataSourceRecommendedManagedTablesDataSourceType) ToPointer() *DestinationDatabricksDataSourceRecommendedManagedTablesDataSourceType {
	return &e
}

func (e *DestinationDatabricksDataSourceRecommendedManagedTablesDataSourceType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "MANAGED_TABLES_STORAGE":
		*e = DestinationDatabricksDataSourceRecommendedManagedTablesDataSourceType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationDatabricksDataSourceRecommendedManagedTablesDataSourceType: %v", v)
	}
}

// DestinationDatabricksDataSourceRecommendedManagedTables - Storage on which the delta lake is built.
type DestinationDatabricksDataSourceRecommendedManagedTables struct {
	DataSourceType DestinationDatabricksDataSourceRecommendedManagedTablesDataSourceType `json:"data_source_type"`
}

type DestinationDatabricksDataSourceType string

const (
	DestinationDatabricksDataSourceTypeDestinationDatabricksDataSourceRecommendedManagedTables DestinationDatabricksDataSourceType = "destination-databricks_Data Source_[Recommended] Managed tables"
	DestinationDatabricksDataSourceTypeDestinationDatabricksDataSourceAmazonS3                 DestinationDatabricksDataSourceType = "destination-databricks_Data Source_Amazon S3"
	DestinationDatabricksDataSourceTypeDestinationDatabricksDataSourceAzureBlobStorage         DestinationDatabricksDataSourceType = "destination-databricks_Data Source_Azure Blob Storage"
)

type DestinationDatabricksDataSource struct {
	DestinationDatabricksDataSourceRecommendedManagedTables *DestinationDatabricksDataSourceRecommendedManagedTables
	DestinationDatabricksDataSourceAmazonS3                 *DestinationDatabricksDataSourceAmazonS3
	DestinationDatabricksDataSourceAzureBlobStorage         *DestinationDatabricksDataSourceAzureBlobStorage

	Type DestinationDatabricksDataSourceType
}

func CreateDestinationDatabricksDataSourceDestinationDatabricksDataSourceRecommendedManagedTables(destinationDatabricksDataSourceRecommendedManagedTables DestinationDatabricksDataSourceRecommendedManagedTables) DestinationDatabricksDataSource {
	typ := DestinationDatabricksDataSourceTypeDestinationDatabricksDataSourceRecommendedManagedTables

	return DestinationDatabricksDataSource{
		DestinationDatabricksDataSourceRecommendedManagedTables: &destinationDatabricksDataSourceRecommendedManagedTables,
		Type: typ,
	}
}

func CreateDestinationDatabricksDataSourceDestinationDatabricksDataSourceAmazonS3(destinationDatabricksDataSourceAmazonS3 DestinationDatabricksDataSourceAmazonS3) DestinationDatabricksDataSource {
	typ := DestinationDatabricksDataSourceTypeDestinationDatabricksDataSourceAmazonS3

	return DestinationDatabricksDataSource{
		DestinationDatabricksDataSourceAmazonS3: &destinationDatabricksDataSourceAmazonS3,
		Type:                                    typ,
	}
}

func CreateDestinationDatabricksDataSourceDestinationDatabricksDataSourceAzureBlobStorage(destinationDatabricksDataSourceAzureBlobStorage DestinationDatabricksDataSourceAzureBlobStorage) DestinationDatabricksDataSource {
	typ := DestinationDatabricksDataSourceTypeDestinationDatabricksDataSourceAzureBlobStorage

	return DestinationDatabricksDataSource{
		DestinationDatabricksDataSourceAzureBlobStorage: &destinationDatabricksDataSourceAzureBlobStorage,
		Type: typ,
	}
}

func (u *DestinationDatabricksDataSource) UnmarshalJSON(data []byte) error {
	var d *json.Decoder

	destinationDatabricksDataSourceRecommendedManagedTables := new(DestinationDatabricksDataSourceRecommendedManagedTables)
	d = json.NewDecoder(bytes.NewReader(data))
	d.DisallowUnknownFields()
	if err := d.Decode(&destinationDatabricksDataSourceRecommendedManagedTables); err == nil {
		u.DestinationDatabricksDataSourceRecommendedManagedTables = destinationDatabricksDataSourceRecommendedManagedTables
		u.Type = DestinationDatabricksDataSourceTypeDestinationDatabricksDataSourceRecommendedManagedTables
		return nil
	}

	destinationDatabricksDataSourceAzureBlobStorage := new(DestinationDatabricksDataSourceAzureBlobStorage)
	d = json.NewDecoder(bytes.NewReader(data))
	d.DisallowUnknownFields()
	if err := d.Decode(&destinationDatabricksDataSourceAzureBlobStorage); err == nil {
		u.DestinationDatabricksDataSourceAzureBlobStorage = destinationDatabricksDataSourceAzureBlobStorage
		u.Type = DestinationDatabricksDataSourceTypeDestinationDatabricksDataSourceAzureBlobStorage
		return nil
	}

	destinationDatabricksDataSourceAmazonS3 := new(DestinationDatabricksDataSourceAmazonS3)
	d = json.NewDecoder(bytes.NewReader(data))
	d.DisallowUnknownFields()
	if err := d.Decode(&destinationDatabricksDataSourceAmazonS3); err == nil {
		u.DestinationDatabricksDataSourceAmazonS3 = destinationDatabricksDataSourceAmazonS3
		u.Type = DestinationDatabricksDataSourceTypeDestinationDatabricksDataSourceAmazonS3
		return nil
	}

	return errors.New("could not unmarshal into supported union types")
}

func (u DestinationDatabricksDataSource) MarshalJSON() ([]byte, error) {
	if u.DestinationDatabricksDataSourceRecommendedManagedTables != nil {
		return json.Marshal(u.DestinationDatabricksDataSourceRecommendedManagedTables)
	}

	if u.DestinationDatabricksDataSourceAzureBlobStorage != nil {
		return json.Marshal(u.DestinationDatabricksDataSourceAzureBlobStorage)
	}

	if u.DestinationDatabricksDataSourceAmazonS3 != nil {
		return json.Marshal(u.DestinationDatabricksDataSourceAmazonS3)
	}

	return nil, nil
}

type DestinationDatabricksDatabricks string

const (
	DestinationDatabricksDatabricksDatabricks DestinationDatabricksDatabricks = "databricks"
)

func (e DestinationDatabricksDatabricks) ToPointer() *DestinationDatabricksDatabricks {
	return &e
}

func (e *DestinationDatabricksDatabricks) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "databricks":
		*e = DestinationDatabricksDatabricks(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationDatabricksDatabricks: %v", v)
	}
}

type DestinationDatabricks struct {
	// You must agree to the Databricks JDBC Driver <a href="https://databricks.com/jdbc-odbc-driver-license">Terms & Conditions</a> to use this connector.
	AcceptTerms bool `json:"accept_terms"`
	// Storage on which the delta lake is built.
	DataSource DestinationDatabricksDataSource `json:"data_source"`
	// The name of the catalog. If not specified otherwise, the "hive_metastore" will be used.
	Database *string `json:"database,omitempty"`
	// Databricks Cluster HTTP Path.
	DatabricksHTTPPath string `json:"databricks_http_path"`
	// Databricks Personal Access Token for making authenticated requests.
	DatabricksPersonalAccessToken string `json:"databricks_personal_access_token"`
	// Databricks Cluster Port.
	DatabricksPort *string `json:"databricks_port,omitempty"`
	// Databricks Cluster Server Hostname.
	DatabricksServerHostname string                          `json:"databricks_server_hostname"`
	DestinationType          DestinationDatabricksDatabricks `json:"destinationType"`
	// Support schema evolution for all streams. If "false", the connector might fail when a stream's schema changes.
	EnableSchemaEvolution *bool `json:"enable_schema_evolution,omitempty"`
	// Default to 'true'. Switch it to 'false' for debugging purpose.
	PurgeStagingData *bool `json:"purge_staging_data,omitempty"`
	// The default schema tables are written. If not specified otherwise, the "default" will be used.
	Schema *string `json:"schema,omitempty"`
}
